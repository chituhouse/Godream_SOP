{"type":"file-history-snapshot","messageId":"c6354fe2-abe4-46ab-a12d-9d1cc7a44de1","snapshot":{"messageId":"c6354fe2-abe4-46ab-a12d-9d1cc7a44de1","trackedFileBackups":{},"timestamp":"2025-12-13T02:27:33.308Z"},"isSnapshotUpdate":false}
{"parentUuid":null,"isSidechain":false,"userType":"external","cwd":"/Users/yunchang/Documents/GitHub/gemeng2","sessionId":"4ece2923-b68d-44f9-80e5-03967c577bcd","version":"2.0.69","gitBranch":"main","type":"user","message":{"role":"user","content":"我看到当前项目使用的模型是OPus4.5,但是我记得这个模型应该是有一个thinking模型的。包括snet4.5，也是有一个think模型的。你查一下官方库，看是不是。"},"uuid":"c6354fe2-abe4-46ab-a12d-9d1cc7a44de1","timestamp":"2025-12-13T02:27:33.301Z","thinkingMetadata":{"level":"high","disabled":false,"triggers":[]},"todos":[]}
{"parentUuid":"c6354fe2-abe4-46ab-a12d-9d1cc7a44de1","isSidechain":false,"userType":"external","cwd":"/Users/yunchang/Documents/GitHub/gemeng2","sessionId":"4ece2923-b68d-44f9-80e5-03967c577bcd","version":"2.0.69","gitBranch":"main","message":{"model":"claude-opus-4-5-20251101","id":"msg_01UZz1ofzxQ2DniJTjcsHfds","type":"message","role":"assistant","content":[{"type":"thinking","thinking":"用户想了解Claude模型是否有thinking版本，特别是Opus 4.5和Sonnet 4.5。让我查一下官方文档来确认这个信息。","signature":"ErICCkYIChgCKkCI8IRADkfPCpu1HDDwImVHJdrKcFvB012Pw9EQYfS/Q6MdJqOd+kMa9CHXy3s2nOrE0OCZxuweWo5FYALWFzwhEgxopz2yz1kwXFFlu+kaDD9mndMEpKBoYXB7OCIwOYXUN9hVJ/NF0h/Y4NHYVRkcIRQwYqn/0K0TRsPwzRHar/aXqL3rvo3KkkOx2DsqKpkBag26BYObLFvR4psbk51ACDGukEOykLIWWSZaaXHYWR2s3ALn4H1J4s6o82GjeIT3j6t1Rz5KgYq57CcPh62Qcc1RJJuKtIDzrIhR5IssBmoc/5eHu1hX/drJdHA6MaIhM2Y4Mqf7t0JcfLMVwgO9jnrCSzTtaTyHQ3NBPYJ4PsD9w96u1uziFGvz7Fh/IwtqK3Hvwk6DAwX3GAE="}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":10,"cache_creation_input_tokens":5325,"cache_read_input_tokens":22185,"cache_creation":{"ephemeral_5m_input_tokens":5325,"ephemeral_1h_input_tokens":0},"output_tokens":3,"service_tier":"standard"}},"type":"assistant","uuid":"b9c3fef8-3a94-48ad-9517-3238a8754201","timestamp":"2025-12-13T02:27:38.378Z"}
{"parentUuid":"b9c3fef8-3a94-48ad-9517-3238a8754201","isSidechain":false,"userType":"external","cwd":"/Users/yunchang/Documents/GitHub/gemeng2","sessionId":"4ece2923-b68d-44f9-80e5-03967c577bcd","version":"2.0.69","gitBranch":"main","message":{"model":"claude-opus-4-5-20251101","id":"msg_01UZz1ofzxQ2DniJTjcsHfds","type":"message","role":"assistant","content":[{"type":"text","text":"我来查一下Claude模型的thinking版本信息。"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":10,"cache_creation_input_tokens":5325,"cache_read_input_tokens":22185,"cache_creation":{"ephemeral_5m_input_tokens":5325,"ephemeral_1h_input_tokens":0},"output_tokens":3,"service_tier":"standard"}},"type":"assistant","uuid":"f9ebd897-fb1f-417b-a3d2-d720669903fd","timestamp":"2025-12-13T02:27:38.959Z"}
{"parentUuid":"f9ebd897-fb1f-417b-a3d2-d720669903fd","isSidechain":false,"userType":"external","cwd":"/Users/yunchang/Documents/GitHub/gemeng2","sessionId":"4ece2923-b68d-44f9-80e5-03967c577bcd","version":"2.0.69","gitBranch":"main","message":{"model":"claude-opus-4-5-20251101","id":"msg_01UZz1ofzxQ2DniJTjcsHfds","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01DfLf9CcQjLp6Cgn99MBZsZ","name":"WebSearch","input":{"query":"Claude Opus 4.5 Sonnet thinking model extended thinking 2025"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":10,"cache_creation_input_tokens":5325,"cache_read_input_tokens":22185,"cache_creation":{"ephemeral_5m_input_tokens":5325,"ephemeral_1h_input_tokens":0},"output_tokens":3,"service_tier":"standard"}},"type":"assistant","uuid":"4158302d-23d2-4296-969b-18faeeb4acba","timestamp":"2025-12-13T02:27:39.379Z"}
{"parentUuid":"4158302d-23d2-4296-969b-18faeeb4acba","isSidechain":false,"userType":"external","cwd":"/Users/yunchang/Documents/GitHub/gemeng2","sessionId":"4ece2923-b68d-44f9-80e5-03967c577bcd","version":"2.0.69","gitBranch":"main","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01DfLf9CcQjLp6Cgn99MBZsZ","type":"tool_result","content":"Web search results for query: \"Claude Opus 4.5 Sonnet thinking model extended thinking 2025\"\n\nLinks: [{\"title\":\"Introducing Claude Sonnet 4.5\",\"url\":\"https://www.anthropic.com/news/claude-sonnet-4-5\"},{\"title\":\"What's new in Claude 4.5 - Claude Docs\",\"url\":\"https://platform.claude.com/docs/en/about-claude/models/whats-new-claude-4-5\"},{\"title\":\"What's new in Claude Sonnet 4.5 - Claude Docs\",\"url\":\"https://docs.claude.com/en/docs/about-claude/models/whats-new-sonnet-4-5\"},{\"title\":\"Introducing Claude Opus 4.5\",\"url\":\"https://www.anthropic.com/news/claude-opus-4-5\"},{\"title\":\"Claude Sonnet 4.5 Extended Thinking Explained: Can Code for 30 Hours Straight, But That’s Not the Scary Part | by Cogni Down Under | Medium\",\"url\":\"https://medium.com/@cognidownunder/claude-sonnet-4-5-4ddf33d53cd4\"},{\"title\":\"Building with extended thinking - Claude Docs\",\"url\":\"https://platform.claude.com/docs/en/build-with-claude/extended-thinking\"},{\"title\":\"Claude Sonnet 4.5\",\"url\":\"https://www.anthropic.com/claude/sonnet\"},{\"title\":\"How to Use Claude Opus 4.5 API - CometAPI - All AI Models in One API\",\"url\":\"https://www.cometapi.com/how-to-use-claude-opus-4-5-api/\"},{\"title\":\"Claude Opus 4.5\",\"url\":\"https://www.anthropic.com/claude/opus\"},{\"title\":\"Anthropic’s Claude Opus 4.5 now available in Amazon Bedrock\",\"url\":\"https://www.aboutamazon.com/news/aws/anthropic-claude-4-opus-sonnet-amazon-bedrock\"}]\n\nHere are the key findings about Claude Opus 4.5, Sonnet 4.5, and extended thinking in 2025:\n\n## Claude Opus 4.5\n\nClaude Opus 4.5 was released by Anthropic in late November 2025 as a more capable, more efficient Opus-class model targeted at professional software engineering, agentic workflows, and long-horizon tasks.(no content)Claude Opus 4.5 is Anthropic's most advanced model, setting a new standard for coding, agents, and computer use, and office tasks.\n\nKey features include:\n- A new effort parameter on the Claude API lets you decide to minimize time and spend or maximize capability. Set to a medium effort level, Opus 4.5 matches Sonnet 4.5's best score on SWE-bench Verified, but uses 76% fewer output tokens.\n- Available with a 200k token context window.\n- Pricing for Opus 4.5 starts at $5 per million input tokens and $25 per million output tokens, with up to 90% cost savings with prompt caching and 50% savings with batch processing.\n\n## Claude Sonnet 4.5\n\nClaude Sonnet 4.5 just coded autonomously for 30 hours straight, built entire applications, bought domain names, and performed security audits without human intervention. Anthropic launched this on September 29, 2025, calling it \"the best coding model in the world.\"(no content)Claude Sonnet 4.5 is Anthropic's best coding model to date, with significant improvements including SWE-bench Verified performance, enhanced planning and system design, improved security engineering, and better instruction following.\n\n## Extended Thinking\n\nThese hybrid reasoning models can toggle between near-instant responses and extended thinking and have set new standards across coding, advanced reasoning, and multi-step workflows.(no content)Extended thinking is available in Claude Sonnet 3.7, Sonnet 4, Sonnet 4.5, Haiku 4.5, Opus 4, Opus 4.1, and Opus 4.5.\n\nKey aspects:\n- Claude Opus 4.5 introduces a new default behavior: thinking blocks from previous assistant turns are preserved in model context by default. This differs from earlier models, which remove thinking blocks from prior turns.\n- When you enable extended thinking, Claude creates visible \"thinking\" blocks where it works through problems step by step. Not summaries or explanations, but actual internal reasoning processes.\n- Claude Sonnet 4.5 thinks between actions. Enable interleaved thinking with the beta header interleaved-thinking-2025-05-14, and watch Claude reason between tool calls rather than just at the beginning.\n- Extended thinking is disabled by default, but Anthropic recommends enabling it for complex coding work. Be aware that extended thinking impacts prompt caching efficiency.\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"93a5a41e-69be-431f-956b-a5d9a060151c","timestamp":"2025-12-13T02:28:13.176Z","toolUseResult":{"query":"Claude Opus 4.5 Sonnet thinking model extended thinking 2025","results":[{"tool_use_id":"srvtoolu_01YQUaNStBaECqeTU9hbqDBh","content":[{"title":"Introducing Claude Sonnet 4.5","url":"https://www.anthropic.com/news/claude-sonnet-4-5"},{"title":"What's new in Claude 4.5 - Claude Docs","url":"https://platform.claude.com/docs/en/about-claude/models/whats-new-claude-4-5"},{"title":"What's new in Claude Sonnet 4.5 - Claude Docs","url":"https://docs.claude.com/en/docs/about-claude/models/whats-new-sonnet-4-5"},{"title":"Introducing Claude Opus 4.5","url":"https://www.anthropic.com/news/claude-opus-4-5"},{"title":"Claude Sonnet 4.5 Extended Thinking Explained: Can Code for 30 Hours Straight, But That’s Not the Scary Part | by Cogni Down Under | Medium","url":"https://medium.com/@cognidownunder/claude-sonnet-4-5-4ddf33d53cd4"},{"title":"Building with extended thinking - Claude Docs","url":"https://platform.claude.com/docs/en/build-with-claude/extended-thinking"},{"title":"Claude Sonnet 4.5","url":"https://www.anthropic.com/claude/sonnet"},{"title":"How to Use Claude Opus 4.5 API - CometAPI - All AI Models in One API","url":"https://www.cometapi.com/how-to-use-claude-opus-4-5-api/"},{"title":"Claude Opus 4.5","url":"https://www.anthropic.com/claude/opus"},{"title":"Anthropic’s Claude Opus 4.5 now available in Amazon Bedrock","url":"https://www.aboutamazon.com/news/aws/anthropic-claude-4-opus-sonnet-amazon-bedrock"}]},"Here are the key findings about Claude Opus 4.5, Sonnet 4.5, and extended thinking in 2025:\n\n## Claude Opus 4.5\n\nClaude Opus 4.5 was released by Anthropic in late November 2025 as a more capable, more efficient Opus-class model targeted at professional software engineering, agentic workflows, and long-horizon tasks.(no content)Claude Opus 4.5 is Anthropic's most advanced model, setting a new standard for coding, agents, and computer use, and office tasks.\n\nKey features include:\n- A new effort parameter on the Claude API lets you decide to minimize time and spend or maximize capability. Set to a medium effort level, Opus 4.5 matches Sonnet 4.5's best score on SWE-bench Verified, but uses 76% fewer output tokens.\n- Available with a 200k token context window.\n- Pricing for Opus 4.5 starts at $5 per million input tokens and $25 per million output tokens, with up to 90% cost savings with prompt caching and 50% savings with batch processing.\n\n## Claude Sonnet 4.5\n\nClaude Sonnet 4.5 just coded autonomously for 30 hours straight, built entire applications, bought domain names, and performed security audits without human intervention. Anthropic launched this on September 29, 2025, calling it \"the best coding model in the world.\"(no content)Claude Sonnet 4.5 is Anthropic's best coding model to date, with significant improvements including SWE-bench Verified performance, enhanced planning and system design, improved security engineering, and better instruction following.\n\n## Extended Thinking\n\nThese hybrid reasoning models can toggle between near-instant responses and extended thinking and have set new standards across coding, advanced reasoning, and multi-step workflows.(no content)Extended thinking is available in Claude Sonnet 3.7, Sonnet 4, Sonnet 4.5, Haiku 4.5, Opus 4, Opus 4.1, and Opus 4.5.\n\nKey aspects:\n- Claude Opus 4.5 introduces a new default behavior: thinking blocks from previous assistant turns are preserved in model context by default. This differs from earlier models, which remove thinking blocks from prior turns.\n- When you enable extended thinking, Claude creates visible \"thinking\" blocks where it works through problems step by step. Not summaries or explanations, but actual internal reasoning processes.\n- Claude Sonnet 4.5 thinks between actions. Enable interleaved thinking with the beta header interleaved-thinking-2025-05-14, and watch Claude reason between tool calls rather than just at the beginning.\n- Extended thinking is disabled by default, but Anthropic recommends enabling it for complex coding work. Be aware that extended thinking impacts prompt caching efficiency."],"durationSeconds":24.35467445800001}}
{"parentUuid":"93a5a41e-69be-431f-956b-a5d9a060151c","isSidechain":false,"userType":"external","cwd":"/Users/yunchang/Documents/GitHub/gemeng2","sessionId":"4ece2923-b68d-44f9-80e5-03967c577bcd","version":"2.0.69","gitBranch":"main","message":{"model":"claude-opus-4-5-20251101","id":"msg_01EWGtXPQZJhWhKxgpPsnBn1","type":"message","role":"assistant","content":[{"type":"thinking","thinking":"好的，我找到了关于Extended Thinking的信息。让我再查一下更详细的模型ID信息。","signature":"Eo8CCkYIChgCKkAoY4J8BZTl1OSwQ56iofMTmdlQSzw/QL31nqHQvd1aed61xakGfZCjXeOGpDaWaiiihh24J4Lnne3RB9T8m/jgEgwi9bLymyuRdKPboAYaDIn3AOrQDjqxTccdgyIw5YivqJy6bIB1X/Hlf6304pbAXFrGvEV2GGNcjUe98GUBk6575vB+CKCMainpJoNvKne9sW6TOiU/Uxvz7CDp//BYYHGXAv3fXAmrnaNjZCmmTtluPq2AORNVvahZiRHD0r7Ur5O8LxwDYyBUkCiivl6vtTRJWj49mnJ5ByPoLjpNPeYN1Nl523Iojun2uYZsHANrDmxZllNBof6p6rEU0kmFZM4BIHM/XRgB"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":1319,"cache_read_input_tokens":27510,"cache_creation":{"ephemeral_5m_input_tokens":1319,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"type":"assistant","uuid":"d09ca0e0-b92e-4f0b-b79f-e9b7e3261b06","timestamp":"2025-12-13T02:28:17.654Z"}
{"parentUuid":"d09ca0e0-b92e-4f0b-b79f-e9b7e3261b06","isSidechain":false,"userType":"external","cwd":"/Users/yunchang/Documents/GitHub/gemeng2","sessionId":"4ece2923-b68d-44f9-80e5-03967c577bcd","version":"2.0.69","gitBranch":"main","message":{"model":"claude-opus-4-5-20251101","id":"msg_01EWGtXPQZJhWhKxgpPsnBn1","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_016w1AgJHSNwZuighBgFp8fi","name":"WebFetch","input":{"url":"https://docs.anthropic.com/en/docs/about-claude/models","prompt":"查找所有Claude模型的model ID，特别是带有extended thinking功能的模型ID，以及如何启用extended thinking"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":1319,"cache_read_input_tokens":27510,"cache_creation":{"ephemeral_5m_input_tokens":1319,"ephemeral_1h_input_tokens":0},"output_tokens":159,"service_tier":"standard"}},"type":"assistant","uuid":"4525577c-7a24-4fc2-b7d8-9c77effb62cc","timestamp":"2025-12-13T02:28:18.916Z"}
{"parentUuid":"4525577c-7a24-4fc2-b7d8-9c77effb62cc","isSidechain":false,"userType":"external","cwd":"/Users/yunchang/Documents/GitHub/gemeng2","sessionId":"4ece2923-b68d-44f9-80e5-03967c577bcd","version":"2.0.69","gitBranch":"main","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_016w1AgJHSNwZuighBgFp8fi","type":"tool_result","content":"REDIRECT DETECTED: The URL redirects to a different host.\n\nOriginal URL: https://docs.anthropic.com/en/docs/about-claude/models\nRedirect URL: https://docs.claude.com/en/docs/about-claude/models\nStatus: 301 Moved Permanently\n\nTo complete your request, I need to fetch content from the redirected URL. Please use WebFetch again with these parameters:\n- url: \"https://docs.claude.com/en/docs/about-claude/models\"\n- prompt: \"查找所有Claude模型的model ID，特别是带有extended thinking功能的模型ID，以及如何启用extended thinking\""}]},"uuid":"0687b3fa-c417-4261-a128-ab5f449b6308","timestamp":"2025-12-13T02:44:13.848Z","toolUseResult":{"bytes":547,"code":301,"codeText":"Moved Permanently","result":"REDIRECT DETECTED: The URL redirects to a different host.\n\nOriginal URL: https://docs.anthropic.com/en/docs/about-claude/models\nRedirect URL: https://docs.claude.com/en/docs/about-claude/models\nStatus: 301 Moved Permanently\n\nTo complete your request, I need to fetch content from the redirected URL. Please use WebFetch again with these parameters:\n- url: \"https://docs.claude.com/en/docs/about-claude/models\"\n- prompt: \"查找所有Claude模型的model ID，特别是带有extended thinking功能的模型ID，以及如何启用extended thinking\"","durationMs":1321,"url":"https://docs.anthropic.com/en/docs/about-claude/models"}}
{"parentUuid":"0687b3fa-c417-4261-a128-ab5f449b6308","isSidechain":false,"userType":"external","cwd":"/Users/yunchang/Documents/GitHub/gemeng2","sessionId":"4ece2923-b68d-44f9-80e5-03967c577bcd","version":"2.0.69","gitBranch":"main","message":{"model":"claude-opus-4-5-20251101","id":"msg_01EqTuYd2aGojFX7yHp6mf6s","type":"message","role":"assistant","content":[{"type":"thinking","thinking":"需要重定向到新的URL。","signature":"EsYBCkYIChgCKkBsQE6Q9eni1UqvCFRpc5jhsj6Ip0YUw59OzZcwdViLUrrUClcxXLPE2hO2lYWT1gbgfO76THZ+CW3QWshqFfiOEgx7VOVRIvAtzAYFHJ8aDHNNSQAL6QH+gSrViiIwecmhxbKI/XYbGypUikQBtGhya1JVg/Z1QjEG1wI0kIPXuzIiLdJKKD211zZsluK8Ki7QkJTT5OfWw75UkPOBhtzD+Q0vBocAkceDcJw1/X9iNZUpke2rHBs2VfFfmZKPGAE="}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":16305,"cache_read_input_tokens":12859,"cache_creation":{"ephemeral_5m_input_tokens":16305,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"type":"assistant","uuid":"4fa7fd30-f4af-47d2-9455-bf5181822111","timestamp":"2025-12-13T02:44:19.203Z"}
{"parentUuid":"4fa7fd30-f4af-47d2-9455-bf5181822111","isSidechain":false,"userType":"external","cwd":"/Users/yunchang/Documents/GitHub/gemeng2","sessionId":"4ece2923-b68d-44f9-80e5-03967c577bcd","version":"2.0.69","gitBranch":"main","message":{"model":"claude-opus-4-5-20251101","id":"msg_01EqTuYd2aGojFX7yHp6mf6s","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_015mFr5WsFpLz2BbHQBn66W1","name":"WebFetch","input":{"url":"https://docs.claude.com/en/docs/about-claude/models","prompt":"查找所有Claude模型的model ID，特别是带有extended thinking功能的模型ID，以及如何启用extended thinking"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":16305,"cache_read_input_tokens":12859,"cache_creation":{"ephemeral_5m_input_tokens":16305,"ephemeral_1h_input_tokens":0},"output_tokens":136,"service_tier":"standard"}},"type":"assistant","uuid":"5caf6bae-359e-407a-8e5b-554a47b8d6d5","timestamp":"2025-12-13T02:44:19.784Z"}
{"parentUuid":"5caf6bae-359e-407a-8e5b-554a47b8d6d5","isSidechain":false,"userType":"external","cwd":"/Users/yunchang/Documents/GitHub/gemeng2","sessionId":"4ece2923-b68d-44f9-80e5-03967c577bcd","version":"2.0.69","gitBranch":"main","type":"user","message":{"role":"user","content":[{"type":"tool_result","content":"Claude Code is unable to fetch from docs.claude.com","is_error":true,"tool_use_id":"toolu_015mFr5WsFpLz2BbHQBn66W1"}]},"uuid":"f9c4c88b-e2d5-41a9-b03f-105f4db58c59","timestamp":"2025-12-13T02:56:46.387Z","toolUseResult":"Error: Claude Code is unable to fetch from docs.claude.com"}
{"parentUuid":"f9c4c88b-e2d5-41a9-b03f-105f4db58c59","isSidechain":false,"userType":"external","cwd":"/Users/yunchang/Documents/GitHub/gemeng2","sessionId":"4ece2923-b68d-44f9-80e5-03967c577bcd","version":"2.0.69","gitBranch":"main","message":{"model":"claude-opus-4-5-20251101","id":"msg_01PKF8HTvoLGRez6oAJjdSru","type":"message","role":"assistant","content":[{"type":"thinking","thinking":"无法访问这个URL。让我试试另一个来源。","signature":"Et4BCkYIChgCKkCvRwiipsvL5VWYp//JtM17ZdX2XIHTIKFT8zVZD8WWoxgkgxuR+XE+ZYihjPbvYxwpoKknvXG+l/lXKXsp4Bl0EgyPZ6e2tTXnHjEduY8aDMDv8AMZusIt3YeHhCIwJtX+gk/ePHghDfTqgbBwO+HTaxq/dhuA5VzqIVFg4Lo8/D1UnbrU/0Fa7JM1+PDHKkZdmiRcgaI3+b0G5JJAXN8jzXccqr5MckBCd5y8Y58692t46LItOo5ADtjrMfnOvdg2KgVFzoIQS9WlKGhb7kdPtgrWs/wQGAE="}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":16480,"cache_read_input_tokens":12859,"cache_creation":{"ephemeral_5m_input_tokens":16480,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"type":"assistant","uuid":"8d8ee0c3-cdec-4bde-8c39-bc938acd7ffd","timestamp":"2025-12-13T02:56:51.518Z"}
{"parentUuid":"8d8ee0c3-cdec-4bde-8c39-bc938acd7ffd","isSidechain":false,"userType":"external","cwd":"/Users/yunchang/Documents/GitHub/gemeng2","sessionId":"4ece2923-b68d-44f9-80e5-03967c577bcd","version":"2.0.69","gitBranch":"main","message":{"model":"claude-opus-4-5-20251101","id":"msg_01PKF8HTvoLGRez6oAJjdSru","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01JBNrGx4hHkyhyNf8LuZw5Y","name":"WebFetch","input":{"url":"https://platform.claude.com/docs/en/about-claude/models","prompt":"查找所有Claude模型的model ID，特别是带有extended thinking功能的模型ID"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":16480,"cache_read_input_tokens":12859,"cache_creation":{"ephemeral_5m_input_tokens":16480,"ephemeral_1h_input_tokens":0},"output_tokens":135,"service_tier":"standard"}},"type":"assistant","uuid":"4c5fc8ec-0569-4cff-89c5-9329a90cc1ae","timestamp":"2025-12-13T02:56:52.789Z"}
{"parentUuid":"4c5fc8ec-0569-4cff-89c5-9329a90cc1ae","isSidechain":false,"userType":"external","cwd":"/Users/yunchang/Documents/GitHub/gemeng2","sessionId":"4ece2923-b68d-44f9-80e5-03967c577bcd","version":"2.0.69","gitBranch":"main","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01JBNrGx4hHkyhyNf8LuZw5Y","type":"tool_result","content":"# Models overview\n\nClaude is a family of state-of-the-art large language models developed by Anthropic. This guide introduces our models and compares their performance.\n\n---\n\n## Choosing a model\n\nIf you're unsure which model to use, we recommend starting with **Claude Sonnet 4.5**. It offers the best balance of intelligence, speed, and cost for most use cases, with exceptional performance in coding and agentic tasks.\n\nAll current Claude models support text and image input, text output, multilingual capabilities, and vision. Models are available via the Anthropic API, AWS Bedrock, and Google Vertex AI.\n\nOnce you've picked a model, [learn how to make your first API call](/docs/en/get-started).\n\n### Latest models comparison\n\n| Feature | Claude Sonnet 4.5 | Claude Haiku 4.5 | Claude Opus 4.5 |\n|:--------|:------------------|:-----------------|:----------------|\n| **Description** | Our smart model for complex agents and coding | Our fastest model with near-frontier intelligence | Premium model combining maximum intelligence with practical performance |\n| **Claude API ID** | claude-sonnet-4-5-20250929 | claude-haiku-4-5-20251001 | claude-opus-4-5-20251101 |\n| **Claude API alias**<sup>1</sup> | claude-sonnet-4-5 | claude-haiku-4-5 | claude-opus-4-5 |\n| **AWS Bedrock ID** | anthropic.claude-sonnet-4-5-20250929-v1:0 | anthropic.claude-haiku-4-5-20251001-v1:0 | anthropic.claude-opus-4-5-20251101-v1:0 |\n| **GCP Vertex AI ID** | claude-sonnet-4-5@20250929 | claude-haiku-4-5@20251001 | claude-opus-4-5@20251101 |\n| **Pricing**<sup>2</sup> | \\$3 / input MTok<br/>\\$15 / output MTok | \\$1 / input MTok<br/>\\$5 / output MTok | \\$5 / input MTok<br/>\\$25 / output MTok |\n| **[Extended thinking](/docs/en/build-with-claude/extended-thinking)** | Yes | Yes | Yes |\n| **[Priority Tier](/docs/en/api/service-tiers)** | Yes | Yes | Yes |\n| **Comparative latency** | Fast | Fastest | Moderate |\n| **Context window** | <Tooltip tooltipContent=\"~150K words \\ ~680K unicode characters\">200K tokens</Tooltip> / <br/> <Tooltip tooltipContent=\"~750K words \\ ~3.4M unicode characters\">1M tokens</Tooltip> (beta)<sup>3</sup> | <Tooltip tooltipContent=\"~150K words \\ ~680K unicode characters\">200K tokens</Tooltip> | <Tooltip tooltipContent=\"~150K words \\ ~680K unicode characters\">200K tokens</Tooltip> |\n| **Max output** | 64K tokens | 64K tokens | 64K tokens |\n| **Reliable knowledge cutoff** | Jan 2025<sup>4</sup> | Feb 2025 | May 2025<sup>4</sup> |\n| **Training data cutoff** | Jul 2025 | Jul 2025 | Aug 2025 |\n\n_<sup>1 - Aliases automatically point to the most recent model snapshot. When we release new model snapshots, we migrate aliases to point to the newest version of a model, typically within a week of the new release. While aliases are useful for experimentation, we recommend using specific model versions (e.g., `claude-sonnet-4-5-20250929`) in production applications to ensure consistent behavior.</sup>_\n\n_<sup>2 - See our [pricing page](/docs/en/about-claude/pricing) for complete pricing information including batch API discounts, prompt caching rates, extended thinking costs, and vision processing fees.</sup>_\n\n_<sup>3 - Claude Sonnet 4.5 supports a [1M token context window](/docs/en/build-with-claude/context-windows#1m-token-context-window) when using the `context-1m-2025-08-07` beta header. [Long context pricing](/docs/en/about-claude/pricing#long-context-pricing) applies to requests exceeding 200K tokens.</sup>_\n\n_<sup>4 - **Reliable knowledge cutoff** indicates the date through which a model's knowledge is most extensive and reliable. **Training data cutoff** is the broader date range of training data used. For example, Claude Sonnet 4.5 was trained on publicly available information through July 2025, but its knowledge is most extensive and reliable through January 2025. For more information, see [Anthropic's Transparency Hub](https://www.anthropic.com/transparency).</sup>_\n\n<Note>Models with the same snapshot date (e.g., 20240620) are identical across all platforms and do not change. The snapshot date in the model name ensures consistency and allows developers to rely on stable performance across different environments.</Note>\n\n<Note>Starting with **Claude Sonnet 4.5 and all future models**, AWS Bedrock and Google Vertex AI offer two endpoint types: **global endpoints** (dynamic routing for maximum availability) and **regional endpoints** (guaranteed data routing through specific geographic regions). For more information, see the [third-party platform pricing section](/docs/en/about-claude/pricing#third-party-platform-pricing).</Note>\n\n<section title=\"Legacy models\">\n\nThe following models are still available but we recommend migrating to current models for improved performance:\n\n| Feature | Claude Opus 4.1 | Claude Sonnet 4 | Claude Sonnet 3.7 | Claude Opus 4 | Claude Haiku 3.5 | Claude Haiku 3 |\n|:--------|:----------------|:----------------|:------------------|:--------------|:-----------------|:---------------|\n| **Claude API ID** | claude-opus-4-1-20250805 | claude-sonnet-4-20250514 | claude-3-7-sonnet-20250219 | claude-opus-4-20250514 | claude-3-5-haiku-20241022 | claude-3-haiku-20240307 |\n| **Claude API alias** | claude-opus-4-1 | claude-sonnet-4-0 | claude-3-7-sonnet-latest | claude-opus-4-0 | claude-3-5-haiku-latest | — |\n| **AWS Bedrock ID** | anthropic.claude-opus-4-1-20250805-v1:0 | anthropic.claude-sonnet-4-20250514-v1:0 | anthropic.claude-3-7-sonnet-20250219-v1:0 | anthropic.claude-opus-4-20250514-v1:0 | anthropic.claude-3-5-haiku-20241022-v1:0 | anthropic.claude-3-haiku-20240307-v1:0 |\n| **GCP Vertex AI ID** | claude-opus-4-1@20250805 | claude-sonnet-4@20250514 | claude-3-7-sonnet@20250219 | claude-opus-4@20250514 | claude-3-5-haiku@20241022 | claude-3-haiku@20240307 |\n| **Pricing** | \\$15 / input MTok<br/>\\$75 / output MTok | \\$3 / input MTok<br/>\\$15 / output MTok | \\$3 / input MTok<br/>\\$15 / output MTok | \\$15 / input MTok<br/>\\$75 / output MTok | \\$0.80 / input MTok<br/>\\$4 / output MTok | \\$0.25 / input MTok<br/>\\$1.25 / output MTok |\n| **[Extended thinking](/docs/en/build-with-claude/extended-thinking)** | Yes | Yes | Yes | Yes | No | No |\n| **[Priority Tier](/docs/en/api/service-tiers)** | Yes | Yes | Yes | Yes | Yes | No |\n| **Comparative latency** | Moderate | Fast | Fast | Moderate | Fastest | Fast |\n| **Context window** | <Tooltip tooltipContent=\"~150K words \\ ~680K unicode characters\">200K tokens</Tooltip> | <Tooltip tooltipContent=\"~150K words \\ ~680K unicode characters\">200K tokens</Tooltip> / <br/> <Tooltip tooltipContent=\"~750K words \\ ~3.4M unicode characters\">1M tokens</Tooltip> (beta)<sup>1</sup> | <Tooltip tooltipContent=\"~150K words \\ ~680K unicode characters\">200K tokens</Tooltip> | <Tooltip tooltipContent=\"~150K words \\ ~680K unicode characters\">200K tokens</Tooltip> | <Tooltip tooltipContent=\"~150K words \\ ~215K unicode characters\">200K tokens</Tooltip> | <Tooltip tooltipContent=\"~150K words \\ ~680K unicode characters\">200K tokens</Tooltip> |\n| **Max output** | 32K tokens | 64K tokens | 64K tokens / 128K tokens (beta)<sup>4</sup> | 32K tokens | 8K tokens | 4K tokens |\n| **Reliable knowledge cutoff** | Jan 2025<sup>2</sup> | Jan 2025<sup>2</sup> | Oct 2024<sup>2</sup> | Jan 2025<sup>2</sup> | <sup>3</sup> | <sup>3</sup> |\n| **Training data cutoff** | Mar 2025 | Mar 2025 | Nov 2024 | Mar 2025 | Jul 2024 | Aug 2023 |\n\n_<sup>1 - Claude Sonnet 4 supports a [1M token context window](/docs/en/build-with-claude/context-windows#1m-token-context-window) when using the `context-1m-2025-08-07` beta header. [Long context pricing](/docs/en/about-claude/pricing#long-context-pricing) applies to requests exceeding 200K tokens.</sup>_\n\n_<sup>2 - **Reliable knowledge cutoff** indicates the date through which a model's knowledge is most extensive and reliable. **Training data cutoff** is the broader date range of training data used.</sup>_\n\n_<sup>3 - Some Haiku models have a single training data cutoff date.</sup>_\n\n_<sup>4 - Include the beta header `output-128k-2025-02-19` in your API request to increase the maximum output token length to 128K tokens for Claude Sonnet 3.7. We strongly suggest using our [streaming Messages API](/docs/en/build-with-claude/streaming) to avoid timeouts when generating longer outputs. See our guidance on [long requests](/docs/en/api/errors#long-requests) for more details.</sup>_\n\n</section>\n\n## Prompt and output performance\n\nClaude 4 models excel in:\n- **Performance**: Top-tier results in reasoning, coding, multilingual tasks, long-context handling, honesty, and image processing. See the [Claude 4 blog post](http://www.anthropic.com/news/claude-4) for more information.\n- **Engaging responses**: Claude models are ideal for applications that require rich, human-like interactions.\n\n    - If you prefer more concise responses, you can adjust your prompts to guide the model toward the desired output length. Refer to our [prompt engineering guides](/docs/en/build-with-claude/prompt-engineering) for details.\n    - For specific Claude 4 prompting best practices, see our [Claude 4 best practices guide](/docs/en/build-with-claude/prompt-engineering/claude-4-best-practices).\n- **Output quality**: When migrating from previous model generations to Claude 4, you may notice larger improvements in overall performance.\n\n## Migrating to Claude 4.5\n\nIf you're currently using Claude 3 models, we recommend migrating to Claude 4.5 to take advantage of improved intelligence and enhanced capabilities. For detailed migration instructions, see [Migrating to Claude 4.5](/docs/en/about-claude/models/migrating-to-claude-4).\n\n## Get started with Claude\n\nIf you're ready to start exploring what Claude can do for you, let's dive in! Whether you're a developer looking to integrate Claude into your applications or a user wanting to experience the power of AI firsthand, we've got you covered.\n\n<Note>Looking to chat with Claude? Visit [claude.ai](http://www.claude.ai)!</Note>\n\n<CardGroup cols={3}>\n  <Card title=\"Intro to Claude\" icon=\"check\" href=\"/docs/en/intro\">\n    Explore Claude's capabilities and development flow.\n  </Card>\n  <Card title=\"Quickstart\" icon=\"lightning\" href=\"/docs/en/get-started\">\n    Learn how to make your first API call in minutes.\n  </Card>\n  <Card title=\"Claude Console\" icon=\"code\" href=\"/\">\n    Craft and test powerful prompts directly in your browser.\n  </Card>\n</CardGroup>\n\nIf you have any questions or need assistance, don't hesitate to reach out to our [support team](https://support.claude.com/) or consult the [Discord community](https://www.anthropic.com/discord)."}]},"uuid":"3e1c1e24-1b2d-4ee7-8288-6963bd9e4b23","timestamp":"2025-12-13T02:56:54.714Z","toolUseResult":{"bytes":10572,"code":200,"codeText":"OK","result":"# Models overview\n\nClaude is a family of state-of-the-art large language models developed by Anthropic. This guide introduces our models and compares their performance.\n\n---\n\n## Choosing a model\n\nIf you're unsure which model to use, we recommend starting with **Claude Sonnet 4.5**. It offers the best balance of intelligence, speed, and cost for most use cases, with exceptional performance in coding and agentic tasks.\n\nAll current Claude models support text and image input, text output, multilingual capabilities, and vision. Models are available via the Anthropic API, AWS Bedrock, and Google Vertex AI.\n\nOnce you've picked a model, [learn how to make your first API call](/docs/en/get-started).\n\n### Latest models comparison\n\n| Feature | Claude Sonnet 4.5 | Claude Haiku 4.5 | Claude Opus 4.5 |\n|:--------|:------------------|:-----------------|:----------------|\n| **Description** | Our smart model for complex agents and coding | Our fastest model with near-frontier intelligence | Premium model combining maximum intelligence with practical performance |\n| **Claude API ID** | claude-sonnet-4-5-20250929 | claude-haiku-4-5-20251001 | claude-opus-4-5-20251101 |\n| **Claude API alias**<sup>1</sup> | claude-sonnet-4-5 | claude-haiku-4-5 | claude-opus-4-5 |\n| **AWS Bedrock ID** | anthropic.claude-sonnet-4-5-20250929-v1:0 | anthropic.claude-haiku-4-5-20251001-v1:0 | anthropic.claude-opus-4-5-20251101-v1:0 |\n| **GCP Vertex AI ID** | claude-sonnet-4-5@20250929 | claude-haiku-4-5@20251001 | claude-opus-4-5@20251101 |\n| **Pricing**<sup>2</sup> | \\$3 / input MTok<br/>\\$15 / output MTok | \\$1 / input MTok<br/>\\$5 / output MTok | \\$5 / input MTok<br/>\\$25 / output MTok |\n| **[Extended thinking](/docs/en/build-with-claude/extended-thinking)** | Yes | Yes | Yes |\n| **[Priority Tier](/docs/en/api/service-tiers)** | Yes | Yes | Yes |\n| **Comparative latency** | Fast | Fastest | Moderate |\n| **Context window** | <Tooltip tooltipContent=\"~150K words \\ ~680K unicode characters\">200K tokens</Tooltip> / <br/> <Tooltip tooltipContent=\"~750K words \\ ~3.4M unicode characters\">1M tokens</Tooltip> (beta)<sup>3</sup> | <Tooltip tooltipContent=\"~150K words \\ ~680K unicode characters\">200K tokens</Tooltip> | <Tooltip tooltipContent=\"~150K words \\ ~680K unicode characters\">200K tokens</Tooltip> |\n| **Max output** | 64K tokens | 64K tokens | 64K tokens |\n| **Reliable knowledge cutoff** | Jan 2025<sup>4</sup> | Feb 2025 | May 2025<sup>4</sup> |\n| **Training data cutoff** | Jul 2025 | Jul 2025 | Aug 2025 |\n\n_<sup>1 - Aliases automatically point to the most recent model snapshot. When we release new model snapshots, we migrate aliases to point to the newest version of a model, typically within a week of the new release. While aliases are useful for experimentation, we recommend using specific model versions (e.g., `claude-sonnet-4-5-20250929`) in production applications to ensure consistent behavior.</sup>_\n\n_<sup>2 - See our [pricing page](/docs/en/about-claude/pricing) for complete pricing information including batch API discounts, prompt caching rates, extended thinking costs, and vision processing fees.</sup>_\n\n_<sup>3 - Claude Sonnet 4.5 supports a [1M token context window](/docs/en/build-with-claude/context-windows#1m-token-context-window) when using the `context-1m-2025-08-07` beta header. [Long context pricing](/docs/en/about-claude/pricing#long-context-pricing) applies to requests exceeding 200K tokens.</sup>_\n\n_<sup>4 - **Reliable knowledge cutoff** indicates the date through which a model's knowledge is most extensive and reliable. **Training data cutoff** is the broader date range of training data used. For example, Claude Sonnet 4.5 was trained on publicly available information through July 2025, but its knowledge is most extensive and reliable through January 2025. For more information, see [Anthropic's Transparency Hub](https://www.anthropic.com/transparency).</sup>_\n\n<Note>Models with the same snapshot date (e.g., 20240620) are identical across all platforms and do not change. The snapshot date in the model name ensures consistency and allows developers to rely on stable performance across different environments.</Note>\n\n<Note>Starting with **Claude Sonnet 4.5 and all future models**, AWS Bedrock and Google Vertex AI offer two endpoint types: **global endpoints** (dynamic routing for maximum availability) and **regional endpoints** (guaranteed data routing through specific geographic regions). For more information, see the [third-party platform pricing section](/docs/en/about-claude/pricing#third-party-platform-pricing).</Note>\n\n<section title=\"Legacy models\">\n\nThe following models are still available but we recommend migrating to current models for improved performance:\n\n| Feature | Claude Opus 4.1 | Claude Sonnet 4 | Claude Sonnet 3.7 | Claude Opus 4 | Claude Haiku 3.5 | Claude Haiku 3 |\n|:--------|:----------------|:----------------|:------------------|:--------------|:-----------------|:---------------|\n| **Claude API ID** | claude-opus-4-1-20250805 | claude-sonnet-4-20250514 | claude-3-7-sonnet-20250219 | claude-opus-4-20250514 | claude-3-5-haiku-20241022 | claude-3-haiku-20240307 |\n| **Claude API alias** | claude-opus-4-1 | claude-sonnet-4-0 | claude-3-7-sonnet-latest | claude-opus-4-0 | claude-3-5-haiku-latest | — |\n| **AWS Bedrock ID** | anthropic.claude-opus-4-1-20250805-v1:0 | anthropic.claude-sonnet-4-20250514-v1:0 | anthropic.claude-3-7-sonnet-20250219-v1:0 | anthropic.claude-opus-4-20250514-v1:0 | anthropic.claude-3-5-haiku-20241022-v1:0 | anthropic.claude-3-haiku-20240307-v1:0 |\n| **GCP Vertex AI ID** | claude-opus-4-1@20250805 | claude-sonnet-4@20250514 | claude-3-7-sonnet@20250219 | claude-opus-4@20250514 | claude-3-5-haiku@20241022 | claude-3-haiku@20240307 |\n| **Pricing** | \\$15 / input MTok<br/>\\$75 / output MTok | \\$3 / input MTok<br/>\\$15 / output MTok | \\$3 / input MTok<br/>\\$15 / output MTok | \\$15 / input MTok<br/>\\$75 / output MTok | \\$0.80 / input MTok<br/>\\$4 / output MTok | \\$0.25 / input MTok<br/>\\$1.25 / output MTok |\n| **[Extended thinking](/docs/en/build-with-claude/extended-thinking)** | Yes | Yes | Yes | Yes | No | No |\n| **[Priority Tier](/docs/en/api/service-tiers)** | Yes | Yes | Yes | Yes | Yes | No |\n| **Comparative latency** | Moderate | Fast | Fast | Moderate | Fastest | Fast |\n| **Context window** | <Tooltip tooltipContent=\"~150K words \\ ~680K unicode characters\">200K tokens</Tooltip> | <Tooltip tooltipContent=\"~150K words \\ ~680K unicode characters\">200K tokens</Tooltip> / <br/> <Tooltip tooltipContent=\"~750K words \\ ~3.4M unicode characters\">1M tokens</Tooltip> (beta)<sup>1</sup> | <Tooltip tooltipContent=\"~150K words \\ ~680K unicode characters\">200K tokens</Tooltip> | <Tooltip tooltipContent=\"~150K words \\ ~680K unicode characters\">200K tokens</Tooltip> | <Tooltip tooltipContent=\"~150K words \\ ~215K unicode characters\">200K tokens</Tooltip> | <Tooltip tooltipContent=\"~150K words \\ ~680K unicode characters\">200K tokens</Tooltip> |\n| **Max output** | 32K tokens | 64K tokens | 64K tokens / 128K tokens (beta)<sup>4</sup> | 32K tokens | 8K tokens | 4K tokens |\n| **Reliable knowledge cutoff** | Jan 2025<sup>2</sup> | Jan 2025<sup>2</sup> | Oct 2024<sup>2</sup> | Jan 2025<sup>2</sup> | <sup>3</sup> | <sup>3</sup> |\n| **Training data cutoff** | Mar 2025 | Mar 2025 | Nov 2024 | Mar 2025 | Jul 2024 | Aug 2023 |\n\n_<sup>1 - Claude Sonnet 4 supports a [1M token context window](/docs/en/build-with-claude/context-windows#1m-token-context-window) when using the `context-1m-2025-08-07` beta header. [Long context pricing](/docs/en/about-claude/pricing#long-context-pricing) applies to requests exceeding 200K tokens.</sup>_\n\n_<sup>2 - **Reliable knowledge cutoff** indicates the date through which a model's knowledge is most extensive and reliable. **Training data cutoff** is the broader date range of training data used.</sup>_\n\n_<sup>3 - Some Haiku models have a single training data cutoff date.</sup>_\n\n_<sup>4 - Include the beta header `output-128k-2025-02-19` in your API request to increase the maximum output token length to 128K tokens for Claude Sonnet 3.7. We strongly suggest using our [streaming Messages API](/docs/en/build-with-claude/streaming) to avoid timeouts when generating longer outputs. See our guidance on [long requests](/docs/en/api/errors#long-requests) for more details.</sup>_\n\n</section>\n\n## Prompt and output performance\n\nClaude 4 models excel in:\n- **Performance**: Top-tier results in reasoning, coding, multilingual tasks, long-context handling, honesty, and image processing. See the [Claude 4 blog post](http://www.anthropic.com/news/claude-4) for more information.\n- **Engaging responses**: Claude models are ideal for applications that require rich, human-like interactions.\n\n    - If you prefer more concise responses, you can adjust your prompts to guide the model toward the desired output length. Refer to our [prompt engineering guides](/docs/en/build-with-claude/prompt-engineering) for details.\n    - For specific Claude 4 prompting best practices, see our [Claude 4 best practices guide](/docs/en/build-with-claude/prompt-engineering/claude-4-best-practices).\n- **Output quality**: When migrating from previous model generations to Claude 4, you may notice larger improvements in overall performance.\n\n## Migrating to Claude 4.5\n\nIf you're currently using Claude 3 models, we recommend migrating to Claude 4.5 to take advantage of improved intelligence and enhanced capabilities. For detailed migration instructions, see [Migrating to Claude 4.5](/docs/en/about-claude/models/migrating-to-claude-4).\n\n## Get started with Claude\n\nIf you're ready to start exploring what Claude can do for you, let's dive in! Whether you're a developer looking to integrate Claude into your applications or a user wanting to experience the power of AI firsthand, we've got you covered.\n\n<Note>Looking to chat with Claude? Visit [claude.ai](http://www.claude.ai)!</Note>\n\n<CardGroup cols={3}>\n  <Card title=\"Intro to Claude\" icon=\"check\" href=\"/docs/en/intro\">\n    Explore Claude's capabilities and development flow.\n  </Card>\n  <Card title=\"Quickstart\" icon=\"lightning\" href=\"/docs/en/get-started\">\n    Learn how to make your first API call in minutes.\n  </Card>\n  <Card title=\"Claude Console\" icon=\"code\" href=\"/\">\n    Craft and test powerful prompts directly in your browser.\n  </Card>\n</CardGroup>\n\nIf you have any questions or need assistance, don't hesitate to reach out to our [support team](https://support.claude.com/) or consult the [Discord community](https://www.anthropic.com/discord).","durationMs":1901,"url":"https://platform.claude.com/docs/en/about-claude/models"}}
{"parentUuid":"3e1c1e24-1b2d-4ee7-8288-6963bd9e4b23","isSidechain":false,"userType":"external","cwd":"/Users/yunchang/Documents/GitHub/gemeng2","sessionId":"4ece2923-b68d-44f9-80e5-03967c577bcd","version":"2.0.69","gitBranch":"main","message":{"model":"claude-opus-4-5-20251101","id":"msg_01EaUhMEunTrGM5zBKsRvrEK","type":"message","role":"assistant","content":[{"type":"thinking","thinking":"很好，我找到了详细的模型信息。让我总结一下：\n\n1. Claude Opus 4.5 (claude-opus-4-5-20251101) - 支持 Extended Thinking\n2. Claude Sonnet 4.5 (claude-sonnet-4-5-20250929) - 支持 Extended Thinking\n3. Claude Haiku 4.5 (claude-haiku-4-5-20251001) - 支持 Extended Thinking\n\nExtended Thinking 是一个功能，可以在 API 调用中启用，而不是单独的模型。所以没有类似 \"claude-opus-4-5-thinking\" 这样的独立模型 ID。\n\nExtended Thinking 需要通过 API 参数来启用，而不是使用不同的模型。让我查看更多关于如何启用 extended thinking 的信息。","signature":"EowGCkYIChgCKkAR9JfIl3LUoGdnk0gqHQG7GdrmInJ3z1Wx8pHgtfp42wC9xqjsWlxnkIFSKg44TyeVcAr3GVPOrkxQTBVbugZEEgz1yw27MltWGAyyLK8aDH/GReK82W0NIsJ+3yIw4PXBOveAvWQ15QLoMbCfaPToTzpaYIAsnvyoYD8I+NdqEfgE6yX6gHLU+gpq3HNdKvMEv2RBIjws+vrCxIJ7NXwxnT63QeWCyHh48/0X+SyzPxFYZT6Oh0pWNkQuM8VI9tkACwIWmCDq96hlpygJOJaptF5B9feYbcnaXlNbl+ozNdk/XH7v7afeCQBpqYtut3ngJ+aT7P50rs65vvKQYEGb7gz5aD+g0DBk9WfxXhCcVL2+9k38ElcekTn62DsKQqsuzG7SXUrECHHNjgTKtlzgJlUK4Unmeb2E+HqWFNg4n2jr1mY4cH+aQmrkxAMBj8t6CmzGQXUZpFJzGdCkt/92kr4fBccrBQ8lRPUamvHt/ez61x85sBS9rNvkO9jh2LKT8DE/Dkc7Iufek0hTE+izjBgCPNrZ7rpjx8nreB7i/oO6gQdP73V6QzusNURnLhBZ2nkvd6KE7WVHz9eXZt+KP0YGPetVXc04sTqjimfZNKhBrdLYQ7X7YQw9GKIsrysJaPXUBGimnhb7WCEu0uyxozTXoIHMgvNW65hQ0Fd71qr3fiLA//7vyj046VDyEoiTHsmHDWGsoHbm97ZGTRwEtnN4O8CwkQyJa6gvW0C3RJC9+8bjKbcTfqw+GWjwsqU926UAUQuZcmVtanRv8htJnpa4zqlABr281/1xYs8UOHeTiuI6cejdEnEFVRd4eBNcoluCPRUMjlImWNceCmwr/+n90m6OLxTfmIAuElXx9oQ+jYB/xRqVJkLabckiI5eADNBw41i0GGGLS+YSMQyCmW8pTxmmJvy4DBVOGSfIs1P9RhfTRSBnzPdobGj3UXiCtGz388Yn9BKVNYogL7d+EGmUJeAWxZE+aGl6sYiDMDBOX6gaSpG/V9oDHGrUs/XMCDLyGAE="}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":3417,"cache_read_input_tokens":29339,"cache_creation":{"ephemeral_5m_input_tokens":3417,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"type":"assistant","uuid":"05375dd7-ba55-4c63-aa6e-6d8e0a2bb813","timestamp":"2025-12-13T02:57:03.483Z"}
{"parentUuid":"05375dd7-ba55-4c63-aa6e-6d8e0a2bb813","isSidechain":false,"userType":"external","cwd":"/Users/yunchang/Documents/GitHub/gemeng2","sessionId":"4ece2923-b68d-44f9-80e5-03967c577bcd","version":"2.0.69","gitBranch":"main","message":{"model":"claude-opus-4-5-20251101","id":"msg_01EaUhMEunTrGM5zBKsRvrEK","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01QhppJEgRd64qYgtKtdyALF","name":"WebFetch","input":{"url":"https://platform.claude.com/docs/en/build-with-claude/extended-thinking","prompt":"查找如何在API中启用extended thinking，具体的参数和配置方式"}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":3417,"cache_read_input_tokens":29339,"cache_creation":{"ephemeral_5m_input_tokens":3417,"ephemeral_1h_input_tokens":0},"output_tokens":351,"service_tier":"standard"},"context_management":{"applied_edits":[]}},"type":"assistant","uuid":"3eef4b52-7ebf-4cf0-910f-d1e1a06e9eac","timestamp":"2025-12-13T02:57:04.526Z"}
{"parentUuid":"3eef4b52-7ebf-4cf0-910f-d1e1a06e9eac","isSidechain":false,"userType":"external","cwd":"/Users/yunchang/Documents/GitHub/gemeng2","sessionId":"4ece2923-b68d-44f9-80e5-03967c577bcd","version":"2.0.69","gitBranch":"main","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01QhppJEgRd64qYgtKtdyALF","type":"tool_result","content":"# Building with extended thinking\n\n---\n\nExtended thinking gives Claude enhanced reasoning capabilities for complex tasks, while providing varying levels of transparency into its step-by-step thought process before it delivers its final answer.\n\n## Supported models\n\nExtended thinking is supported in the following models:\n\n- Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)\n- Claude Sonnet 4 (`claude-sonnet-4-20250514`)\n- Claude Sonnet 3.7 (`claude-3-7-sonnet-20250219`) ([deprecated](/docs/en/about-claude/model-deprecations))\n- Claude Haiku 4.5 (`claude-haiku-4-5-20251001`)\n- Claude Opus 4.5 (`claude-opus-4-5-20251101`)\n- Claude Opus 4.1 (`claude-opus-4-1-20250805`)\n- Claude Opus 4 (`claude-opus-4-20250514`)\n\n<Note>\nAPI behavior differs across Claude Sonnet 3.7 and Claude 4 models, but the API shapes remain exactly the same.\n\nFor more information, see [Differences in thinking across model versions](#differences-in-thinking-across-model-versions).\n</Note>\n\n## How extended thinking works\n\nWhen extended thinking is turned on, Claude creates `thinking` content blocks where it outputs its internal reasoning. Claude incorporates insights from this reasoning before crafting a final response.\n\nThe API response will include `thinking` content blocks, followed by `text` content blocks.\n\nHere's an example of the default response format:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"thinking\",\n      \"thinking\": \"Let me analyze this step by step...\",\n      \"signature\": \"WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL....\"\n    },\n    {\n      \"type\": \"text\",\n      \"text\": \"Based on my analysis...\"\n    }\n  ]\n}\n```\n\nFor more information about the response format of extended thinking, see the [Messages API Reference](/docs/en/api/messages).\n\n## How to use extended thinking\n\nHere is an example of using extended thinking in the Messages API:\n\n<CodeGroup>\n```bash Shell\ncurl https://api.anthropic.com/v1/messages \\\n     --header \"x-api-key: $ANTHROPIC_API_KEY\" \\\n     --header \"anthropic-version: 2023-06-01\" \\\n     --header \"content-type: application/json\" \\\n     --data \\\n'{\n    \"model\": \"claude-sonnet-4-5\",\n    \"max_tokens\": 16000,\n    \"thinking\": {\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"Are there an infinite number of prime numbers such that n mod 4 == 3?\"\n        }\n    ]\n}'\n```\n\n```python Python\nimport anthropic\n\nclient = anthropic.Anthropic()\n\nresponse = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Are there an infinite number of prime numbers such that n mod 4 == 3?\"\n    }]\n)\n\n# The response will contain summarized thinking blocks and text blocks\nfor block in response.content:\n    if block.type == \"thinking\":\n        print(f\"\\nThinking summary: {block.thinking}\")\n    elif block.type == \"text\":\n        print(f\"\\nResponse: {block.text}\")\n```\n\n```typescript TypeScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst client = new Anthropic();\n\nconst response = await client.messages.create({\n  model: \"claude-sonnet-4-5\",\n  max_tokens: 16000,\n  thinking: {\n    type: \"enabled\",\n    budget_tokens: 10000\n  },\n  messages: [{\n    role: \"user\",\n    content: \"Are there an infinite number of prime numbers such that n mod 4 == 3?\"\n  }]\n});\n\n// The response will contain summarized thinking blocks and text blocks\nfor (const block of response.content) {\n  if (block.type === \"thinking\") {\n    console.log(`\\nThinking summary: ${block.thinking}`);\n  } else if (block.type === \"text\") {\n    console.log(`\\nResponse: ${block.text}`);\n  }\n}\n```\n\n```java Java\nimport com.anthropic.client.AnthropicClient;\nimport com.anthropic.client.okhttp.AnthropicOkHttpClient;\nimport com.anthropic.models.beta.messages.*;\nimport com.anthropic.models.beta.messages.MessageCreateParams;\nimport com.anthropic.models.messages.*;\n\npublic class SimpleThinkingExample {\n    public static void main(String[] args) {\n        AnthropicClient client = AnthropicOkHttpClient.fromEnv();\n\n        BetaMessage response = client.beta().messages().create(\n                MessageCreateParams.builder()\n                        .model(Model.CLAUDE_OPUS_4_0)\n                        .maxTokens(16000)\n                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())\n                        .addUserMessage(\"Are there an infinite number of prime numbers such that n mod 4 == 3?\")\n                        .build()\n        );\n\n        System.out.println(response);\n    }\n}\n```\n\n</CodeGroup>\n\nTo turn on extended thinking, add a `thinking` object, with the `type` parameter set to `enabled` and the `budget_tokens` to a specified token budget for extended thinking.\n\nThe `budget_tokens` parameter determines the maximum number of tokens Claude is allowed to use for its internal reasoning process. In Claude 4 models, this limit applies to full thinking tokens, and not to [the summarized output](#summarized-thinking). Larger budgets can improve response quality by enabling more thorough analysis for complex problems, although Claude may not use the entire budget allocated, especially at ranges above 32k.\n\n`budget_tokens` must be set to a value less than `max_tokens`. However, when using [interleaved thinking with tools](#interleaved-thinking), you can exceed this limit as the token limit becomes your entire context window (200k tokens). \n\n### Summarized thinking\n\nWith extended thinking enabled, the Messages API for Claude 4 models returns a summary of Claude's full thinking process. Summarized thinking provides the full intelligence benefits of extended thinking, while preventing misuse.\n\nHere are some important considerations for summarized thinking:\n\n- You're charged for the full thinking tokens generated by the original request, not the summary tokens.\n- The billed output token count will **not match** the count of tokens you see in the response.\n- The first few lines of thinking output are more verbose, providing detailed reasoning that's particularly helpful for prompt engineering purposes. \n- As Anthropic seeks to improve the extended thinking feature, summarization behavior is subject to change.\n- Summarization preserves the key ideas of Claude's thinking process with minimal added latency, enabling a streamable user experience and easy migration from Claude Sonnet 3.7 to Claude 4 models.\n- Summarization is processed by a different model than the one you target in your requests. The thinking model does not see the summarized output.\n\n<Note>\nClaude Sonnet 3.7 continues to return full thinking output.\n\nIn rare cases where you need access to full thinking output for Claude 4 models, [contact our sales team](mailto:sales@anthropic.com).\n</Note>\n\n### Streaming thinking\n\nYou can stream extended thinking responses using [server-sent events (SSE)](https://developer.mozilla.org/en-US/Web/API/Server-sent%5Fevents/Using%5Fserver-sent%5Fevents).\n\nWhen streaming is enabled for extended thinking, you receive thinking content via `thinking_delta` events.\n\nFor more documention on streaming via the Messages API, see [Streaming Messages](/docs/en/build-with-claude/streaming).\n\nHere's how to handle streaming with thinking:\n\n<CodeGroup>\n```bash Shell\ncurl https://api.anthropic.com/v1/messages \\\n     --header \"x-api-key: $ANTHROPIC_API_KEY\" \\\n     --header \"anthropic-version: 2023-06-01\" \\\n     --header \"content-type: application/json\" \\\n     --data \\\n'{\n    \"model\": \"claude-sonnet-4-5\",\n    \"max_tokens\": 16000,\n    \"stream\": true,\n    \"thinking\": {\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"What is 27 * 453?\"\n        }\n    ]\n}'\n```\n\n```python Python\nimport anthropic\n\nclient = anthropic.Anthropic()\n\nwith client.messages.stream(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=16000,\n    thinking={\"type\": \"enabled\", \"budget_tokens\": 10000},\n    messages=[{\"role\": \"user\", \"content\": \"What is 27 * 453?\"}],\n) as stream:\n    thinking_started = False\n    response_started = False\n\n    for event in stream:\n        if event.type == \"content_block_start\":\n            print(f\"\\nStarting {event.content_block.type} block...\")\n            # Reset flags for each new block\n            thinking_started = False\n            response_started = False\n        elif event.type == \"content_block_delta\":\n            if event.delta.type == \"thinking_delta\":\n                if not thinking_started:\n                    print(\"Thinking: \", end=\"\", flush=True)\n                    thinking_started = True\n                print(event.delta.thinking, end=\"\", flush=True)\n            elif event.delta.type == \"text_delta\":\n                if not response_started:\n                    print(\"Response: \", end=\"\", flush=True)\n                    response_started = True\n                print(event.delta.text, end=\"\", flush=True)\n        elif event.type == \"content_block_stop\":\n            print(\"\\nBlock complete.\")\n```\n\n```typescript TypeScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst client = new Anthropic();\n\nconst stream = await client.messages.stream({\n  model: \"claude-sonnet-4-5\",\n  max_tokens: 16000,\n  thinking: {\n    type: \"enabled\",\n    budget_tokens: 10000\n  },\n  messages: [{\n    role: \"user\",\n    content: \"What is 27 * 453?\"\n  }]\n});\n\nlet thinkingStarted = false;\nlet responseStarted = false;\n\nfor await (const event of stream) {\n  if (event.type === 'content_block_start') {\n    console.log(`\\nStarting ${event.content_block.type} block...`);\n    // Reset flags for each new block\n    thinkingStarted = false;\n    responseStarted = false;\n  } else if (event.type === 'content_block_delta') {\n    if (event.delta.type === 'thinking_delta') {\n      if (!thinkingStarted) {\n        process.stdout.write('Thinking: ');\n        thinkingStarted = true;\n      }\n      process.stdout.write(event.delta.thinking);\n    } else if (event.delta.type === 'text_delta') {\n      if (!responseStarted) {\n        process.stdout.write('Response: ');\n        responseStarted = true;\n      }\n      process.stdout.write(event.delta.text);\n    }\n  } else if (event.type === 'content_block_stop') {\n    console.log('\\nBlock complete.');\n  }\n}\n```\n\n```java Java\nimport com.anthropic.client.AnthropicClient;\nimport com.anthropic.client.okhttp.AnthropicOkHttpClient;\nimport com.anthropic.core.http.StreamResponse;\nimport com.anthropic.models.beta.messages.MessageCreateParams;\nimport com.anthropic.models.beta.messages.BetaRawMessageStreamEvent;\nimport com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;\nimport com.anthropic.models.messages.Model;\n\npublic class SimpleThinkingStreamingExample {\n    private static boolean thinkingStarted = false;\n    private static boolean responseStarted = false;\n    \n    public static void main(String[] args) {\n        AnthropicClient client = AnthropicOkHttpClient.fromEnv();\n\n        MessageCreateParams createParams = MessageCreateParams.builder()\n                .model(Model.CLAUDE_OPUS_4_0)\n                .maxTokens(16000)\n                .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())\n                .addUserMessage(\"What is 27 * 453?\")\n                .build();\n\n        try (StreamResponse<BetaRawMessageStreamEvent> streamResponse =\n                     client.beta().messages().createStreaming(createParams)) {\n            streamResponse.stream()\n                    .forEach(event -> {\n                        if (event.isContentBlockStart()) {\n                            System.out.printf(\"\\nStarting %s block...%n\",\n                                    event.asContentBlockStart()._type());\n                            // Reset flags for each new block\n                            thinkingStarted = false;\n                            responseStarted = false;\n                        } else if (event.isContentBlockDelta()) {\n                            var delta = event.asContentBlockDelta().delta();\n                            if (delta.isBetaThinking()) {\n                                if (!thinkingStarted) {\n                                    System.out.print(\"Thinking: \");\n                                    thinkingStarted = true;\n                                }\n                                System.out.print(delta.asBetaThinking().thinking());\n                                System.out.flush();\n                            } else if (delta.isBetaText()) {\n                                if (!responseStarted) {\n                                    System.out.print(\"Response: \");\n                                    responseStarted = true;\n                                }\n                                System.out.print(delta.asBetaText().text());\n                                System.out.flush();\n                            }\n                        } else if (event.isContentBlockStop()) {\n                            System.out.println(\"\\nBlock complete.\");\n                        }\n                    });\n        }\n    }\n}\n```\n\n</CodeGroup>\n\n<TryInConsoleButton userPrompt=\"What is 27 * 453?\" thinkingBudgetTokens={16000}>\n  Try in Console\n</TryInConsoleButton>\n  \n\nExample streaming output:\n```json\nevent: message_start\ndata: {\"type\": \"message_start\", \"message\": {\"id\": \"msg_01...\", \"type\": \"message\", \"role\": \"assistant\", \"content\": [], \"model\": \"claude-sonnet-4-5\", \"stop_reason\": null, \"stop_sequence\": null}}\n\nevent: content_block_start\ndata: {\"type\": \"content_block_start\", \"index\": 0, \"content_block\": {\"type\": \"thinking\", \"thinking\": \"\"}}\n\nevent: content_block_delta\ndata: {\"type\": \"content_block_delta\", \"index\": 0, \"delta\": {\"type\": \"thinking_delta\", \"thinking\": \"Let me solve this step by step:\\n\\n1. First break down 27 * 453\"}}\n\nevent: content_block_delta\ndata: {\"type\": \"content_block_delta\", \"index\": 0, \"delta\": {\"type\": \"thinking_delta\", \"thinking\": \"\\n2. 453 = 400 + 50 + 3\"}}\n\n// Additional thinking deltas...\n\nevent: content_block_delta\ndata: {\"type\": \"content_block_delta\", \"index\": 0, \"delta\": {\"type\": \"signature_delta\", \"signature\": \"EqQBCgIYAhIM1gbcDa9GJwZA2b3hGgxBdjrkzLoky3dl1pkiMOYds...\"}}\n\nevent: content_block_stop\ndata: {\"type\": \"content_block_stop\", \"index\": 0}\n\nevent: content_block_start\ndata: {\"type\": \"content_block_start\", \"index\": 1, \"content_block\": {\"type\": \"text\", \"text\": \"\"}}\n\nevent: content_block_delta\ndata: {\"type\": \"content_block_delta\", \"index\": 1, \"delta\": {\"type\": \"text_delta\", \"text\": \"27 * 453 = 12,231\"}}\n\n// Additional text deltas...\n\nevent: content_block_stop\ndata: {\"type\": \"content_block_stop\", \"index\": 1}\n\nevent: message_delta\ndata: {\"type\": \"message_delta\", \"delta\": {\"stop_reason\": \"end_turn\", \"stop_sequence\": null}}\n\nevent: message_stop\ndata: {\"type\": \"message_stop\"}\n```\n\n<Note>\nWhen using streaming with thinking enabled, you might notice that text sometimes arrives in larger chunks alternating with smaller, token-by-token delivery. This is expected behavior, especially for thinking content.\n\nThe streaming system needs to process content in batches for optimal performance, which can result in this \"chunky\" delivery pattern, with possible delays between streaming events. We're continuously working to improve this experience, with future updates focused on making thinking content stream more smoothly.\n</Note>\n\n## Extended thinking with tool use\n\nExtended thinking can be used alongside [tool use](/docs/en/agents-and-tools/tool-use/overview), allowing Claude to reason through tool selection and results processing.\n\nWhen using extended thinking with tool use, be aware of the following limitations:\n\n1. **Tool choice limitation**: Tool use with thinking only supports `tool_choice: {\"type\": \"auto\"}` (the default) or `tool_choice: {\"type\": \"none\"}`. Using `tool_choice: {\"type\": \"any\"}` or `tool_choice: {\"type\": \"tool\", \"name\": \"...\"}` will result in an error because these options force tool use, which is incompatible with extended thinking.\n\n2. **Preserving thinking blocks**: During tool use, you must pass `thinking` blocks back to the API for the last assistant message. Include the complete unmodified block back to the API to maintain reasoning continuity.\n\n### Toggling thinking modes in conversations\n\nYou cannot toggle thinking in the middle of an assistant turn, including during tool use loops. The entire assistant turn must operate in a single thinking mode: \n\n- **If thinking is enabled**, the final assistant turn must start with a thinking block.\n- **If thinking is disabled**, the final assistant turn must not contain any thinking blocks\n\nFrom the model's perspective, **tool use loops are part of the assistant turn**. An assistant turn doesn't complete until Claude finishes its full response, which may include multiple tool calls and results.\n\nFor example, this sequence is all part of a **single assistant turn**:\n```\nUser: \"What's the weather in Paris?\"\nAssistant: [thinking] + [tool_use: get_weather]\nUser: [tool_result: \"20°C, sunny\"]\nAssistant: [text: \"The weather in Paris is 20°C and sunny\"]\n```\n\nEven though there are multiple API messages, the tool use loop is conceptually part of one continuous assistant response.\n\n#### Common error scenarios\n\nYou might encounter this error:\n```\nExpected `thinking` or `redacted_thinking`, but found `tool_use`.\nWhen `thinking` is enabled, a final `assistant` message must start\nwith a thinking block (preceding the lastmost set of `tool_use` and\n`tool_result` blocks).\n```\n\nThis typically occurs when:\n1. You had thinking **disabled** during a tool use sequence\n2. You want to enable thinking again\n3. Your last assistant message contains tool use blocks but no thinking block\n\n#### Practical guidance\n\n**✗ Invalid: Toggling thinking immediately after tool use**\n```\nUser: \"What's the weather?\"\nAssistant: [tool_use] (thinking disabled)\nUser: [tool_result]\n// Cannot enable thinking here - still in the same assistant turn\n```\n\n**✓ Valid: Complete the assistant turn first**\n```\nUser: \"What's the weather?\"\nAssistant: [tool_use] (thinking disabled)\nUser: [tool_result]\nAssistant: [text: \"It's sunny\"] \nUser: \"What about tomorrow?\" (thinking disabled)\nAssistant: [thinking] + [text: \"...\"] (thinking enabled - new turn)\n```\n\n**Best practice**: Plan your thinking strategy at the start of each turn rather than trying to toggle mid-turn.\n\n<Note>\nToggling thinking modes also invalidates prompt caching for message history. For more details, see the [Extended thinking with prompt caching](#extended-thinking-with-prompt-caching) section.\n</Note>\n\n<section title=\"Example: Passing thinking blocks with tool results\">\n\nHere's a practical example showing how to preserve thinking blocks when providing tool results:\n\n<CodeGroup>\n```python Python\nweather_tool = {\n    \"name\": \"get_weather\",\n    \"description\": \"Get current weather for a location\",\n    \"input_schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\"type\": \"string\"}\n        },\n        \"required\": [\"location\"]\n    }\n}\n\n# First request - Claude responds with thinking and tool request\nresponse = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    tools=[weather_tool],\n    messages=[\n        {\"role\": \"user\", \"content\": \"What's the weather in Paris?\"}\n    ]\n)\n```\n\n```typescript TypeScript\nconst weatherTool = {\n  name: \"get_weather\",\n  description: \"Get current weather for a location\",\n  input_schema: {\n    type: \"object\",\n    properties: {\n      location: { type: \"string\" }\n    },\n    required: [\"location\"]\n  }\n};\n\n// First request - Claude responds with thinking and tool request\nconst response = await client.messages.create({\n  model: \"claude-sonnet-4-5\",\n  max_tokens: 16000,\n  thinking: {\n    type: \"enabled\",\n    budget_tokens: 10000\n  },\n  tools: [weatherTool],\n  messages: [\n    { role: \"user\", content: \"What's the weather in Paris?\" }\n  ]\n});\n```\n\n```java Java\nimport java.util.List;\nimport java.util.Map;\n\nimport com.anthropic.client.AnthropicClient;\nimport com.anthropic.client.okhttp.AnthropicOkHttpClient;\nimport com.anthropic.core.JsonValue;\nimport com.anthropic.models.beta.messages.BetaMessage;\nimport com.anthropic.models.beta.messages.MessageCreateParams;\nimport com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;\nimport com.anthropic.models.beta.messages.BetaTool;\nimport com.anthropic.models.beta.messages.BetaTool.InputSchema;\nimport com.anthropic.models.messages.Model;\n\npublic class ThinkingWithToolsExample {\n    public static void main(String[] args) {\n        AnthropicClient client = AnthropicOkHttpClient.fromEnv();\n\n        InputSchema schema = InputSchema.builder()\n                .properties(JsonValue.from(Map.of(\n                        \"location\", Map.of(\"type\", \"string\")\n                )))\n                .putAdditionalProperty(\"required\", JsonValue.from(List.of(\"location\")))\n                .build();\n\n        BetaTool weatherTool = BetaTool.builder()\n                .name(\"get_weather\")\n                .description(\"Get current weather for a location\")\n                .inputSchema(schema)\n                .build();\n\n        BetaMessage response = client.beta().messages().create(\n                MessageCreateParams.builder()\n                        .model(Model.CLAUDE_OPUS_4_0)\n                        .maxTokens(16000)\n                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())\n                        .addTool(weatherTool)\n                        .addUserMessage(\"What's the weather in Paris?\")\n                        .build()\n        );\n\n        System.out.println(response);\n    }\n}\n```\n</CodeGroup>\n\nThe API response will include thinking, text, and tool_use blocks:\n\n```json\n{\n    \"content\": [\n        {\n            \"type\": \"thinking\",\n            \"thinking\": \"The user wants to know the current weather in Paris. I have access to a function `get_weather`...\",\n            \"signature\": \"BDaL4VrbR2Oj0hO4XpJxT28J5TILnCrrUXoKiiNBZW9P+nr8XSj1zuZzAl4egiCCpQNvfyUuFFJP5CncdYZEQPPmLxYsNrcs....\"\n        },\n        {\n            \"type\": \"text\",\n            \"text\": \"I can help you get the current weather information for Paris. Let me check that for you\"\n        },\n        {\n            \"type\": \"tool_use\",\n            \"id\": \"toolu_01CswdEQBMshySk6Y9DFKrfq\",\n            \"name\": \"get_weather\",\n            \"input\": {\n                \"location\": \"Paris\"\n            }\n        }\n    ]\n}\n```\n\nNow let's continue the conversation and use the tool\n\n<CodeGroup>\n```python Python\n# Extract thinking block and tool use block\nthinking_block = next((block for block in response.content\n                      if block.type == 'thinking'), None)\ntool_use_block = next((block for block in response.content\n                      if block.type == 'tool_use'), None)\n\n# Call your actual weather API, here is where your actual API call would go\n# let's pretend this is what we get back\nweather_data = {\"temperature\": 88}\n\n# Second request - Include thinking block and tool result\n# No new thinking blocks will be generated in the response\ncontinuation = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    tools=[weather_tool],\n    messages=[\n        {\"role\": \"user\", \"content\": \"What's the weather in Paris?\"},\n        # notice that the thinking_block is passed in as well as the tool_use_block\n        # if this is not passed in, an error is raised\n        {\"role\": \"assistant\", \"content\": [thinking_block, tool_use_block]},\n        {\"role\": \"user\", \"content\": [{\n            \"type\": \"tool_result\",\n            \"tool_use_id\": tool_use_block.id,\n            \"content\": f\"Current temperature: {weather_data['temperature']}°F\"\n        }]}\n    ]\n)\n```\n\n```typescript TypeScript\n// Extract thinking block and tool use block\nconst thinkingBlock = response.content.find(block =>\n  block.type === 'thinking');\nconst toolUseBlock = response.content.find(block =>\n  block.type === 'tool_use');\n\n// Call your actual weather API, here is where your actual API call would go\n// let's pretend this is what we get back\nconst weatherData = { temperature: 88 };\n\n// Second request - Include thinking block and tool result\n// No new thinking blocks will be generated in the response\nconst continuation = await client.messages.create({\n  model: \"claude-sonnet-4-5\",\n  max_tokens: 16000,\n  thinking: {\n    type: \"enabled\",\n    budget_tokens: 10000\n  },\n  tools: [weatherTool],\n  messages: [\n    { role: \"user\", content: \"What's the weather in Paris?\" },\n    // notice that the thinkingBlock is passed in as well as the toolUseBlock\n    // if this is not passed in, an error is raised\n    { role: \"assistant\", content: [thinkingBlock, toolUseBlock] },\n    { role: \"user\", content: [{\n      type: \"tool_result\",\n      tool_use_id: toolUseBlock.id,\n      content: `Current temperature: ${weatherData.temperature}°F`\n    }]}\n  ]\n});\n```\n\n```java Java\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Optional;\n\nimport com.anthropic.client.AnthropicClient;\nimport com.anthropic.client.okhttp.AnthropicOkHttpClient;\nimport com.anthropic.core.JsonValue;\nimport com.anthropic.models.beta.messages.*;\nimport com.anthropic.models.beta.messages.BetaTool.InputSchema;\nimport com.anthropic.models.messages.Model;\n\npublic class ThinkingToolsResultExample {\n    public static void main(String[] args) {\n        AnthropicClient client = AnthropicOkHttpClient.fromEnv();\n\n        InputSchema schema = InputSchema.builder()\n                .properties(JsonValue.from(Map.of(\n                        \"location\", Map.of(\"type\", \"string\")\n                )))\n                .putAdditionalProperty(\"required\", JsonValue.from(List.of(\"location\")))\n                .build();\n\n        BetaTool weatherTool = BetaTool.builder()\n                .name(\"get_weather\")\n                .description(\"Get current weather for a location\")\n                .inputSchema(schema)\n                .build();\n\n        BetaMessage response = client.beta().messages().create(\n                MessageCreateParams.builder()\n                        .model(Model.CLAUDE_OPUS_4_0)\n                        .maxTokens(16000)\n                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())\n                        .addTool(weatherTool)\n                        .addUserMessage(\"What's the weather in Paris?\")\n                        .build()\n        );\n\n        // Extract thinking block and tool use block\n        Optional<BetaThinkingBlock> thinkingBlockOpt = response.content().stream()\n                .filter(BetaContentBlock::isThinking)\n                .map(BetaContentBlock::asThinking)\n                .findFirst();\n\n        Optional<BetaToolUseBlock> toolUseBlockOpt = response.content().stream()\n                .filter(BetaContentBlock::isToolUse)\n                .map(BetaContentBlock::asToolUse)\n                .findFirst();\n\n        if (thinkingBlockOpt.isPresent() && toolUseBlockOpt.isPresent()) {\n            BetaThinkingBlock thinkingBlock = thinkingBlockOpt.get();\n            BetaToolUseBlock toolUseBlock = toolUseBlockOpt.get();\n\n            // Call your actual weather API, here is where your actual API call would go\n            // let's pretend this is what we get back\n            Map<String, Object> weatherData = Map.of(\"temperature\", 88);\n\n            // Second request - Include thinking block and tool result\n            // No new thinking blocks will be generated in the response\n            BetaMessage continuation = client.beta().messages().create(\n                    MessageCreateParams.builder()\n                            .model(Model.CLAUDE_OPUS_4_0)\n                            .maxTokens(16000)\n                            .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())\n                            .addTool(weatherTool)\n                            .addUserMessage(\"What's the weather in Paris?\")\n                            .addAssistantMessageOfBetaContentBlockParams(\n                                    // notice that the thinkingBlock is passed in as well as the toolUseBlock\n                                    // if this is not passed in, an error is raised\n                                    List.of(\n                                            BetaContentBlockParam.ofThinking(thinkingBlock.toParam()),\n                                            BetaContentBlockParam.ofToolUse(toolUseBlock.toParam())\n                                    )\n                            )\n                            .addUserMessageOfBetaContentBlockParams(List.of(\n                                    BetaContentBlockParam.ofToolResult(\n                                            BetaToolResultBlockParam.builder()\n                                                    .toolUseId(toolUseBlock.id())\n                                                    .content(String.format(\"Current temperature: %d°F\", (Integer)weatherData.get(\"temperature\")))\n                                                    .build()\n                                    )\n                            ))\n                            .build()\n            );\n\n            System.out.println(continuation);\n        }\n    }\n}\n```\n</CodeGroup>\n\nThe API response will now **only** include text\n\n```json\n{\n    \"content\": [\n        {\n            \"type\": \"text\",\n            \"text\": \"Currently in Paris, the temperature is 88°F (31°C)\"\n        }\n    ]\n}\n```\n\n</section>\n\n### Preserving thinking blocks\n\nDuring tool use, you must pass `thinking` blocks back to the API, and you must include the complete unmodified block back to the API. This is critical for maintaining the model's reasoning flow and conversation integrity.\n\n<Tip>\nWhile you can omit `thinking` blocks from prior `assistant` role turns, we suggest always passing back all thinking blocks to the API for any multi-turn conversation. The API will:\n- Automatically filter the provided thinking blocks\n- Use the relevant thinking blocks necessary to preserve the model's reasoning\n- Only bill for the input tokens for the blocks shown to Claude\n</Tip>\n\n<Note>\nWhen toggling thinking modes during a conversation, remember that the entire assistant turn (including tool use loops) must operate in a single thinking mode. For more details, see [Toggling thinking modes in conversations](#toggling-thinking-modes-in-conversations).\n</Note>\n\nWhen Claude invokes tools, it is pausing its construction of a response to await external information. When tool results are returned, Claude will continue building that existing response. This necessitates preserving thinking blocks during tool use, for a couple of reasons:\n\n1. **Reasoning continuity**: The thinking blocks capture Claude's step-by-step reasoning that led to tool requests. When you post tool results, including the original thinking ensures Claude can continue its reasoning from where it left off.\n\n2. **Context maintenance**: While tool results appear as user messages in the API structure, they're part of a continuous reasoning flow. Preserving thinking blocks maintains this conceptual flow across multiple API calls. For more information on context management, see our [guide on context windows](/docs/en/build-with-claude/context-windows).\n\n**Important**: When providing `thinking` blocks, the entire sequence of consecutive `thinking` blocks must match the outputs generated by the model during the original request; you cannot rearrange or modify the sequence of these blocks.\n\n### Interleaved thinking\n\nExtended thinking with tool use in Claude 4 models supports interleaved thinking, which enables Claude to think between tool calls and make more sophisticated reasoning after receiving tool results.\n\nWith interleaved thinking, Claude can:\n- Reason about the results of a tool call before deciding what to do next\n- Chain multiple tool calls with reasoning steps in between\n- Make more nuanced decisions based on intermediate results\n\nTo enable interleaved thinking, add [the beta header](/docs/en/api/beta-headers) `interleaved-thinking-2025-05-14` to your API request.\n\nHere are some important considerations for interleaved thinking:\n- With interleaved thinking, the `budget_tokens` can exceed the `max_tokens` parameter, as it represents the total budget across all thinking blocks within one assistant turn.\n- Interleaved thinking is only supported for [tools used via the Messages API](/docs/en/agents-and-tools/tool-use/overview).\n- Interleaved thinking is supported for Claude 4 models only, with the beta header `interleaved-thinking-2025-05-14`.\n- Direct calls to the Claude API allow you to pass `interleaved-thinking-2025-05-14` in requests to any model, with no effect.\n- On 3rd-party platforms (e.g., [Amazon Bedrock](/docs/en/build-with-claude/claude-on-amazon-bedrock) and [Vertex AI](/docs/en/build-with-claude/claude-on-vertex-ai)), if you pass `interleaved-thinking-2025-05-14` to any model aside from Claude Opus 4.5, Claude Opus 4.1, Opus 4, or Sonnet 4, your request will fail.\n\n<section title=\"Tool use without interleaved thinking\">\n\nWithout interleaved thinking, Claude thinks once at the start of the assistant turn. Subsequent responses after tool results continue without new thinking blocks.\n\n```\nUser: \"What's the total revenue if we sold 150 units at $50 each,\n       and how does this compare to our average monthly revenue?\"\n\nTurn 1: [thinking] \"I need to calculate 150 * $50, then check the database...\"\n        [tool_use: calculator] { \"expression\": \"150 * 50\" }\n  ↓ tool result: \"7500\"\n\nTurn 2: [tool_use: database_query] { \"query\": \"SELECT AVG(revenue)...\" }\n        ↑ no thinking block\n  ↓ tool result: \"5200\"\n\nTurn 3: [text] \"The total revenue is $7,500, which is 44% above your\n        average monthly revenue of $5,200.\"\n        ↑ no thinking block\n```\n\n</section>\n\n<section title=\"Tool use with interleaved thinking\">\n\nWith interleaved thinking enabled, Claude can think after receiving each tool result, allowing it to reason about intermediate results before continuing.\n\n```\nUser: \"What's the total revenue if we sold 150 units at $50 each,\n       and how does this compare to our average monthly revenue?\"\n\nTurn 1: [thinking] \"I need to calculate 150 * $50 first...\"\n        [tool_use: calculator] { \"expression\": \"150 * 50\" }\n  ↓ tool result: \"7500\"\n\nTurn 2: [thinking] \"Got $7,500. Now I should query the database to compare...\"\n        [tool_use: database_query] { \"query\": \"SELECT AVG(revenue)...\" }\n        ↑ thinking after receiving calculator result\n  ↓ tool result: \"5200\"\n\nTurn 3: [thinking] \"$7,500 vs $5,200 average - that's a 44% increase...\"\n        [text] \"The total revenue is $7,500, which is 44% above your\n        average monthly revenue of $5,200.\"\n        ↑ thinking before final answer\n```\n\n</section>\n\n## Extended thinking with prompt caching\n\n[Prompt caching](/docs/en/build-with-claude/prompt-caching) with thinking has several important considerations:\n\n<Tip>\nExtended thinking tasks often take longer than 5 minutes to complete. Consider using the [1-hour cache duration](/docs/en/build-with-claude/prompt-caching#1-hour-cache-duration) to maintain cache hits across longer thinking sessions and multi-step workflows.\n</Tip>\n\n**Thinking block context removal**\n- Thinking blocks from previous turns are removed from context, which can affect cache breakpoints\n- When continuing conversations with tool use, thinking blocks are cached and count as input tokens when read from cache\n- This creates a tradeoff: while thinking blocks don't consume context window space visually, they still count toward your input token usage when cached\n- If thinking becomes disabled, requests will fail if you pass thinking content in the current tool use turn. In other contexts, thinking content passed to the API is simply ignored\n\n**Cache invalidation patterns**\n- Changes to thinking parameters (enabled/disabled or budget allocation) invalidate message cache breakpoints\n- [Interleaved thinking](#interleaved-thinking) amplifies cache invalidation, as thinking blocks can occur between multiple [tool calls](#extended-thinking-with-tool-use)\n- System prompts and tools remain cached despite thinking parameter changes or block removal\n\n<Note>\nWhile thinking blocks are removed for caching and context calculations, they must be preserved when continuing conversations with [tool use](#extended-thinking-with-tool-use), especially with [interleaved thinking](#interleaved-thinking).\n</Note>\n\n### Understanding thinking block caching behavior\n\nWhen using extended thinking with tool use, thinking blocks exhibit specific caching behavior that affects token counting:\n\n**How it works:**\n\n1. Caching only occurs when you make a subsequent request that includes tool results\n2. When the subsequent request is made, the previous conversation history (including thinking blocks) can be cached\n3. These cached thinking blocks count as input tokens in your usage metrics when read from the cache\n4. When a non-tool-result user block is included, all previous thinking blocks are ignored and stripped from context\n\n**Detailed example flow:**\n\n**Request 1:**\n```\nUser: \"What's the weather in Paris?\"\n```\n**Response 1:**\n```\n[thinking_block_1] + [tool_use block 1]\n```\n\n**Request 2:**\n```\nUser: [\"What's the weather in Paris?\"], \nAssistant: [thinking_block_1] + [tool_use block 1], \nUser: [tool_result_1, cache=True]\n```\n**Response 2:**\n```\n[thinking_block_2] + [text block 2]\n```\nRequest 2 writes a cache of the request content (not the response). The cache includes the original user message, the first thinking block, tool use block, and the tool result.\n\n**Request 3:**\n```\nUser: [\"What's the weather in Paris?\"],\nAssistant: [thinking_block_1] + [tool_use block 1],\nUser: [tool_result_1, cache=True],\nAssistant: [thinking_block_2] + [text block 2],\nUser: [Text response, cache=True]\n```\nFor Claude Opus 4.5 and later, all previous thinking blocks are kept by default. For older models, because a non-tool-result user block was included, all previous thinking blocks are ignored. This request will be processed the same as:\n```\nUser: [\"What's the weather in Paris?\"],\nAssistant: [tool_use block 1],\nUser: [tool_result_1, cache=True],\nAssistant: [text block 2],\nUser: [Text response, cache=True]\n```\n\n**Key points:**\n- This caching behavior happens automatically, even without explicit `cache_control` markers\n- This behavior is consistent whether using regular thinking or interleaved thinking\n\n<section title=\"System prompt caching (preserved when thinking changes)\">\n\n<CodeGroup>\n```python Python\nfrom anthropic import Anthropic\nimport requests\nfrom bs4 import BeautifulSoup\n\nclient = Anthropic()\n\ndef fetch_article_content(url):\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    # Remove script and style elements\n    for script in soup([\"script\", \"style\"]):\n        script.decompose()\n\n    # Get text\n    text = soup.get_text()\n\n    # Break into lines and remove leading and trailing space on each\n    lines = (line.strip() for line in text.splitlines())\n    # Break multi-headlines into a line each\n    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n    # Drop blank lines\n    text = '\\n'.join(chunk for chunk in chunks if chunk)\n\n    return text\n\n# Fetch the content of the article\nbook_url = \"https://www.gutenberg.org/cache/epub/1342/pg1342.txt\"\nbook_content = fetch_article_content(book_url)\n# Use just enough text for caching (first few chapters)\nLARGE_TEXT = book_content[:5000]\n\nSYSTEM_PROMPT=[\n    {\n        \"type\": \"text\",\n        \"text\": \"You are an AI assistant that is tasked with literary analysis. Analyze the following text carefully.\",\n    },\n    {\n        \"type\": \"text\",\n        \"text\": LARGE_TEXT,\n        \"cache_control\": {\"type\": \"ephemeral\"}\n    }\n]\n\nMESSAGES = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Analyze the tone of this passage.\"\n    }\n]\n\n# First request - establish cache\nprint(\"First request - establishing cache\")\nresponse1 = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=20000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 4000\n    },\n    system=SYSTEM_PROMPT,\n    messages=MESSAGES\n)\n\nprint(f\"First response usage: {response1.usage}\")\n\nMESSAGES.append({\n    \"role\": \"assistant\",\n    \"content\": response1.content\n})\nMESSAGES.append({\n    \"role\": \"user\",\n    \"content\": \"Analyze the characters in this passage.\"\n})\n# Second request - same thinking parameters (cache hit expected)\nprint(\"\\nSecond request - same thinking parameters (cache hit expected)\")\nresponse2 = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=20000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 4000\n    },\n    system=SYSTEM_PROMPT,\n    messages=MESSAGES\n)\n\nprint(f\"Second response usage: {response2.usage}\")\n\n# Third request - different thinking parameters (cache miss for messages)\nprint(\"\\nThird request - different thinking parameters (cache miss for messages)\")\nresponse3 = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=20000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 8000  # Changed thinking budget\n    },\n    system=SYSTEM_PROMPT,  # System prompt remains cached\n    messages=MESSAGES  # Messages cache is invalidated\n)\n\nprint(f\"Third response usage: {response3.usage}\")\n```\n\n```typescript TypeScript\nimport Anthropic from '@anthropic-ai/sdk';\nimport axios from 'axios';\nimport * as cheerio from 'cheerio';\n\nconst client = new Anthropic();\n\nasync function fetchArticleContent(url: string): Promise<string> {\n  const response = await axios.get(url);\n  const $ = cheerio.load(response.data);\n  \n  // Remove script and style elements\n  $('script, style').remove();\n  \n  // Get text\n  let text = $.text();\n  \n  // Break into lines and remove leading and trailing space on each\n  const lines = text.split('\\n').map(line => line.trim());\n  // Drop blank lines\n  text = lines.filter(line => line.length > 0).join('\\n');\n  \n  return text;\n}\n\n// Fetch the content of the article\nconst bookUrl = \"https://www.gutenberg.org/cache/epub/1342/pg1342.txt\";\nconst bookContent = await fetchArticleContent(bookUrl);\n// Use just enough text for caching (first few chapters)\nconst LARGE_TEXT = bookContent.slice(0, 5000);\n\nconst SYSTEM_PROMPT = [\n  {\n    type: \"text\",\n    text: \"You are an AI assistant that is tasked with literary analysis. Analyze the following text carefully.\",\n  },\n  {\n    type: \"text\",\n    text: LARGE_TEXT,\n    cache_control: { type: \"ephemeral\" }\n  }\n];\n\nconst MESSAGES = [\n  {\n    role: \"user\",\n    content: \"Analyze the tone of this passage.\"\n  }\n];\n\n// First request - establish cache\nconsole.log(\"First request - establishing cache\");\nconst response1 = await client.messages.create({\n  model: \"claude-sonnet-4-5\",\n  max_tokens: 20000,\n  thinking: {\n    type: \"enabled\",\n    budget_tokens: 4000\n  },\n  system: SYSTEM_PROMPT,\n  messages: MESSAGES\n});\n\nconsole.log(`First response usage: ${response1.usage}`);\n\nMESSAGES.push({\n  role: \"assistant\",\n  content: response1.content\n});\nMESSAGES.push({\n  role: \"user\",\n  content: \"Analyze the characters in this passage.\"\n});\n\n// Second request - same thinking parameters (cache hit expected)\nconsole.log(\"\\nSecond request - same thinking parameters (cache hit expected)\");\nconst response2 = await client.messages.create({\n  model: \"claude-sonnet-4-5\",\n  max_tokens: 20000,\n  thinking: {\n    type: \"enabled\",\n    budget_tokens: 4000\n  },\n  system: SYSTEM_PROMPT,\n  messages: MESSAGES\n});\n\nconsole.log(`Second response usage: ${response2.usage}`);\n\n// Third request - different thinking parameters (cache miss for messages)\nconsole.log(\"\\nThird request - different thinking parameters (cache miss for messages)\");\nconst response3 = await client.messages.create({\n  model: \"claude-sonnet-4-5\",\n  max_tokens: 20000,\n  thinking: {\n    type: \"enabled\",\n    budget_tokens: 8000  // Changed thinking budget\n  },\n  system: SYSTEM_PROMPT,  // System prompt remains cached\n  messages: MESSAGES  // Messages cache is invalidated\n});\n\nconsole.log(`Third response usage: ${response3.usage}`);\n```\n</CodeGroup>\n\n</section>\n<section title=\"Messages caching (invalidated when thinking changes)\">\n\n<CodeGroup>\n```python Python\nfrom anthropic import Anthropic\nimport requests\nfrom bs4 import BeautifulSoup\n\nclient = Anthropic()\n\ndef fetch_article_content(url):\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    # Remove script and style elements\n    for script in soup([\"script\", \"style\"]):\n        script.decompose()\n\n    # Get text\n    text = soup.get_text()\n\n    # Break into lines and remove leading and trailing space on each\n    lines = (line.strip() for line in text.splitlines())\n    # Break multi-headlines into a line each\n    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n    # Drop blank lines\n    text = '\\n'.join(chunk for chunk in chunks if chunk)\n\n    return text\n\n# Fetch the content of the article\nbook_url = \"https://www.gutenberg.org/cache/epub/1342/pg1342.txt\"\nbook_content = fetch_article_content(book_url)\n# Use just enough text for caching (first few chapters)\nLARGE_TEXT = book_content[:5000]\n\n# No system prompt - caching in messages instead\nMESSAGES = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"text\",\n                \"text\": LARGE_TEXT,\n                \"cache_control\": {\"type\": \"ephemeral\"},\n            },\n            {\n                \"type\": \"text\",\n                \"text\": \"Analyze the tone of this passage.\"\n            }\n        ]\n    }\n]\n\n# First request - establish cache\nprint(\"First request - establishing cache\")\nresponse1 = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=20000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 4000\n    },\n    messages=MESSAGES\n)\n\nprint(f\"First response usage: {response1.usage}\")\n\nMESSAGES.append({\n    \"role\": \"assistant\",\n    \"content\": response1.content\n})\nMESSAGES.append({\n    \"role\": \"user\",\n    \"content\": \"Analyze the characters in this passage.\"\n})\n# Second request - same thinking parameters (cache hit expected)\nprint(\"\\nSecond request - same thinking parameters (cache hit expected)\")\nresponse2 = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=20000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 4000  # Same thinking budget\n    },\n    messages=MESSAGES\n)\n\nprint(f\"Second response usage: {response2.usage}\")\n\nMESSAGES.append({\n    \"role\": \"assistant\",\n    \"content\": response2.content\n})\nMESSAGES.append({\n    \"role\": \"user\",\n    \"content\": \"Analyze the setting in this passage.\"\n})\n\n# Third request - different thinking budget (cache miss expected)\nprint(\"\\nThird request - different thinking budget (cache miss expected)\")\nresponse3 = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=20000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 8000  # Different thinking budget breaks cache\n    },\n    messages=MESSAGES\n)\n\nprint(f\"Third response usage: {response3.usage}\")\n```\n\n```typescript TypeScript\nimport Anthropic from '@anthropic-ai/sdk';\nimport axios from 'axios';\nimport * as cheerio from 'cheerio';\n\nconst client = new Anthropic();\n\nasync function fetchArticleContent(url: string): Promise<string> {\n  const response = await axios.get(url);\n  const $ = cheerio.load(response.data);\n\n  // Remove script and style elements\n  $('script, style').remove();\n\n  // Get text\n  let text = $.text();\n\n  // Clean up text (break into lines, remove whitespace)\n  const lines = text.split('\\n').map(line => line.trim());\n  const chunks = lines.flatMap(line => line.split('  ').map(phrase => phrase.trim()));\n  text = chunks.filter(chunk => chunk).join('\\n');\n\n  return text;\n}\n\nasync function main() {\n  // Fetch the content of the article\n  const bookUrl = \"https://www.gutenberg.org/cache/epub/1342/pg1342.txt\";\n  const bookContent = await fetchArticleContent(bookUrl);\n  // Use just enough text for caching (first few chapters)\n  const LARGE_TEXT = bookContent.substring(0, 5000);\n\n  // No system prompt - caching in messages instead\n  let MESSAGES = [\n    {\n      role: \"user\",\n      content: [\n        {\n          type: \"text\",\n          text: LARGE_TEXT,\n          cache_control: {type: \"ephemeral\"},\n        },\n        {\n          type: \"text\",\n          text: \"Analyze the tone of this passage.\"\n        }\n      ]\n    }\n  ];\n\n  // First request - establish cache\n  console.log(\"First request - establishing cache\");\n  const response1 = await client.messages.create({\n    model: \"claude-sonnet-4-5\",\n    max_tokens: 20000,\n    thinking: {\n      type: \"enabled\",\n      budget_tokens: 4000\n    },\n    messages: MESSAGES\n  });\n\n  console.log(`First response usage: `, response1.usage);\n\n  MESSAGES = [\n    ...MESSAGES,\n    {\n      role: \"assistant\",\n      content: response1.content\n    },\n    {\n      role: \"user\",\n      content: \"Analyze the characters in this passage.\"\n    }\n  ];\n\n  // Second request - same thinking parameters (cache hit expected)\n  console.log(\"\\nSecond request - same thinking parameters (cache hit expected)\");\n  const response2 = await client.messages.create({\n    model: \"claude-sonnet-4-5\",\n    max_tokens: 20000,\n    thinking: {\n      type: \"enabled\",\n      budget_tokens: 4000  // Same thinking budget\n    },\n    messages: MESSAGES\n  });\n\n  console.log(`Second response usage: `, response2.usage);\n\n  MESSAGES = [\n    ...MESSAGES,\n    {\n      role: \"assistant\",\n      content: response2.content\n    },\n    {\n      role: \"user\",\n      content: \"Analyze the setting in this passage.\"\n    }\n  ];\n\n  // Third request - different thinking budget (cache miss expected)\n  console.log(\"\\nThird request - different thinking budget (cache miss expected)\");\n  const response3 = await client.messages.create({\n    model: \"claude-sonnet-4-5\",\n    max_tokens: 20000,\n    thinking: {\n      type: \"enabled\",\n      budget_tokens: 8000  // Different thinking budget breaks cache\n    },\n    messages: MESSAGES\n  });\n\n  console.log(`Third response usage: `, response3.usage);\n}\n\nmain().catch(console.error);\n```\n\n```java Java\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.io.BufferedReader;\nimport java.io.InputStreamReader;\nimport java.net.URL;\nimport java.util.Arrays;\nimport java.util.regex.Pattern;\n\nimport com.anthropic.client.AnthropicClient;\nimport com.anthropic.client.okhttp.AnthropicOkHttpClient;\nimport com.anthropic.models.beta.messages.*;\nimport com.anthropic.models.beta.messages.MessageCreateParams;\nimport com.anthropic.models.messages.Model;\n\nimport static java.util.stream.Collectors.joining;\nimport static java.util.stream.Collectors.toList;\n\npublic class ThinkingCacheExample {\n    public static void main(String[] args) throws IOException {\n        AnthropicClient client = AnthropicOkHttpClient.fromEnv();\n\n        // Fetch the content of the article\n        String bookUrl = \"https://www.gutenberg.org/cache/epub/1342/pg1342.txt\";\n        String bookContent = fetchArticleContent(bookUrl);\n        // Use just enough text for caching (first few chapters)\n        String largeText = bookContent.substring(0, 5000);\n\n        List<BetaTextBlockParam> systemPrompt = List.of(\n                BetaTextBlockParam.builder()\n                        .text(\"You are an AI assistant that is tasked with literary analysis. Analyze the following text carefully.\")\n                        .build(),\n                BetaTextBlockParam.builder()\n                        .text(largeText)\n                        .cacheControl(BetaCacheControlEphemeral.builder().build())\n                        .build()\n        );\n\n        List<BetaMessageParam> messages = new ArrayList<>();\n        messages.add(BetaMessageParam.builder()\n                .role(BetaMessageParam.Role.USER)\n                .content(\"Analyze the tone of this passage.\")\n                .build());\n\n        // First request - establish cache\n        System.out.println(\"First request - establishing cache\");\n        BetaMessage response1 = client.beta().messages().create(\n                MessageCreateParams.builder()\n                        .model(Model.CLAUDE_OPUS_4_0)\n                        .maxTokens(20000)\n                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(4000).build())\n                        .systemOfBetaTextBlockParams(systemPrompt)\n                        .messages(messages)\n                        .build()\n        );\n\n        System.out.println(\"First response usage: \" + response1.usage());\n\n        // Second request - same thinking parameters (cache hit expected)\n        System.out.println(\"\\nSecond request - same thinking parameters (cache hit expected)\");\n        BetaMessage response2 = client.beta().messages().create(\n                MessageCreateParams.builder()\n                        .model(Model.CLAUDE_OPUS_4_0)\n                        .maxTokens(20000)\n                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(4000).build())\n                        .systemOfBetaTextBlockParams(systemPrompt)\n                        .addMessage(response1)\n                        .addUserMessage(\"Analyze the characters in this passage.\")\n                        .messages(messages)\n                        .build()\n        );\n\n        System.out.println(\"Second response usage: \" + response2.usage());\n\n        // Third request - different thinking budget (cache hit expected because system prompt caching)\n        System.out.println(\"\\nThird request - different thinking budget (cache hit expected)\");\n        BetaMessage response3 = client.beta().messages().create(\n                MessageCreateParams.builder()\n                        .model(Model.CLAUDE_OPUS_4_0)\n                        .maxTokens(20000)\n                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(8000).build())\n                        .systemOfBetaTextBlockParams(systemPrompt)\n                        .addMessage(response1)\n                        .addUserMessage(\"Analyze the characters in this passage.\")\n                        .addMessage(response2)\n                        .addUserMessage(\"Analyze the setting in this passage.\")\n                        .build()\n        );\n\n        System.out.println(\"Third response usage: \" + response3.usage());\n    }\n\n    private static String fetchArticleContent(String url) throws IOException {\n        // Fetch HTML content\n        String htmlContent = fetchHtml(url);\n\n        // Remove script and style elements\n        String noScriptStyle = removeElements(htmlContent, \"script\", \"style\");\n\n        // Extract text (simple approach - remove HTML tags)\n        String text = removeHtmlTags(noScriptStyle);\n\n        // Clean up text (break into lines, remove whitespace)\n        List<String> lines = Arrays.asList(text.split(\"\\n\"));\n        List<String> trimmedLines = lines.stream()\n                .map(String::trim)\n                .collect(toList());\n\n        // Split on double spaces and flatten\n        List<String> chunks = trimmedLines.stream()\n                .flatMap(line -> Arrays.stream(line.split(\"  \"))\n                        .map(String::trim))\n                .collect(toList());\n\n        // Filter empty chunks and join with newlines\n        return chunks.stream()\n                .filter(chunk -> !chunk.isEmpty())\n                .collect(joining(\"\\n\"));\n    }\n\n    /**\n     * Fetches HTML content from a URL\n     */\n    private static String fetchHtml(String urlString) throws IOException {\n        try (InputStream inputStream = new URL(urlString).openStream()) {\n            StringBuilder content = new StringBuilder();\n            try (BufferedReader reader = new BufferedReader(\n                    new InputStreamReader(inputStream))) {\n                String line;\n                while ((line = reader.readLine()) != null) {\n                    content.append(line).append(\"\\n\");\n                }\n            }\n            return content.toString();\n        }\n    }\n\n    /**\n     * Removes specified HTML elements and their content\n     */\n    private static String removeElements(String html, String... elementNames) {\n        String result = html;\n        for (String element : elementNames) {\n            // Pattern to match <element>...</element> and self-closing tags\n            String pattern = \"<\" + element + \"\\\\s*[^>]*>.*?</\" + element + \">|<\" + element + \"\\\\s*[^>]*/?>\";\n            result = Pattern.compile(pattern, Pattern.DOTALL).matcher(result).replaceAll(\"\");\n        }\n        return result;\n    }\n\n    /**\n     * Removes all HTML tags from content\n     */\n    private static String removeHtmlTags(String html) {\n        // Replace <br> and <p> tags with newlines for better text formatting\n        String withLineBreaks = html.replaceAll(\"<br\\\\s*/?\\\\s*>|</?p\\\\s*[^>]*>\", \"\\n\");\n\n        // Remove remaining HTML tags\n        String noTags = withLineBreaks.replaceAll(\"<[^>]*>\", \"\");\n\n        // Decode HTML entities (simplified for common entities)\n        return decodeHtmlEntities(noTags);\n    }\n\n    /**\n     * Simple HTML entity decoder for common entities\n     */\n    private static String decodeHtmlEntities(String text) {\n        return text\n                .replaceAll(\"&nbsp;\", \" \")\n                .replaceAll(\"&amp;\", \"&\")\n                .replaceAll(\"&lt;\", \"<\")\n                .replaceAll(\"&gt;\", \">\")\n                .replaceAll(\"&quot;\", \"\\\"\")\n                .replaceAll(\"&#39;\", \"'\")\n                .replaceAll(\"&hellip;\", \"...\")\n                .replaceAll(\"&mdash;\", \"—\");\n    }\n\n}\n```\n</CodeGroup>\n\nHere is the output of the script (you may see slightly different numbers)\n\n```\nFirst request - establishing cache\nFirst response usage: { cache_creation_input_tokens: 1370, cache_read_input_tokens: 0, input_tokens: 17, output_tokens: 700 }\n\nSecond request - same thinking parameters (cache hit expected)\n\nSecond response usage: { cache_creation_input_tokens: 0, cache_read_input_tokens: 1370, input_tokens: 303, output_tokens: 874 }\n\nThird request - different thinking budget (cache miss expected)\nThird response usage: { cache_creation_input_tokens: 1370, cache_read_input_tokens: 0, input_tokens: 747, output_tokens: 619 }\n```\n\nThis example demonstrates that when caching is set up in the messages array, changing the thinking parameters (budget_tokens increased from 4000 to 8000) **invalidates the cache**. The third request shows no cache hit with `cache_creation_input_tokens=1370` and `cache_read_input_tokens=0`, proving that message-based caching is invalidated when thinking parameters change.\n\n</section>\n\n## Max tokens and context window size with extended thinking\n\nIn older Claude models (prior to Claude Sonnet 3.7), if the sum of prompt tokens and `max_tokens` exceeded the model's context window, the system would automatically adjust `max_tokens` to fit within the context limit. This meant you could set a large `max_tokens` value and the system would silently reduce it as needed.\n\nWith Claude 3.7 and 4 models, `max_tokens` (which includes your thinking budget when thinking is enabled) is enforced as a strict limit. The system will now return a validation error if prompt tokens + `max_tokens` exceeds the context window size.\n\n<Note>\nYou can read through our [guide on context windows](/docs/en/build-with-claude/context-windows) for a more thorough deep dive.\n</Note>\n\n### The context window with extended thinking\n\nWhen calculating context window usage with thinking enabled, there are some considerations to be aware of:\n\n- Thinking blocks from previous turns are stripped and not counted towards your context window\n- Current turn thinking counts towards your `max_tokens` limit for that turn\n\nThe diagram below demonstrates the specialized token management when extended thinking is enabled:\n\n![Context window diagram with extended thinking](/docs/images/context-window-thinking.svg)\n\nThe effective context window is calculated as:\n\n```\ncontext window =\n  (current input tokens - previous thinking tokens) +\n  (thinking tokens + encrypted thinking tokens + text output tokens)\n```\n\nWe recommend using the [token counting API](/docs/en/build-with-claude/token-counting) to get accurate token counts for your specific use case, especially when working with multi-turn conversations that include thinking.\n\n### The context window with extended thinking and tool use\n\nWhen using extended thinking with tool use, thinking blocks must be explicitly preserved and returned with the tool results.\n\nThe effective context window calculation for extended thinking with tool use becomes: \n\n```\ncontext window =\n  (current input tokens + previous thinking tokens + tool use tokens) +\n  (thinking tokens + encrypted thinking tokens + text output tokens)\n```\n\nThe diagram below illustrates token management for extended thinking with tool use:\n\n![Context window diagram with extended thinking and tool use](/docs/images/context-window-thinking-tools.svg)\n\n### Managing tokens with extended thinking\n\nGiven the context window and `max_tokens` behavior with extended thinking Claude 3.7 and 4 models, you may need to:\n\n- More actively monitor and manage your token usage\n- Adjust `max_tokens` values as your prompt length changes\n- Potentially use the [token counting endpoints](/docs/en/build-with-claude/token-counting) more frequently\n- Be aware that previous thinking blocks don't accumulate in your context window\n\nThis change has been made to provide more predictable and transparent behavior, especially as maximum token limits have increased significantly.\n\n## Thinking encryption\n\nFull thinking content is encrypted and returned in the `signature` field. This field is used to verify that thinking blocks were generated by Claude when passed back to the API. \n\n<Note>\nIt is only strictly necessary to send back thinking blocks when using [tools with extended thinking](#extended-thinking-with-tool-use). Otherwise you can omit thinking blocks from previous turns, or let the API strip them for you if you pass them back. \n\nIf sending back thinking blocks, we recommend passing everything back as you received it for consistency and to avoid potential issues.\n</Note>\n\nHere are some important considerations on thinking encryption:\n- When [streaming responses](#streaming-thinking), the signature is added via a `signature_delta` inside a `content_block_delta` event just before the `content_block_stop` event.\n- `signature` values are significantly longer in Claude 4 models than in previous models.\n- The `signature` field is an opaque field and should not be interpreted or parsed - it exists solely for verification purposes.\n- `signature` values are compatible across platforms (Claude APIs, [Amazon Bedrock](/docs/en/build-with-claude/claude-on-amazon-bedrock), and [Vertex AI](/docs/en/build-with-claude/claude-on-vertex-ai)). Values generated on one platform will be compatible with another.\n\n### Thinking redaction\n\nOccasionally Claude's internal reasoning will be flagged by our safety systems. When this occurs, we encrypt some or all of the `thinking` block and return it to you as a `redacted_thinking` block. `redacted_thinking` blocks are decrypted when passed back to the API, allowing Claude to continue its response without losing context.\n\nWhen building customer-facing applications that use extended thinking:\n\n- Be aware that redacted thinking blocks contain encrypted content that isn't human-readable\n- Consider providing a simple explanation like: \"Some of Claude's internal reasoning has been automatically encrypted for safety reasons. This doesn't affect the quality of responses.\"\n- If showing thinking blocks to users, you can filter out redacted blocks while preserving normal thinking blocks\n- Be transparent that using extended thinking features may occasionally result in some reasoning being encrypted\n- Implement appropriate error handling to gracefully manage redacted thinking without breaking your UI\n\nHere's an example showing both normal and redacted thinking blocks:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"thinking\",\n      \"thinking\": \"Let me analyze this step by step...\",\n      \"signature\": \"WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL....\"\n    },\n    {\n      \"type\": \"redacted_thinking\",\n      \"data\": \"EmwKAhgBEgy3va3pzix/LafPsn4aDFIT2Xlxh0L5L8rLVyIwxtE3rAFBa8cr3qpPkNRj2YfWXGmKDxH4mPnZ5sQ7vB9URj2pLmN3kF8/dW5hR7xJ0aP1oLs9yTcMnKVf2wRpEGjH9XZaBt4UvDcPrQ...\"\n    },\n    {\n      \"type\": \"text\",\n      \"text\": \"Based on my analysis...\"\n    }\n  ]\n}\n```\n\n<Note>\nSeeing redacted thinking blocks in your output is expected behavior. The model can still use this redacted reasoning to inform its responses while maintaining safety guardrails.\n\nIf you need to test redacted thinking handling in your application, you can use this special test string as your prompt: `ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB`\n</Note>\n\nWhen passing `thinking` and `redacted_thinking` blocks back to the API in a multi-turn conversation, you must include the complete unmodified block back to the API for the last assistant turn. This is critical for maintaining the model's reasoning flow. We suggest always passing back all thinking blocks to the API. For more details, see the [Preserving thinking blocks](#preserving-thinking-blocks) section above.\n\n<section title=\"Example: Working with redacted thinking blocks\">\n\nThis example demonstrates how to handle `redacted_thinking` blocks that may appear in responses when Claude's internal reasoning contains content flagged by safety systems:\n\n<CodeGroup>\n```python Python\nimport anthropic\n\nclient = anthropic.Anthropic()\n\n# Using a special prompt that triggers redacted thinking (for demonstration purposes only)\nresponse = client.messages.create(\n    model=\"claude-sonnet-4-5-20250929\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB\"\n    }]\n)\n\n# Identify redacted thinking blocks\nhas_redacted_thinking = any(\n    block.type == \"redacted_thinking\" for block in response.content\n)\n\nif has_redacted_thinking:\n    print(\"Response contains redacted thinking blocks\")\n    # These blocks are still usable in subsequent requests\n\n    # Extract all blocks (both redacted and non-redacted)\n    all_thinking_blocks = [\n        block for block in response.content\n        if block.type in [\"thinking\", \"redacted_thinking\"]\n    ]\n\n    # When passing to subsequent requests, include all blocks without modification\n    # This preserves the integrity of Claude's reasoning\n\n    print(f\"Found {len(all_thinking_blocks)} thinking blocks total\")\n    print(f\"These blocks are still billable as output tokens\")\n```\n\n```typescript TypeScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst client = new Anthropic();\n\n// Using a special prompt that triggers redacted thinking (for demonstration purposes only)\nconst response = await client.messages.create({\n  model: \"claude-sonnet-4-5-20250929\",\n  max_tokens: 16000,\n  thinking: {\n    type: \"enabled\",\n    budget_tokens: 10000\n  },\n  messages: [{\n    role: \"user\",\n    content: \"ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB\"\n  }]\n});\n\n// Identify redacted thinking blocks\nconst hasRedactedThinking = response.content.some(\n  block => block.type === \"redacted_thinking\"\n);\n\nif (hasRedactedThinking) {\n  console.log(\"Response contains redacted thinking blocks\");\n  // These blocks are still usable in subsequent requests\n\n  // Extract all blocks (both redacted and non-redacted)\n  const allThinkingBlocks = response.content.filter(\n    block => block.type === \"thinking\" || block.type === \"redacted_thinking\"\n  );\n\n  // When passing to subsequent requests, include all blocks without modification\n  // This preserves the integrity of Claude's reasoning\n\n  console.log(`Found ${allThinkingBlocks.length} thinking blocks total`);\n  console.log(`These blocks are still billable as output tokens`);\n}\n```\n\n```java Java\nimport java.util.List;\n\nimport static java.util.stream.Collectors.toList;\n\nimport com.anthropic.client.AnthropicClient;\nimport com.anthropic.client.okhttp.AnthropicOkHttpClient;\nimport com.anthropic.models.beta.messages.BetaContentBlock;\nimport com.anthropic.models.beta.messages.BetaMessage;\nimport com.anthropic.models.beta.messages.MessageCreateParams;\nimport com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;\nimport com.anthropic.models.messages.Model;\n\npublic class RedactedThinkingExample {\n    public static void main(String[] args) {\n        AnthropicClient client = AnthropicOkHttpClient.fromEnv();\n\n        // Using a special prompt that triggers redacted thinking (for demonstration purposes only)\n        BetaMessage response = client.beta().messages().create(\n                MessageCreateParams.builder()\n                        .model(Model.CLAUDE_SONNET_4_5)\n                        .maxTokens(16000)\n                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())\n                        .addUserMessage(\"ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB\")\n                        .build()\n        );\n\n        // Identify redacted thinking blocks\n        boolean hasRedactedThinking = response.content().stream()\n                .anyMatch(BetaContentBlock::isRedactedThinking);\n\n        if (hasRedactedThinking) {\n            System.out.println(\"Response contains redacted thinking blocks\");\n            // These blocks are still usable in subsequent requests\n            // Extract all blocks (both redacted and non-redacted)\n            List<BetaContentBlock> allThinkingBlocks = response.content().stream()\n                    .filter(block -> block.isThinking() ||\n                            block.isRedactedThinking())\n                    .collect(toList());\n\n            // When passing to subsequent requests, include all blocks without modification\n            // This preserves the integrity of Claude's reasoning\n            System.out.println(\"Found \" + allThinkingBlocks.size() + \" thinking blocks total\");\n            System.out.println(\"These blocks are still billable as output tokens\");\n        }\n    }\n}\n```\n\n</CodeGroup>\n\n<TryInConsoleButton\n  userPrompt=\"ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB\"\n  thinkingBudgetTokens={16000}\n>\n  Try in Console\n</TryInConsoleButton>\n\n</section>\n\n## Differences in thinking across model versions\n\nThe Messages API handles thinking differently across Claude Sonnet 3.7 and Claude 4 models, primarily in redaction and summarization behavior.\n\nSee the table below for a condensed comparison:\n\n| Feature | Claude Sonnet 3.7 | Claude 4 Models (pre-Opus 4.5) | Claude Opus 4.5 and later |\n|---------|------------------|-------------------------------|--------------------------|\n| **Thinking Output** | Returns full thinking output | Returns summarized thinking | Returns summarized thinking |\n| **Interleaved Thinking** | Not supported | Supported with `interleaved-thinking-2025-05-14` beta header | Supported with `interleaved-thinking-2025-05-14` beta header |\n| **Thinking Block Preservation** | Not preserved across turns | Not preserved across turns | **Preserved by default** (enables cache optimization, token savings) |\n\n### Thinking block preservation in Claude Opus 4.5\n\nClaude Opus 4.5 introduces a new default behavior: **thinking blocks from previous assistant turns are preserved in model context by default**. This differs from earlier models, which remove thinking blocks from prior turns.\n\n**Benefits of thinking block preservation:**\n\n- **Cache optimization**: When using tool use, preserved thinking blocks enable cache hits as they are passed back with tool results and cached incrementally across the assistant turn, resulting in token savings in multi-step workflows\n- **No intelligence impact**: Preserving thinking blocks has no negative effect on model performance\n\n**Important considerations:**\n\n- **Context usage**: Long conversations will consume more context space since thinking blocks are retained in context\n- **Automatic behavior**: This is the default behavior for Claude Opus 4.5—no code changes or beta headers required\n- **Backward compatibility**: To leverage this feature, continue passing complete, unmodified thinking blocks back to the API as you would for tool use\n\n<Note>\nFor earlier models (Claude Sonnet 4.5, Opus 4.1, etc.), thinking blocks from previous turns continue to be removed from context. The existing behavior described in the [Extended thinking with prompt caching](#extended-thinking-with-prompt-caching) section applies to those models.\n</Note>\n\n## Pricing\n\nFor complete pricing information including base rates, cache writes, cache hits, and output tokens, see the [pricing page](/docs/en/about-claude/pricing).\n\nThe thinking process incurs charges for:\n- Tokens used during thinking (output tokens)\n- Thinking blocks from the last assistant turn included in subsequent requests (input tokens)\n- Standard text output tokens\n\n<Note>\nWhen extended thinking is enabled, a specialized system prompt is automatically included to support this feature.\n</Note>\n\nWhen using summarized thinking:\n- **Input tokens**: Tokens in your original request (excludes thinking tokens from previous turns)\n- **Output tokens (billed)**: The original thinking tokens that Claude generated internally\n- **Output tokens (visible)**: The summarized thinking tokens you see in the response\n- **No charge**: Tokens used to generate the summary\n\n<Warning>\nThe billed output token count will **not** match the visible token count in the response. You are billed for the full thinking process, not the summary you see.\n</Warning>\n\n## Best practices and considerations for extended thinking\n\n### Working with thinking budgets\n\n- **Budget optimization:** The minimum budget is 1,024 tokens. We suggest starting at the minimum and increasing the thinking budget incrementally to find the optimal range for your use case. Higher token counts enable more comprehensive reasoning but with diminishing returns depending on the task. Increasing the budget can improve response quality at the tradeoff of increased latency. For critical tasks, test different settings to find the optimal balance. Note that the thinking budget is a target rather than a strict limit—actual token usage may vary based on the task.\n- **Starting points:** Start with larger thinking budgets (16k+ tokens) for complex tasks and adjust based on your needs.\n- **Large budgets:** For thinking budgets above 32k, we recommend using [batch processing](/docs/en/build-with-claude/batch-processing) to avoid networking issues. Requests pushing the model to think above 32k tokens causes long running requests that might run up against system timeouts and open connection limits.\n- **Token usage tracking:** Monitor thinking token usage to optimize costs and performance.\n\n### Performance considerations\n\n- **Response times:** Be prepared for potentially longer response times due to the additional processing required for the reasoning process. Factor in that generating thinking blocks may increase overall response time.\n- **Streaming requirements:** Streaming is required when `max_tokens` is greater than 21,333. When streaming, be prepared to handle both thinking and text content blocks as they arrive.\n\n### Feature compatibility\n\n- Thinking isn't compatible with `temperature` or `top_k` modifications as well as [forced tool use](/docs/en/agents-and-tools/tool-use/implement-tool-use#forcing-tool-use).\n- When thinking is enabled, you can set `top_p` to values between 1 and 0.95.\n- You cannot pre-fill responses when thinking is enabled.\n- Changes to the thinking budget invalidate cached prompt prefixes that include messages. However, cached system prompts and tool definitions will continue to work when thinking parameters change.\n\n### Usage guidelines\n\n- **Task selection:** Use extended thinking for particularly complex tasks that benefit from step-by-step reasoning like math, coding, and analysis.\n- **Context handling:** You do not need to remove previous thinking blocks yourself. The Claude API automatically ignores thinking blocks from previous turns and they are not included when calculating context usage.\n- **Prompt engineering:** Review our [extended thinking prompting tips](/docs/en/build-with-claude/prompt-engineering/extended-thinking-tips) if you want to maximize Claude's thinking capabilities.\n\n## Next steps\n\n<CardGroup>\n  <Card title=\"Try the extended thinking cookbook\" icon=\"book\" href=\"https://github.com/anthropics/anthropic-cookbook/tree/main/extended_thinking\">\n    Explore practical examples of thinking in our cookbook.\n  </Card>\n  <Card title=\"Extended thinking prompting tips\" icon=\"code\" href=\"/docs/en/build-with-claude/prompt-engineering/extended-thinking-tips\">\n    Learn prompt engineering best practices for extended thinking.\n  </Card>\n</CardGroup>"}]},"uuid":"77ce093c-f4b9-46b1-b122-d9da13590648","timestamp":"2025-12-13T02:57:06.147Z","toolUseResult":{"bytes":77690,"code":200,"codeText":"OK","result":"# Building with extended thinking\n\n---\n\nExtended thinking gives Claude enhanced reasoning capabilities for complex tasks, while providing varying levels of transparency into its step-by-step thought process before it delivers its final answer.\n\n## Supported models\n\nExtended thinking is supported in the following models:\n\n- Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)\n- Claude Sonnet 4 (`claude-sonnet-4-20250514`)\n- Claude Sonnet 3.7 (`claude-3-7-sonnet-20250219`) ([deprecated](/docs/en/about-claude/model-deprecations))\n- Claude Haiku 4.5 (`claude-haiku-4-5-20251001`)\n- Claude Opus 4.5 (`claude-opus-4-5-20251101`)\n- Claude Opus 4.1 (`claude-opus-4-1-20250805`)\n- Claude Opus 4 (`claude-opus-4-20250514`)\n\n<Note>\nAPI behavior differs across Claude Sonnet 3.7 and Claude 4 models, but the API shapes remain exactly the same.\n\nFor more information, see [Differences in thinking across model versions](#differences-in-thinking-across-model-versions).\n</Note>\n\n## How extended thinking works\n\nWhen extended thinking is turned on, Claude creates `thinking` content blocks where it outputs its internal reasoning. Claude incorporates insights from this reasoning before crafting a final response.\n\nThe API response will include `thinking` content blocks, followed by `text` content blocks.\n\nHere's an example of the default response format:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"thinking\",\n      \"thinking\": \"Let me analyze this step by step...\",\n      \"signature\": \"WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL....\"\n    },\n    {\n      \"type\": \"text\",\n      \"text\": \"Based on my analysis...\"\n    }\n  ]\n}\n```\n\nFor more information about the response format of extended thinking, see the [Messages API Reference](/docs/en/api/messages).\n\n## How to use extended thinking\n\nHere is an example of using extended thinking in the Messages API:\n\n<CodeGroup>\n```bash Shell\ncurl https://api.anthropic.com/v1/messages \\\n     --header \"x-api-key: $ANTHROPIC_API_KEY\" \\\n     --header \"anthropic-version: 2023-06-01\" \\\n     --header \"content-type: application/json\" \\\n     --data \\\n'{\n    \"model\": \"claude-sonnet-4-5\",\n    \"max_tokens\": 16000,\n    \"thinking\": {\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"Are there an infinite number of prime numbers such that n mod 4 == 3?\"\n        }\n    ]\n}'\n```\n\n```python Python\nimport anthropic\n\nclient = anthropic.Anthropic()\n\nresponse = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Are there an infinite number of prime numbers such that n mod 4 == 3?\"\n    }]\n)\n\n# The response will contain summarized thinking blocks and text blocks\nfor block in response.content:\n    if block.type == \"thinking\":\n        print(f\"\\nThinking summary: {block.thinking}\")\n    elif block.type == \"text\":\n        print(f\"\\nResponse: {block.text}\")\n```\n\n```typescript TypeScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst client = new Anthropic();\n\nconst response = await client.messages.create({\n  model: \"claude-sonnet-4-5\",\n  max_tokens: 16000,\n  thinking: {\n    type: \"enabled\",\n    budget_tokens: 10000\n  },\n  messages: [{\n    role: \"user\",\n    content: \"Are there an infinite number of prime numbers such that n mod 4 == 3?\"\n  }]\n});\n\n// The response will contain summarized thinking blocks and text blocks\nfor (const block of response.content) {\n  if (block.type === \"thinking\") {\n    console.log(`\\nThinking summary: ${block.thinking}`);\n  } else if (block.type === \"text\") {\n    console.log(`\\nResponse: ${block.text}`);\n  }\n}\n```\n\n```java Java\nimport com.anthropic.client.AnthropicClient;\nimport com.anthropic.client.okhttp.AnthropicOkHttpClient;\nimport com.anthropic.models.beta.messages.*;\nimport com.anthropic.models.beta.messages.MessageCreateParams;\nimport com.anthropic.models.messages.*;\n\npublic class SimpleThinkingExample {\n    public static void main(String[] args) {\n        AnthropicClient client = AnthropicOkHttpClient.fromEnv();\n\n        BetaMessage response = client.beta().messages().create(\n                MessageCreateParams.builder()\n                        .model(Model.CLAUDE_OPUS_4_0)\n                        .maxTokens(16000)\n                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())\n                        .addUserMessage(\"Are there an infinite number of prime numbers such that n mod 4 == 3?\")\n                        .build()\n        );\n\n        System.out.println(response);\n    }\n}\n```\n\n</CodeGroup>\n\nTo turn on extended thinking, add a `thinking` object, with the `type` parameter set to `enabled` and the `budget_tokens` to a specified token budget for extended thinking.\n\nThe `budget_tokens` parameter determines the maximum number of tokens Claude is allowed to use for its internal reasoning process. In Claude 4 models, this limit applies to full thinking tokens, and not to [the summarized output](#summarized-thinking). Larger budgets can improve response quality by enabling more thorough analysis for complex problems, although Claude may not use the entire budget allocated, especially at ranges above 32k.\n\n`budget_tokens` must be set to a value less than `max_tokens`. However, when using [interleaved thinking with tools](#interleaved-thinking), you can exceed this limit as the token limit becomes your entire context window (200k tokens). \n\n### Summarized thinking\n\nWith extended thinking enabled, the Messages API for Claude 4 models returns a summary of Claude's full thinking process. Summarized thinking provides the full intelligence benefits of extended thinking, while preventing misuse.\n\nHere are some important considerations for summarized thinking:\n\n- You're charged for the full thinking tokens generated by the original request, not the summary tokens.\n- The billed output token count will **not match** the count of tokens you see in the response.\n- The first few lines of thinking output are more verbose, providing detailed reasoning that's particularly helpful for prompt engineering purposes. \n- As Anthropic seeks to improve the extended thinking feature, summarization behavior is subject to change.\n- Summarization preserves the key ideas of Claude's thinking process with minimal added latency, enabling a streamable user experience and easy migration from Claude Sonnet 3.7 to Claude 4 models.\n- Summarization is processed by a different model than the one you target in your requests. The thinking model does not see the summarized output.\n\n<Note>\nClaude Sonnet 3.7 continues to return full thinking output.\n\nIn rare cases where you need access to full thinking output for Claude 4 models, [contact our sales team](mailto:sales@anthropic.com).\n</Note>\n\n### Streaming thinking\n\nYou can stream extended thinking responses using [server-sent events (SSE)](https://developer.mozilla.org/en-US/Web/API/Server-sent%5Fevents/Using%5Fserver-sent%5Fevents).\n\nWhen streaming is enabled for extended thinking, you receive thinking content via `thinking_delta` events.\n\nFor more documention on streaming via the Messages API, see [Streaming Messages](/docs/en/build-with-claude/streaming).\n\nHere's how to handle streaming with thinking:\n\n<CodeGroup>\n```bash Shell\ncurl https://api.anthropic.com/v1/messages \\\n     --header \"x-api-key: $ANTHROPIC_API_KEY\" \\\n     --header \"anthropic-version: 2023-06-01\" \\\n     --header \"content-type: application/json\" \\\n     --data \\\n'{\n    \"model\": \"claude-sonnet-4-5\",\n    \"max_tokens\": 16000,\n    \"stream\": true,\n    \"thinking\": {\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"What is 27 * 453?\"\n        }\n    ]\n}'\n```\n\n```python Python\nimport anthropic\n\nclient = anthropic.Anthropic()\n\nwith client.messages.stream(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=16000,\n    thinking={\"type\": \"enabled\", \"budget_tokens\": 10000},\n    messages=[{\"role\": \"user\", \"content\": \"What is 27 * 453?\"}],\n) as stream:\n    thinking_started = False\n    response_started = False\n\n    for event in stream:\n        if event.type == \"content_block_start\":\n            print(f\"\\nStarting {event.content_block.type} block...\")\n            # Reset flags for each new block\n            thinking_started = False\n            response_started = False\n        elif event.type == \"content_block_delta\":\n            if event.delta.type == \"thinking_delta\":\n                if not thinking_started:\n                    print(\"Thinking: \", end=\"\", flush=True)\n                    thinking_started = True\n                print(event.delta.thinking, end=\"\", flush=True)\n            elif event.delta.type == \"text_delta\":\n                if not response_started:\n                    print(\"Response: \", end=\"\", flush=True)\n                    response_started = True\n                print(event.delta.text, end=\"\", flush=True)\n        elif event.type == \"content_block_stop\":\n            print(\"\\nBlock complete.\")\n```\n\n```typescript TypeScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst client = new Anthropic();\n\nconst stream = await client.messages.stream({\n  model: \"claude-sonnet-4-5\",\n  max_tokens: 16000,\n  thinking: {\n    type: \"enabled\",\n    budget_tokens: 10000\n  },\n  messages: [{\n    role: \"user\",\n    content: \"What is 27 * 453?\"\n  }]\n});\n\nlet thinkingStarted = false;\nlet responseStarted = false;\n\nfor await (const event of stream) {\n  if (event.type === 'content_block_start') {\n    console.log(`\\nStarting ${event.content_block.type} block...`);\n    // Reset flags for each new block\n    thinkingStarted = false;\n    responseStarted = false;\n  } else if (event.type === 'content_block_delta') {\n    if (event.delta.type === 'thinking_delta') {\n      if (!thinkingStarted) {\n        process.stdout.write('Thinking: ');\n        thinkingStarted = true;\n      }\n      process.stdout.write(event.delta.thinking);\n    } else if (event.delta.type === 'text_delta') {\n      if (!responseStarted) {\n        process.stdout.write('Response: ');\n        responseStarted = true;\n      }\n      process.stdout.write(event.delta.text);\n    }\n  } else if (event.type === 'content_block_stop') {\n    console.log('\\nBlock complete.');\n  }\n}\n```\n\n```java Java\nimport com.anthropic.client.AnthropicClient;\nimport com.anthropic.client.okhttp.AnthropicOkHttpClient;\nimport com.anthropic.core.http.StreamResponse;\nimport com.anthropic.models.beta.messages.MessageCreateParams;\nimport com.anthropic.models.beta.messages.BetaRawMessageStreamEvent;\nimport com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;\nimport com.anthropic.models.messages.Model;\n\npublic class SimpleThinkingStreamingExample {\n    private static boolean thinkingStarted = false;\n    private static boolean responseStarted = false;\n    \n    public static void main(String[] args) {\n        AnthropicClient client = AnthropicOkHttpClient.fromEnv();\n\n        MessageCreateParams createParams = MessageCreateParams.builder()\n                .model(Model.CLAUDE_OPUS_4_0)\n                .maxTokens(16000)\n                .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())\n                .addUserMessage(\"What is 27 * 453?\")\n                .build();\n\n        try (StreamResponse<BetaRawMessageStreamEvent> streamResponse =\n                     client.beta().messages().createStreaming(createParams)) {\n            streamResponse.stream()\n                    .forEach(event -> {\n                        if (event.isContentBlockStart()) {\n                            System.out.printf(\"\\nStarting %s block...%n\",\n                                    event.asContentBlockStart()._type());\n                            // Reset flags for each new block\n                            thinkingStarted = false;\n                            responseStarted = false;\n                        } else if (event.isContentBlockDelta()) {\n                            var delta = event.asContentBlockDelta().delta();\n                            if (delta.isBetaThinking()) {\n                                if (!thinkingStarted) {\n                                    System.out.print(\"Thinking: \");\n                                    thinkingStarted = true;\n                                }\n                                System.out.print(delta.asBetaThinking().thinking());\n                                System.out.flush();\n                            } else if (delta.isBetaText()) {\n                                if (!responseStarted) {\n                                    System.out.print(\"Response: \");\n                                    responseStarted = true;\n                                }\n                                System.out.print(delta.asBetaText().text());\n                                System.out.flush();\n                            }\n                        } else if (event.isContentBlockStop()) {\n                            System.out.println(\"\\nBlock complete.\");\n                        }\n                    });\n        }\n    }\n}\n```\n\n</CodeGroup>\n\n<TryInConsoleButton userPrompt=\"What is 27 * 453?\" thinkingBudgetTokens={16000}>\n  Try in Console\n</TryInConsoleButton>\n  \n\nExample streaming output:\n```json\nevent: message_start\ndata: {\"type\": \"message_start\", \"message\": {\"id\": \"msg_01...\", \"type\": \"message\", \"role\": \"assistant\", \"content\": [], \"model\": \"claude-sonnet-4-5\", \"stop_reason\": null, \"stop_sequence\": null}}\n\nevent: content_block_start\ndata: {\"type\": \"content_block_start\", \"index\": 0, \"content_block\": {\"type\": \"thinking\", \"thinking\": \"\"}}\n\nevent: content_block_delta\ndata: {\"type\": \"content_block_delta\", \"index\": 0, \"delta\": {\"type\": \"thinking_delta\", \"thinking\": \"Let me solve this step by step:\\n\\n1. First break down 27 * 453\"}}\n\nevent: content_block_delta\ndata: {\"type\": \"content_block_delta\", \"index\": 0, \"delta\": {\"type\": \"thinking_delta\", \"thinking\": \"\\n2. 453 = 400 + 50 + 3\"}}\n\n// Additional thinking deltas...\n\nevent: content_block_delta\ndata: {\"type\": \"content_block_delta\", \"index\": 0, \"delta\": {\"type\": \"signature_delta\", \"signature\": \"EqQBCgIYAhIM1gbcDa9GJwZA2b3hGgxBdjrkzLoky3dl1pkiMOYds...\"}}\n\nevent: content_block_stop\ndata: {\"type\": \"content_block_stop\", \"index\": 0}\n\nevent: content_block_start\ndata: {\"type\": \"content_block_start\", \"index\": 1, \"content_block\": {\"type\": \"text\", \"text\": \"\"}}\n\nevent: content_block_delta\ndata: {\"type\": \"content_block_delta\", \"index\": 1, \"delta\": {\"type\": \"text_delta\", \"text\": \"27 * 453 = 12,231\"}}\n\n// Additional text deltas...\n\nevent: content_block_stop\ndata: {\"type\": \"content_block_stop\", \"index\": 1}\n\nevent: message_delta\ndata: {\"type\": \"message_delta\", \"delta\": {\"stop_reason\": \"end_turn\", \"stop_sequence\": null}}\n\nevent: message_stop\ndata: {\"type\": \"message_stop\"}\n```\n\n<Note>\nWhen using streaming with thinking enabled, you might notice that text sometimes arrives in larger chunks alternating with smaller, token-by-token delivery. This is expected behavior, especially for thinking content.\n\nThe streaming system needs to process content in batches for optimal performance, which can result in this \"chunky\" delivery pattern, with possible delays between streaming events. We're continuously working to improve this experience, with future updates focused on making thinking content stream more smoothly.\n</Note>\n\n## Extended thinking with tool use\n\nExtended thinking can be used alongside [tool use](/docs/en/agents-and-tools/tool-use/overview), allowing Claude to reason through tool selection and results processing.\n\nWhen using extended thinking with tool use, be aware of the following limitations:\n\n1. **Tool choice limitation**: Tool use with thinking only supports `tool_choice: {\"type\": \"auto\"}` (the default) or `tool_choice: {\"type\": \"none\"}`. Using `tool_choice: {\"type\": \"any\"}` or `tool_choice: {\"type\": \"tool\", \"name\": \"...\"}` will result in an error because these options force tool use, which is incompatible with extended thinking.\n\n2. **Preserving thinking blocks**: During tool use, you must pass `thinking` blocks back to the API for the last assistant message. Include the complete unmodified block back to the API to maintain reasoning continuity.\n\n### Toggling thinking modes in conversations\n\nYou cannot toggle thinking in the middle of an assistant turn, including during tool use loops. The entire assistant turn must operate in a single thinking mode: \n\n- **If thinking is enabled**, the final assistant turn must start with a thinking block.\n- **If thinking is disabled**, the final assistant turn must not contain any thinking blocks\n\nFrom the model's perspective, **tool use loops are part of the assistant turn**. An assistant turn doesn't complete until Claude finishes its full response, which may include multiple tool calls and results.\n\nFor example, this sequence is all part of a **single assistant turn**:\n```\nUser: \"What's the weather in Paris?\"\nAssistant: [thinking] + [tool_use: get_weather]\nUser: [tool_result: \"20°C, sunny\"]\nAssistant: [text: \"The weather in Paris is 20°C and sunny\"]\n```\n\nEven though there are multiple API messages, the tool use loop is conceptually part of one continuous assistant response.\n\n#### Common error scenarios\n\nYou might encounter this error:\n```\nExpected `thinking` or `redacted_thinking`, but found `tool_use`.\nWhen `thinking` is enabled, a final `assistant` message must start\nwith a thinking block (preceding the lastmost set of `tool_use` and\n`tool_result` blocks).\n```\n\nThis typically occurs when:\n1. You had thinking **disabled** during a tool use sequence\n2. You want to enable thinking again\n3. Your last assistant message contains tool use blocks but no thinking block\n\n#### Practical guidance\n\n**✗ Invalid: Toggling thinking immediately after tool use**\n```\nUser: \"What's the weather?\"\nAssistant: [tool_use] (thinking disabled)\nUser: [tool_result]\n// Cannot enable thinking here - still in the same assistant turn\n```\n\n**✓ Valid: Complete the assistant turn first**\n```\nUser: \"What's the weather?\"\nAssistant: [tool_use] (thinking disabled)\nUser: [tool_result]\nAssistant: [text: \"It's sunny\"] \nUser: \"What about tomorrow?\" (thinking disabled)\nAssistant: [thinking] + [text: \"...\"] (thinking enabled - new turn)\n```\n\n**Best practice**: Plan your thinking strategy at the start of each turn rather than trying to toggle mid-turn.\n\n<Note>\nToggling thinking modes also invalidates prompt caching for message history. For more details, see the [Extended thinking with prompt caching](#extended-thinking-with-prompt-caching) section.\n</Note>\n\n<section title=\"Example: Passing thinking blocks with tool results\">\n\nHere's a practical example showing how to preserve thinking blocks when providing tool results:\n\n<CodeGroup>\n```python Python\nweather_tool = {\n    \"name\": \"get_weather\",\n    \"description\": \"Get current weather for a location\",\n    \"input_schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\"type\": \"string\"}\n        },\n        \"required\": [\"location\"]\n    }\n}\n\n# First request - Claude responds with thinking and tool request\nresponse = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    tools=[weather_tool],\n    messages=[\n        {\"role\": \"user\", \"content\": \"What's the weather in Paris?\"}\n    ]\n)\n```\n\n```typescript TypeScript\nconst weatherTool = {\n  name: \"get_weather\",\n  description: \"Get current weather for a location\",\n  input_schema: {\n    type: \"object\",\n    properties: {\n      location: { type: \"string\" }\n    },\n    required: [\"location\"]\n  }\n};\n\n// First request - Claude responds with thinking and tool request\nconst response = await client.messages.create({\n  model: \"claude-sonnet-4-5\",\n  max_tokens: 16000,\n  thinking: {\n    type: \"enabled\",\n    budget_tokens: 10000\n  },\n  tools: [weatherTool],\n  messages: [\n    { role: \"user\", content: \"What's the weather in Paris?\" }\n  ]\n});\n```\n\n```java Java\nimport java.util.List;\nimport java.util.Map;\n\nimport com.anthropic.client.AnthropicClient;\nimport com.anthropic.client.okhttp.AnthropicOkHttpClient;\nimport com.anthropic.core.JsonValue;\nimport com.anthropic.models.beta.messages.BetaMessage;\nimport com.anthropic.models.beta.messages.MessageCreateParams;\nimport com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;\nimport com.anthropic.models.beta.messages.BetaTool;\nimport com.anthropic.models.beta.messages.BetaTool.InputSchema;\nimport com.anthropic.models.messages.Model;\n\npublic class ThinkingWithToolsExample {\n    public static void main(String[] args) {\n        AnthropicClient client = AnthropicOkHttpClient.fromEnv();\n\n        InputSchema schema = InputSchema.builder()\n                .properties(JsonValue.from(Map.of(\n                        \"location\", Map.of(\"type\", \"string\")\n                )))\n                .putAdditionalProperty(\"required\", JsonValue.from(List.of(\"location\")))\n                .build();\n\n        BetaTool weatherTool = BetaTool.builder()\n                .name(\"get_weather\")\n                .description(\"Get current weather for a location\")\n                .inputSchema(schema)\n                .build();\n\n        BetaMessage response = client.beta().messages().create(\n                MessageCreateParams.builder()\n                        .model(Model.CLAUDE_OPUS_4_0)\n                        .maxTokens(16000)\n                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())\n                        .addTool(weatherTool)\n                        .addUserMessage(\"What's the weather in Paris?\")\n                        .build()\n        );\n\n        System.out.println(response);\n    }\n}\n```\n</CodeGroup>\n\nThe API response will include thinking, text, and tool_use blocks:\n\n```json\n{\n    \"content\": [\n        {\n            \"type\": \"thinking\",\n            \"thinking\": \"The user wants to know the current weather in Paris. I have access to a function `get_weather`...\",\n            \"signature\": \"BDaL4VrbR2Oj0hO4XpJxT28J5TILnCrrUXoKiiNBZW9P+nr8XSj1zuZzAl4egiCCpQNvfyUuFFJP5CncdYZEQPPmLxYsNrcs....\"\n        },\n        {\n            \"type\": \"text\",\n            \"text\": \"I can help you get the current weather information for Paris. Let me check that for you\"\n        },\n        {\n            \"type\": \"tool_use\",\n            \"id\": \"toolu_01CswdEQBMshySk6Y9DFKrfq\",\n            \"name\": \"get_weather\",\n            \"input\": {\n                \"location\": \"Paris\"\n            }\n        }\n    ]\n}\n```\n\nNow let's continue the conversation and use the tool\n\n<CodeGroup>\n```python Python\n# Extract thinking block and tool use block\nthinking_block = next((block for block in response.content\n                      if block.type == 'thinking'), None)\ntool_use_block = next((block for block in response.content\n                      if block.type == 'tool_use'), None)\n\n# Call your actual weather API, here is where your actual API call would go\n# let's pretend this is what we get back\nweather_data = {\"temperature\": 88}\n\n# Second request - Include thinking block and tool result\n# No new thinking blocks will be generated in the response\ncontinuation = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    tools=[weather_tool],\n    messages=[\n        {\"role\": \"user\", \"content\": \"What's the weather in Paris?\"},\n        # notice that the thinking_block is passed in as well as the tool_use_block\n        # if this is not passed in, an error is raised\n        {\"role\": \"assistant\", \"content\": [thinking_block, tool_use_block]},\n        {\"role\": \"user\", \"content\": [{\n            \"type\": \"tool_result\",\n            \"tool_use_id\": tool_use_block.id,\n            \"content\": f\"Current temperature: {weather_data['temperature']}°F\"\n        }]}\n    ]\n)\n```\n\n```typescript TypeScript\n// Extract thinking block and tool use block\nconst thinkingBlock = response.content.find(block =>\n  block.type === 'thinking');\nconst toolUseBlock = response.content.find(block =>\n  block.type === 'tool_use');\n\n// Call your actual weather API, here is where your actual API call would go\n// let's pretend this is what we get back\nconst weatherData = { temperature: 88 };\n\n// Second request - Include thinking block and tool result\n// No new thinking blocks will be generated in the response\nconst continuation = await client.messages.create({\n  model: \"claude-sonnet-4-5\",\n  max_tokens: 16000,\n  thinking: {\n    type: \"enabled\",\n    budget_tokens: 10000\n  },\n  tools: [weatherTool],\n  messages: [\n    { role: \"user\", content: \"What's the weather in Paris?\" },\n    // notice that the thinkingBlock is passed in as well as the toolUseBlock\n    // if this is not passed in, an error is raised\n    { role: \"assistant\", content: [thinkingBlock, toolUseBlock] },\n    { role: \"user\", content: [{\n      type: \"tool_result\",\n      tool_use_id: toolUseBlock.id,\n      content: `Current temperature: ${weatherData.temperature}°F`\n    }]}\n  ]\n});\n```\n\n```java Java\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Optional;\n\nimport com.anthropic.client.AnthropicClient;\nimport com.anthropic.client.okhttp.AnthropicOkHttpClient;\nimport com.anthropic.core.JsonValue;\nimport com.anthropic.models.beta.messages.*;\nimport com.anthropic.models.beta.messages.BetaTool.InputSchema;\nimport com.anthropic.models.messages.Model;\n\npublic class ThinkingToolsResultExample {\n    public static void main(String[] args) {\n        AnthropicClient client = AnthropicOkHttpClient.fromEnv();\n\n        InputSchema schema = InputSchema.builder()\n                .properties(JsonValue.from(Map.of(\n                        \"location\", Map.of(\"type\", \"string\")\n                )))\n                .putAdditionalProperty(\"required\", JsonValue.from(List.of(\"location\")))\n                .build();\n\n        BetaTool weatherTool = BetaTool.builder()\n                .name(\"get_weather\")\n                .description(\"Get current weather for a location\")\n                .inputSchema(schema)\n                .build();\n\n        BetaMessage response = client.beta().messages().create(\n                MessageCreateParams.builder()\n                        .model(Model.CLAUDE_OPUS_4_0)\n                        .maxTokens(16000)\n                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())\n                        .addTool(weatherTool)\n                        .addUserMessage(\"What's the weather in Paris?\")\n                        .build()\n        );\n\n        // Extract thinking block and tool use block\n        Optional<BetaThinkingBlock> thinkingBlockOpt = response.content().stream()\n                .filter(BetaContentBlock::isThinking)\n                .map(BetaContentBlock::asThinking)\n                .findFirst();\n\n        Optional<BetaToolUseBlock> toolUseBlockOpt = response.content().stream()\n                .filter(BetaContentBlock::isToolUse)\n                .map(BetaContentBlock::asToolUse)\n                .findFirst();\n\n        if (thinkingBlockOpt.isPresent() && toolUseBlockOpt.isPresent()) {\n            BetaThinkingBlock thinkingBlock = thinkingBlockOpt.get();\n            BetaToolUseBlock toolUseBlock = toolUseBlockOpt.get();\n\n            // Call your actual weather API, here is where your actual API call would go\n            // let's pretend this is what we get back\n            Map<String, Object> weatherData = Map.of(\"temperature\", 88);\n\n            // Second request - Include thinking block and tool result\n            // No new thinking blocks will be generated in the response\n            BetaMessage continuation = client.beta().messages().create(\n                    MessageCreateParams.builder()\n                            .model(Model.CLAUDE_OPUS_4_0)\n                            .maxTokens(16000)\n                            .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())\n                            .addTool(weatherTool)\n                            .addUserMessage(\"What's the weather in Paris?\")\n                            .addAssistantMessageOfBetaContentBlockParams(\n                                    // notice that the thinkingBlock is passed in as well as the toolUseBlock\n                                    // if this is not passed in, an error is raised\n                                    List.of(\n                                            BetaContentBlockParam.ofThinking(thinkingBlock.toParam()),\n                                            BetaContentBlockParam.ofToolUse(toolUseBlock.toParam())\n                                    )\n                            )\n                            .addUserMessageOfBetaContentBlockParams(List.of(\n                                    BetaContentBlockParam.ofToolResult(\n                                            BetaToolResultBlockParam.builder()\n                                                    .toolUseId(toolUseBlock.id())\n                                                    .content(String.format(\"Current temperature: %d°F\", (Integer)weatherData.get(\"temperature\")))\n                                                    .build()\n                                    )\n                            ))\n                            .build()\n            );\n\n            System.out.println(continuation);\n        }\n    }\n}\n```\n</CodeGroup>\n\nThe API response will now **only** include text\n\n```json\n{\n    \"content\": [\n        {\n            \"type\": \"text\",\n            \"text\": \"Currently in Paris, the temperature is 88°F (31°C)\"\n        }\n    ]\n}\n```\n\n</section>\n\n### Preserving thinking blocks\n\nDuring tool use, you must pass `thinking` blocks back to the API, and you must include the complete unmodified block back to the API. This is critical for maintaining the model's reasoning flow and conversation integrity.\n\n<Tip>\nWhile you can omit `thinking` blocks from prior `assistant` role turns, we suggest always passing back all thinking blocks to the API for any multi-turn conversation. The API will:\n- Automatically filter the provided thinking blocks\n- Use the relevant thinking blocks necessary to preserve the model's reasoning\n- Only bill for the input tokens for the blocks shown to Claude\n</Tip>\n\n<Note>\nWhen toggling thinking modes during a conversation, remember that the entire assistant turn (including tool use loops) must operate in a single thinking mode. For more details, see [Toggling thinking modes in conversations](#toggling-thinking-modes-in-conversations).\n</Note>\n\nWhen Claude invokes tools, it is pausing its construction of a response to await external information. When tool results are returned, Claude will continue building that existing response. This necessitates preserving thinking blocks during tool use, for a couple of reasons:\n\n1. **Reasoning continuity**: The thinking blocks capture Claude's step-by-step reasoning that led to tool requests. When you post tool results, including the original thinking ensures Claude can continue its reasoning from where it left off.\n\n2. **Context maintenance**: While tool results appear as user messages in the API structure, they're part of a continuous reasoning flow. Preserving thinking blocks maintains this conceptual flow across multiple API calls. For more information on context management, see our [guide on context windows](/docs/en/build-with-claude/context-windows).\n\n**Important**: When providing `thinking` blocks, the entire sequence of consecutive `thinking` blocks must match the outputs generated by the model during the original request; you cannot rearrange or modify the sequence of these blocks.\n\n### Interleaved thinking\n\nExtended thinking with tool use in Claude 4 models supports interleaved thinking, which enables Claude to think between tool calls and make more sophisticated reasoning after receiving tool results.\n\nWith interleaved thinking, Claude can:\n- Reason about the results of a tool call before deciding what to do next\n- Chain multiple tool calls with reasoning steps in between\n- Make more nuanced decisions based on intermediate results\n\nTo enable interleaved thinking, add [the beta header](/docs/en/api/beta-headers) `interleaved-thinking-2025-05-14` to your API request.\n\nHere are some important considerations for interleaved thinking:\n- With interleaved thinking, the `budget_tokens` can exceed the `max_tokens` parameter, as it represents the total budget across all thinking blocks within one assistant turn.\n- Interleaved thinking is only supported for [tools used via the Messages API](/docs/en/agents-and-tools/tool-use/overview).\n- Interleaved thinking is supported for Claude 4 models only, with the beta header `interleaved-thinking-2025-05-14`.\n- Direct calls to the Claude API allow you to pass `interleaved-thinking-2025-05-14` in requests to any model, with no effect.\n- On 3rd-party platforms (e.g., [Amazon Bedrock](/docs/en/build-with-claude/claude-on-amazon-bedrock) and [Vertex AI](/docs/en/build-with-claude/claude-on-vertex-ai)), if you pass `interleaved-thinking-2025-05-14` to any model aside from Claude Opus 4.5, Claude Opus 4.1, Opus 4, or Sonnet 4, your request will fail.\n\n<section title=\"Tool use without interleaved thinking\">\n\nWithout interleaved thinking, Claude thinks once at the start of the assistant turn. Subsequent responses after tool results continue without new thinking blocks.\n\n```\nUser: \"What's the total revenue if we sold 150 units at $50 each,\n       and how does this compare to our average monthly revenue?\"\n\nTurn 1: [thinking] \"I need to calculate 150 * $50, then check the database...\"\n        [tool_use: calculator] { \"expression\": \"150 * 50\" }\n  ↓ tool result: \"7500\"\n\nTurn 2: [tool_use: database_query] { \"query\": \"SELECT AVG(revenue)...\" }\n        ↑ no thinking block\n  ↓ tool result: \"5200\"\n\nTurn 3: [text] \"The total revenue is $7,500, which is 44% above your\n        average monthly revenue of $5,200.\"\n        ↑ no thinking block\n```\n\n</section>\n\n<section title=\"Tool use with interleaved thinking\">\n\nWith interleaved thinking enabled, Claude can think after receiving each tool result, allowing it to reason about intermediate results before continuing.\n\n```\nUser: \"What's the total revenue if we sold 150 units at $50 each,\n       and how does this compare to our average monthly revenue?\"\n\nTurn 1: [thinking] \"I need to calculate 150 * $50 first...\"\n        [tool_use: calculator] { \"expression\": \"150 * 50\" }\n  ↓ tool result: \"7500\"\n\nTurn 2: [thinking] \"Got $7,500. Now I should query the database to compare...\"\n        [tool_use: database_query] { \"query\": \"SELECT AVG(revenue)...\" }\n        ↑ thinking after receiving calculator result\n  ↓ tool result: \"5200\"\n\nTurn 3: [thinking] \"$7,500 vs $5,200 average - that's a 44% increase...\"\n        [text] \"The total revenue is $7,500, which is 44% above your\n        average monthly revenue of $5,200.\"\n        ↑ thinking before final answer\n```\n\n</section>\n\n## Extended thinking with prompt caching\n\n[Prompt caching](/docs/en/build-with-claude/prompt-caching) with thinking has several important considerations:\n\n<Tip>\nExtended thinking tasks often take longer than 5 minutes to complete. Consider using the [1-hour cache duration](/docs/en/build-with-claude/prompt-caching#1-hour-cache-duration) to maintain cache hits across longer thinking sessions and multi-step workflows.\n</Tip>\n\n**Thinking block context removal**\n- Thinking blocks from previous turns are removed from context, which can affect cache breakpoints\n- When continuing conversations with tool use, thinking blocks are cached and count as input tokens when read from cache\n- This creates a tradeoff: while thinking blocks don't consume context window space visually, they still count toward your input token usage when cached\n- If thinking becomes disabled, requests will fail if you pass thinking content in the current tool use turn. In other contexts, thinking content passed to the API is simply ignored\n\n**Cache invalidation patterns**\n- Changes to thinking parameters (enabled/disabled or budget allocation) invalidate message cache breakpoints\n- [Interleaved thinking](#interleaved-thinking) amplifies cache invalidation, as thinking blocks can occur between multiple [tool calls](#extended-thinking-with-tool-use)\n- System prompts and tools remain cached despite thinking parameter changes or block removal\n\n<Note>\nWhile thinking blocks are removed for caching and context calculations, they must be preserved when continuing conversations with [tool use](#extended-thinking-with-tool-use), especially with [interleaved thinking](#interleaved-thinking).\n</Note>\n\n### Understanding thinking block caching behavior\n\nWhen using extended thinking with tool use, thinking blocks exhibit specific caching behavior that affects token counting:\n\n**How it works:**\n\n1. Caching only occurs when you make a subsequent request that includes tool results\n2. When the subsequent request is made, the previous conversation history (including thinking blocks) can be cached\n3. These cached thinking blocks count as input tokens in your usage metrics when read from the cache\n4. When a non-tool-result user block is included, all previous thinking blocks are ignored and stripped from context\n\n**Detailed example flow:**\n\n**Request 1:**\n```\nUser: \"What's the weather in Paris?\"\n```\n**Response 1:**\n```\n[thinking_block_1] + [tool_use block 1]\n```\n\n**Request 2:**\n```\nUser: [\"What's the weather in Paris?\"], \nAssistant: [thinking_block_1] + [tool_use block 1], \nUser: [tool_result_1, cache=True]\n```\n**Response 2:**\n```\n[thinking_block_2] + [text block 2]\n```\nRequest 2 writes a cache of the request content (not the response). The cache includes the original user message, the first thinking block, tool use block, and the tool result.\n\n**Request 3:**\n```\nUser: [\"What's the weather in Paris?\"],\nAssistant: [thinking_block_1] + [tool_use block 1],\nUser: [tool_result_1, cache=True],\nAssistant: [thinking_block_2] + [text block 2],\nUser: [Text response, cache=True]\n```\nFor Claude Opus 4.5 and later, all previous thinking blocks are kept by default. For older models, because a non-tool-result user block was included, all previous thinking blocks are ignored. This request will be processed the same as:\n```\nUser: [\"What's the weather in Paris?\"],\nAssistant: [tool_use block 1],\nUser: [tool_result_1, cache=True],\nAssistant: [text block 2],\nUser: [Text response, cache=True]\n```\n\n**Key points:**\n- This caching behavior happens automatically, even without explicit `cache_control` markers\n- This behavior is consistent whether using regular thinking or interleaved thinking\n\n<section title=\"System prompt caching (preserved when thinking changes)\">\n\n<CodeGroup>\n```python Python\nfrom anthropic import Anthropic\nimport requests\nfrom bs4 import BeautifulSoup\n\nclient = Anthropic()\n\ndef fetch_article_content(url):\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    # Remove script and style elements\n    for script in soup([\"script\", \"style\"]):\n        script.decompose()\n\n    # Get text\n    text = soup.get_text()\n\n    # Break into lines and remove leading and trailing space on each\n    lines = (line.strip() for line in text.splitlines())\n    # Break multi-headlines into a line each\n    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n    # Drop blank lines\n    text = '\\n'.join(chunk for chunk in chunks if chunk)\n\n    return text\n\n# Fetch the content of the article\nbook_url = \"https://www.gutenberg.org/cache/epub/1342/pg1342.txt\"\nbook_content = fetch_article_content(book_url)\n# Use just enough text for caching (first few chapters)\nLARGE_TEXT = book_content[:5000]\n\nSYSTEM_PROMPT=[\n    {\n        \"type\": \"text\",\n        \"text\": \"You are an AI assistant that is tasked with literary analysis. Analyze the following text carefully.\",\n    },\n    {\n        \"type\": \"text\",\n        \"text\": LARGE_TEXT,\n        \"cache_control\": {\"type\": \"ephemeral\"}\n    }\n]\n\nMESSAGES = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Analyze the tone of this passage.\"\n    }\n]\n\n# First request - establish cache\nprint(\"First request - establishing cache\")\nresponse1 = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=20000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 4000\n    },\n    system=SYSTEM_PROMPT,\n    messages=MESSAGES\n)\n\nprint(f\"First response usage: {response1.usage}\")\n\nMESSAGES.append({\n    \"role\": \"assistant\",\n    \"content\": response1.content\n})\nMESSAGES.append({\n    \"role\": \"user\",\n    \"content\": \"Analyze the characters in this passage.\"\n})\n# Second request - same thinking parameters (cache hit expected)\nprint(\"\\nSecond request - same thinking parameters (cache hit expected)\")\nresponse2 = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=20000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 4000\n    },\n    system=SYSTEM_PROMPT,\n    messages=MESSAGES\n)\n\nprint(f\"Second response usage: {response2.usage}\")\n\n# Third request - different thinking parameters (cache miss for messages)\nprint(\"\\nThird request - different thinking parameters (cache miss for messages)\")\nresponse3 = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=20000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 8000  # Changed thinking budget\n    },\n    system=SYSTEM_PROMPT,  # System prompt remains cached\n    messages=MESSAGES  # Messages cache is invalidated\n)\n\nprint(f\"Third response usage: {response3.usage}\")\n```\n\n```typescript TypeScript\nimport Anthropic from '@anthropic-ai/sdk';\nimport axios from 'axios';\nimport * as cheerio from 'cheerio';\n\nconst client = new Anthropic();\n\nasync function fetchArticleContent(url: string): Promise<string> {\n  const response = await axios.get(url);\n  const $ = cheerio.load(response.data);\n  \n  // Remove script and style elements\n  $('script, style').remove();\n  \n  // Get text\n  let text = $.text();\n  \n  // Break into lines and remove leading and trailing space on each\n  const lines = text.split('\\n').map(line => line.trim());\n  // Drop blank lines\n  text = lines.filter(line => line.length > 0).join('\\n');\n  \n  return text;\n}\n\n// Fetch the content of the article\nconst bookUrl = \"https://www.gutenberg.org/cache/epub/1342/pg1342.txt\";\nconst bookContent = await fetchArticleContent(bookUrl);\n// Use just enough text for caching (first few chapters)\nconst LARGE_TEXT = bookContent.slice(0, 5000);\n\nconst SYSTEM_PROMPT = [\n  {\n    type: \"text\",\n    text: \"You are an AI assistant that is tasked with literary analysis. Analyze the following text carefully.\",\n  },\n  {\n    type: \"text\",\n    text: LARGE_TEXT,\n    cache_control: { type: \"ephemeral\" }\n  }\n];\n\nconst MESSAGES = [\n  {\n    role: \"user\",\n    content: \"Analyze the tone of this passage.\"\n  }\n];\n\n// First request - establish cache\nconsole.log(\"First request - establishing cache\");\nconst response1 = await client.messages.create({\n  model: \"claude-sonnet-4-5\",\n  max_tokens: 20000,\n  thinking: {\n    type: \"enabled\",\n    budget_tokens: 4000\n  },\n  system: SYSTEM_PROMPT,\n  messages: MESSAGES\n});\n\nconsole.log(`First response usage: ${response1.usage}`);\n\nMESSAGES.push({\n  role: \"assistant\",\n  content: response1.content\n});\nMESSAGES.push({\n  role: \"user\",\n  content: \"Analyze the characters in this passage.\"\n});\n\n// Second request - same thinking parameters (cache hit expected)\nconsole.log(\"\\nSecond request - same thinking parameters (cache hit expected)\");\nconst response2 = await client.messages.create({\n  model: \"claude-sonnet-4-5\",\n  max_tokens: 20000,\n  thinking: {\n    type: \"enabled\",\n    budget_tokens: 4000\n  },\n  system: SYSTEM_PROMPT,\n  messages: MESSAGES\n});\n\nconsole.log(`Second response usage: ${response2.usage}`);\n\n// Third request - different thinking parameters (cache miss for messages)\nconsole.log(\"\\nThird request - different thinking parameters (cache miss for messages)\");\nconst response3 = await client.messages.create({\n  model: \"claude-sonnet-4-5\",\n  max_tokens: 20000,\n  thinking: {\n    type: \"enabled\",\n    budget_tokens: 8000  // Changed thinking budget\n  },\n  system: SYSTEM_PROMPT,  // System prompt remains cached\n  messages: MESSAGES  // Messages cache is invalidated\n});\n\nconsole.log(`Third response usage: ${response3.usage}`);\n```\n</CodeGroup>\n\n</section>\n<section title=\"Messages caching (invalidated when thinking changes)\">\n\n<CodeGroup>\n```python Python\nfrom anthropic import Anthropic\nimport requests\nfrom bs4 import BeautifulSoup\n\nclient = Anthropic()\n\ndef fetch_article_content(url):\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    # Remove script and style elements\n    for script in soup([\"script\", \"style\"]):\n        script.decompose()\n\n    # Get text\n    text = soup.get_text()\n\n    # Break into lines and remove leading and trailing space on each\n    lines = (line.strip() for line in text.splitlines())\n    # Break multi-headlines into a line each\n    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n    # Drop blank lines\n    text = '\\n'.join(chunk for chunk in chunks if chunk)\n\n    return text\n\n# Fetch the content of the article\nbook_url = \"https://www.gutenberg.org/cache/epub/1342/pg1342.txt\"\nbook_content = fetch_article_content(book_url)\n# Use just enough text for caching (first few chapters)\nLARGE_TEXT = book_content[:5000]\n\n# No system prompt - caching in messages instead\nMESSAGES = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"text\",\n                \"text\": LARGE_TEXT,\n                \"cache_control\": {\"type\": \"ephemeral\"},\n            },\n            {\n                \"type\": \"text\",\n                \"text\": \"Analyze the tone of this passage.\"\n            }\n        ]\n    }\n]\n\n# First request - establish cache\nprint(\"First request - establishing cache\")\nresponse1 = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=20000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 4000\n    },\n    messages=MESSAGES\n)\n\nprint(f\"First response usage: {response1.usage}\")\n\nMESSAGES.append({\n    \"role\": \"assistant\",\n    \"content\": response1.content\n})\nMESSAGES.append({\n    \"role\": \"user\",\n    \"content\": \"Analyze the characters in this passage.\"\n})\n# Second request - same thinking parameters (cache hit expected)\nprint(\"\\nSecond request - same thinking parameters (cache hit expected)\")\nresponse2 = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=20000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 4000  # Same thinking budget\n    },\n    messages=MESSAGES\n)\n\nprint(f\"Second response usage: {response2.usage}\")\n\nMESSAGES.append({\n    \"role\": \"assistant\",\n    \"content\": response2.content\n})\nMESSAGES.append({\n    \"role\": \"user\",\n    \"content\": \"Analyze the setting in this passage.\"\n})\n\n# Third request - different thinking budget (cache miss expected)\nprint(\"\\nThird request - different thinking budget (cache miss expected)\")\nresponse3 = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=20000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 8000  # Different thinking budget breaks cache\n    },\n    messages=MESSAGES\n)\n\nprint(f\"Third response usage: {response3.usage}\")\n```\n\n```typescript TypeScript\nimport Anthropic from '@anthropic-ai/sdk';\nimport axios from 'axios';\nimport * as cheerio from 'cheerio';\n\nconst client = new Anthropic();\n\nasync function fetchArticleContent(url: string): Promise<string> {\n  const response = await axios.get(url);\n  const $ = cheerio.load(response.data);\n\n  // Remove script and style elements\n  $('script, style').remove();\n\n  // Get text\n  let text = $.text();\n\n  // Clean up text (break into lines, remove whitespace)\n  const lines = text.split('\\n').map(line => line.trim());\n  const chunks = lines.flatMap(line => line.split('  ').map(phrase => phrase.trim()));\n  text = chunks.filter(chunk => chunk).join('\\n');\n\n  return text;\n}\n\nasync function main() {\n  // Fetch the content of the article\n  const bookUrl = \"https://www.gutenberg.org/cache/epub/1342/pg1342.txt\";\n  const bookContent = await fetchArticleContent(bookUrl);\n  // Use just enough text for caching (first few chapters)\n  const LARGE_TEXT = bookContent.substring(0, 5000);\n\n  // No system prompt - caching in messages instead\n  let MESSAGES = [\n    {\n      role: \"user\",\n      content: [\n        {\n          type: \"text\",\n          text: LARGE_TEXT,\n          cache_control: {type: \"ephemeral\"},\n        },\n        {\n          type: \"text\",\n          text: \"Analyze the tone of this passage.\"\n        }\n      ]\n    }\n  ];\n\n  // First request - establish cache\n  console.log(\"First request - establishing cache\");\n  const response1 = await client.messages.create({\n    model: \"claude-sonnet-4-5\",\n    max_tokens: 20000,\n    thinking: {\n      type: \"enabled\",\n      budget_tokens: 4000\n    },\n    messages: MESSAGES\n  });\n\n  console.log(`First response usage: `, response1.usage);\n\n  MESSAGES = [\n    ...MESSAGES,\n    {\n      role: \"assistant\",\n      content: response1.content\n    },\n    {\n      role: \"user\",\n      content: \"Analyze the characters in this passage.\"\n    }\n  ];\n\n  // Second request - same thinking parameters (cache hit expected)\n  console.log(\"\\nSecond request - same thinking parameters (cache hit expected)\");\n  const response2 = await client.messages.create({\n    model: \"claude-sonnet-4-5\",\n    max_tokens: 20000,\n    thinking: {\n      type: \"enabled\",\n      budget_tokens: 4000  // Same thinking budget\n    },\n    messages: MESSAGES\n  });\n\n  console.log(`Second response usage: `, response2.usage);\n\n  MESSAGES = [\n    ...MESSAGES,\n    {\n      role: \"assistant\",\n      content: response2.content\n    },\n    {\n      role: \"user\",\n      content: \"Analyze the setting in this passage.\"\n    }\n  ];\n\n  // Third request - different thinking budget (cache miss expected)\n  console.log(\"\\nThird request - different thinking budget (cache miss expected)\");\n  const response3 = await client.messages.create({\n    model: \"claude-sonnet-4-5\",\n    max_tokens: 20000,\n    thinking: {\n      type: \"enabled\",\n      budget_tokens: 8000  // Different thinking budget breaks cache\n    },\n    messages: MESSAGES\n  });\n\n  console.log(`Third response usage: `, response3.usage);\n}\n\nmain().catch(console.error);\n```\n\n```java Java\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.io.BufferedReader;\nimport java.io.InputStreamReader;\nimport java.net.URL;\nimport java.util.Arrays;\nimport java.util.regex.Pattern;\n\nimport com.anthropic.client.AnthropicClient;\nimport com.anthropic.client.okhttp.AnthropicOkHttpClient;\nimport com.anthropic.models.beta.messages.*;\nimport com.anthropic.models.beta.messages.MessageCreateParams;\nimport com.anthropic.models.messages.Model;\n\nimport static java.util.stream.Collectors.joining;\nimport static java.util.stream.Collectors.toList;\n\npublic class ThinkingCacheExample {\n    public static void main(String[] args) throws IOException {\n        AnthropicClient client = AnthropicOkHttpClient.fromEnv();\n\n        // Fetch the content of the article\n        String bookUrl = \"https://www.gutenberg.org/cache/epub/1342/pg1342.txt\";\n        String bookContent = fetchArticleContent(bookUrl);\n        // Use just enough text for caching (first few chapters)\n        String largeText = bookContent.substring(0, 5000);\n\n        List<BetaTextBlockParam> systemPrompt = List.of(\n                BetaTextBlockParam.builder()\n                        .text(\"You are an AI assistant that is tasked with literary analysis. Analyze the following text carefully.\")\n                        .build(),\n                BetaTextBlockParam.builder()\n                        .text(largeText)\n                        .cacheControl(BetaCacheControlEphemeral.builder().build())\n                        .build()\n        );\n\n        List<BetaMessageParam> messages = new ArrayList<>();\n        messages.add(BetaMessageParam.builder()\n                .role(BetaMessageParam.Role.USER)\n                .content(\"Analyze the tone of this passage.\")\n                .build());\n\n        // First request - establish cache\n        System.out.println(\"First request - establishing cache\");\n        BetaMessage response1 = client.beta().messages().create(\n                MessageCreateParams.builder()\n                        .model(Model.CLAUDE_OPUS_4_0)\n                        .maxTokens(20000)\n                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(4000).build())\n                        .systemOfBetaTextBlockParams(systemPrompt)\n                        .messages(messages)\n                        .build()\n        );\n\n        System.out.println(\"First response usage: \" + response1.usage());\n\n        // Second request - same thinking parameters (cache hit expected)\n        System.out.println(\"\\nSecond request - same thinking parameters (cache hit expected)\");\n        BetaMessage response2 = client.beta().messages().create(\n                MessageCreateParams.builder()\n                        .model(Model.CLAUDE_OPUS_4_0)\n                        .maxTokens(20000)\n                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(4000).build())\n                        .systemOfBetaTextBlockParams(systemPrompt)\n                        .addMessage(response1)\n                        .addUserMessage(\"Analyze the characters in this passage.\")\n                        .messages(messages)\n                        .build()\n        );\n\n        System.out.println(\"Second response usage: \" + response2.usage());\n\n        // Third request - different thinking budget (cache hit expected because system prompt caching)\n        System.out.println(\"\\nThird request - different thinking budget (cache hit expected)\");\n        BetaMessage response3 = client.beta().messages().create(\n                MessageCreateParams.builder()\n                        .model(Model.CLAUDE_OPUS_4_0)\n                        .maxTokens(20000)\n                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(8000).build())\n                        .systemOfBetaTextBlockParams(systemPrompt)\n                        .addMessage(response1)\n                        .addUserMessage(\"Analyze the characters in this passage.\")\n                        .addMessage(response2)\n                        .addUserMessage(\"Analyze the setting in this passage.\")\n                        .build()\n        );\n\n        System.out.println(\"Third response usage: \" + response3.usage());\n    }\n\n    private static String fetchArticleContent(String url) throws IOException {\n        // Fetch HTML content\n        String htmlContent = fetchHtml(url);\n\n        // Remove script and style elements\n        String noScriptStyle = removeElements(htmlContent, \"script\", \"style\");\n\n        // Extract text (simple approach - remove HTML tags)\n        String text = removeHtmlTags(noScriptStyle);\n\n        // Clean up text (break into lines, remove whitespace)\n        List<String> lines = Arrays.asList(text.split(\"\\n\"));\n        List<String> trimmedLines = lines.stream()\n                .map(String::trim)\n                .collect(toList());\n\n        // Split on double spaces and flatten\n        List<String> chunks = trimmedLines.stream()\n                .flatMap(line -> Arrays.stream(line.split(\"  \"))\n                        .map(String::trim))\n                .collect(toList());\n\n        // Filter empty chunks and join with newlines\n        return chunks.stream()\n                .filter(chunk -> !chunk.isEmpty())\n                .collect(joining(\"\\n\"));\n    }\n\n    /**\n     * Fetches HTML content from a URL\n     */\n    private static String fetchHtml(String urlString) throws IOException {\n        try (InputStream inputStream = new URL(urlString).openStream()) {\n            StringBuilder content = new StringBuilder();\n            try (BufferedReader reader = new BufferedReader(\n                    new InputStreamReader(inputStream))) {\n                String line;\n                while ((line = reader.readLine()) != null) {\n                    content.append(line).append(\"\\n\");\n                }\n            }\n            return content.toString();\n        }\n    }\n\n    /**\n     * Removes specified HTML elements and their content\n     */\n    private static String removeElements(String html, String... elementNames) {\n        String result = html;\n        for (String element : elementNames) {\n            // Pattern to match <element>...</element> and self-closing tags\n            String pattern = \"<\" + element + \"\\\\s*[^>]*>.*?</\" + element + \">|<\" + element + \"\\\\s*[^>]*/?>\";\n            result = Pattern.compile(pattern, Pattern.DOTALL).matcher(result).replaceAll(\"\");\n        }\n        return result;\n    }\n\n    /**\n     * Removes all HTML tags from content\n     */\n    private static String removeHtmlTags(String html) {\n        // Replace <br> and <p> tags with newlines for better text formatting\n        String withLineBreaks = html.replaceAll(\"<br\\\\s*/?\\\\s*>|</?p\\\\s*[^>]*>\", \"\\n\");\n\n        // Remove remaining HTML tags\n        String noTags = withLineBreaks.replaceAll(\"<[^>]*>\", \"\");\n\n        // Decode HTML entities (simplified for common entities)\n        return decodeHtmlEntities(noTags);\n    }\n\n    /**\n     * Simple HTML entity decoder for common entities\n     */\n    private static String decodeHtmlEntities(String text) {\n        return text\n                .replaceAll(\"&nbsp;\", \" \")\n                .replaceAll(\"&amp;\", \"&\")\n                .replaceAll(\"&lt;\", \"<\")\n                .replaceAll(\"&gt;\", \">\")\n                .replaceAll(\"&quot;\", \"\\\"\")\n                .replaceAll(\"&#39;\", \"'\")\n                .replaceAll(\"&hellip;\", \"...\")\n                .replaceAll(\"&mdash;\", \"—\");\n    }\n\n}\n```\n</CodeGroup>\n\nHere is the output of the script (you may see slightly different numbers)\n\n```\nFirst request - establishing cache\nFirst response usage: { cache_creation_input_tokens: 1370, cache_read_input_tokens: 0, input_tokens: 17, output_tokens: 700 }\n\nSecond request - same thinking parameters (cache hit expected)\n\nSecond response usage: { cache_creation_input_tokens: 0, cache_read_input_tokens: 1370, input_tokens: 303, output_tokens: 874 }\n\nThird request - different thinking budget (cache miss expected)\nThird response usage: { cache_creation_input_tokens: 1370, cache_read_input_tokens: 0, input_tokens: 747, output_tokens: 619 }\n```\n\nThis example demonstrates that when caching is set up in the messages array, changing the thinking parameters (budget_tokens increased from 4000 to 8000) **invalidates the cache**. The third request shows no cache hit with `cache_creation_input_tokens=1370` and `cache_read_input_tokens=0`, proving that message-based caching is invalidated when thinking parameters change.\n\n</section>\n\n## Max tokens and context window size with extended thinking\n\nIn older Claude models (prior to Claude Sonnet 3.7), if the sum of prompt tokens and `max_tokens` exceeded the model's context window, the system would automatically adjust `max_tokens` to fit within the context limit. This meant you could set a large `max_tokens` value and the system would silently reduce it as needed.\n\nWith Claude 3.7 and 4 models, `max_tokens` (which includes your thinking budget when thinking is enabled) is enforced as a strict limit. The system will now return a validation error if prompt tokens + `max_tokens` exceeds the context window size.\n\n<Note>\nYou can read through our [guide on context windows](/docs/en/build-with-claude/context-windows) for a more thorough deep dive.\n</Note>\n\n### The context window with extended thinking\n\nWhen calculating context window usage with thinking enabled, there are some considerations to be aware of:\n\n- Thinking blocks from previous turns are stripped and not counted towards your context window\n- Current turn thinking counts towards your `max_tokens` limit for that turn\n\nThe diagram below demonstrates the specialized token management when extended thinking is enabled:\n\n![Context window diagram with extended thinking](/docs/images/context-window-thinking.svg)\n\nThe effective context window is calculated as:\n\n```\ncontext window =\n  (current input tokens - previous thinking tokens) +\n  (thinking tokens + encrypted thinking tokens + text output tokens)\n```\n\nWe recommend using the [token counting API](/docs/en/build-with-claude/token-counting) to get accurate token counts for your specific use case, especially when working with multi-turn conversations that include thinking.\n\n### The context window with extended thinking and tool use\n\nWhen using extended thinking with tool use, thinking blocks must be explicitly preserved and returned with the tool results.\n\nThe effective context window calculation for extended thinking with tool use becomes: \n\n```\ncontext window =\n  (current input tokens + previous thinking tokens + tool use tokens) +\n  (thinking tokens + encrypted thinking tokens + text output tokens)\n```\n\nThe diagram below illustrates token management for extended thinking with tool use:\n\n![Context window diagram with extended thinking and tool use](/docs/images/context-window-thinking-tools.svg)\n\n### Managing tokens with extended thinking\n\nGiven the context window and `max_tokens` behavior with extended thinking Claude 3.7 and 4 models, you may need to:\n\n- More actively monitor and manage your token usage\n- Adjust `max_tokens` values as your prompt length changes\n- Potentially use the [token counting endpoints](/docs/en/build-with-claude/token-counting) more frequently\n- Be aware that previous thinking blocks don't accumulate in your context window\n\nThis change has been made to provide more predictable and transparent behavior, especially as maximum token limits have increased significantly.\n\n## Thinking encryption\n\nFull thinking content is encrypted and returned in the `signature` field. This field is used to verify that thinking blocks were generated by Claude when passed back to the API. \n\n<Note>\nIt is only strictly necessary to send back thinking blocks when using [tools with extended thinking](#extended-thinking-with-tool-use). Otherwise you can omit thinking blocks from previous turns, or let the API strip them for you if you pass them back. \n\nIf sending back thinking blocks, we recommend passing everything back as you received it for consistency and to avoid potential issues.\n</Note>\n\nHere are some important considerations on thinking encryption:\n- When [streaming responses](#streaming-thinking), the signature is added via a `signature_delta` inside a `content_block_delta` event just before the `content_block_stop` event.\n- `signature` values are significantly longer in Claude 4 models than in previous models.\n- The `signature` field is an opaque field and should not be interpreted or parsed - it exists solely for verification purposes.\n- `signature` values are compatible across platforms (Claude APIs, [Amazon Bedrock](/docs/en/build-with-claude/claude-on-amazon-bedrock), and [Vertex AI](/docs/en/build-with-claude/claude-on-vertex-ai)). Values generated on one platform will be compatible with another.\n\n### Thinking redaction\n\nOccasionally Claude's internal reasoning will be flagged by our safety systems. When this occurs, we encrypt some or all of the `thinking` block and return it to you as a `redacted_thinking` block. `redacted_thinking` blocks are decrypted when passed back to the API, allowing Claude to continue its response without losing context.\n\nWhen building customer-facing applications that use extended thinking:\n\n- Be aware that redacted thinking blocks contain encrypted content that isn't human-readable\n- Consider providing a simple explanation like: \"Some of Claude's internal reasoning has been automatically encrypted for safety reasons. This doesn't affect the quality of responses.\"\n- If showing thinking blocks to users, you can filter out redacted blocks while preserving normal thinking blocks\n- Be transparent that using extended thinking features may occasionally result in some reasoning being encrypted\n- Implement appropriate error handling to gracefully manage redacted thinking without breaking your UI\n\nHere's an example showing both normal and redacted thinking blocks:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"thinking\",\n      \"thinking\": \"Let me analyze this step by step...\",\n      \"signature\": \"WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL....\"\n    },\n    {\n      \"type\": \"redacted_thinking\",\n      \"data\": \"EmwKAhgBEgy3va3pzix/LafPsn4aDFIT2Xlxh0L5L8rLVyIwxtE3rAFBa8cr3qpPkNRj2YfWXGmKDxH4mPnZ5sQ7vB9URj2pLmN3kF8/dW5hR7xJ0aP1oLs9yTcMnKVf2wRpEGjH9XZaBt4UvDcPrQ...\"\n    },\n    {\n      \"type\": \"text\",\n      \"text\": \"Based on my analysis...\"\n    }\n  ]\n}\n```\n\n<Note>\nSeeing redacted thinking blocks in your output is expected behavior. The model can still use this redacted reasoning to inform its responses while maintaining safety guardrails.\n\nIf you need to test redacted thinking handling in your application, you can use this special test string as your prompt: `ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB`\n</Note>\n\nWhen passing `thinking` and `redacted_thinking` blocks back to the API in a multi-turn conversation, you must include the complete unmodified block back to the API for the last assistant turn. This is critical for maintaining the model's reasoning flow. We suggest always passing back all thinking blocks to the API. For more details, see the [Preserving thinking blocks](#preserving-thinking-blocks) section above.\n\n<section title=\"Example: Working with redacted thinking blocks\">\n\nThis example demonstrates how to handle `redacted_thinking` blocks that may appear in responses when Claude's internal reasoning contains content flagged by safety systems:\n\n<CodeGroup>\n```python Python\nimport anthropic\n\nclient = anthropic.Anthropic()\n\n# Using a special prompt that triggers redacted thinking (for demonstration purposes only)\nresponse = client.messages.create(\n    model=\"claude-sonnet-4-5-20250929\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB\"\n    }]\n)\n\n# Identify redacted thinking blocks\nhas_redacted_thinking = any(\n    block.type == \"redacted_thinking\" for block in response.content\n)\n\nif has_redacted_thinking:\n    print(\"Response contains redacted thinking blocks\")\n    # These blocks are still usable in subsequent requests\n\n    # Extract all blocks (both redacted and non-redacted)\n    all_thinking_blocks = [\n        block for block in response.content\n        if block.type in [\"thinking\", \"redacted_thinking\"]\n    ]\n\n    # When passing to subsequent requests, include all blocks without modification\n    # This preserves the integrity of Claude's reasoning\n\n    print(f\"Found {len(all_thinking_blocks)} thinking blocks total\")\n    print(f\"These blocks are still billable as output tokens\")\n```\n\n```typescript TypeScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst client = new Anthropic();\n\n// Using a special prompt that triggers redacted thinking (for demonstration purposes only)\nconst response = await client.messages.create({\n  model: \"claude-sonnet-4-5-20250929\",\n  max_tokens: 16000,\n  thinking: {\n    type: \"enabled\",\n    budget_tokens: 10000\n  },\n  messages: [{\n    role: \"user\",\n    content: \"ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB\"\n  }]\n});\n\n// Identify redacted thinking blocks\nconst hasRedactedThinking = response.content.some(\n  block => block.type === \"redacted_thinking\"\n);\n\nif (hasRedactedThinking) {\n  console.log(\"Response contains redacted thinking blocks\");\n  // These blocks are still usable in subsequent requests\n\n  // Extract all blocks (both redacted and non-redacted)\n  const allThinkingBlocks = response.content.filter(\n    block => block.type === \"thinking\" || block.type === \"redacted_thinking\"\n  );\n\n  // When passing to subsequent requests, include all blocks without modification\n  // This preserves the integrity of Claude's reasoning\n\n  console.log(`Found ${allThinkingBlocks.length} thinking blocks total`);\n  console.log(`These blocks are still billable as output tokens`);\n}\n```\n\n```java Java\nimport java.util.List;\n\nimport static java.util.stream.Collectors.toList;\n\nimport com.anthropic.client.AnthropicClient;\nimport com.anthropic.client.okhttp.AnthropicOkHttpClient;\nimport com.anthropic.models.beta.messages.BetaContentBlock;\nimport com.anthropic.models.beta.messages.BetaMessage;\nimport com.anthropic.models.beta.messages.MessageCreateParams;\nimport com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;\nimport com.anthropic.models.messages.Model;\n\npublic class RedactedThinkingExample {\n    public static void main(String[] args) {\n        AnthropicClient client = AnthropicOkHttpClient.fromEnv();\n\n        // Using a special prompt that triggers redacted thinking (for demonstration purposes only)\n        BetaMessage response = client.beta().messages().create(\n                MessageCreateParams.builder()\n                        .model(Model.CLAUDE_SONNET_4_5)\n                        .maxTokens(16000)\n                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())\n                        .addUserMessage(\"ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB\")\n                        .build()\n        );\n\n        // Identify redacted thinking blocks\n        boolean hasRedactedThinking = response.content().stream()\n                .anyMatch(BetaContentBlock::isRedactedThinking);\n\n        if (hasRedactedThinking) {\n            System.out.println(\"Response contains redacted thinking blocks\");\n            // These blocks are still usable in subsequent requests\n            // Extract all blocks (both redacted and non-redacted)\n            List<BetaContentBlock> allThinkingBlocks = response.content().stream()\n                    .filter(block -> block.isThinking() ||\n                            block.isRedactedThinking())\n                    .collect(toList());\n\n            // When passing to subsequent requests, include all blocks without modification\n            // This preserves the integrity of Claude's reasoning\n            System.out.println(\"Found \" + allThinkingBlocks.size() + \" thinking blocks total\");\n            System.out.println(\"These blocks are still billable as output tokens\");\n        }\n    }\n}\n```\n\n</CodeGroup>\n\n<TryInConsoleButton\n  userPrompt=\"ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB\"\n  thinkingBudgetTokens={16000}\n>\n  Try in Console\n</TryInConsoleButton>\n\n</section>\n\n## Differences in thinking across model versions\n\nThe Messages API handles thinking differently across Claude Sonnet 3.7 and Claude 4 models, primarily in redaction and summarization behavior.\n\nSee the table below for a condensed comparison:\n\n| Feature | Claude Sonnet 3.7 | Claude 4 Models (pre-Opus 4.5) | Claude Opus 4.5 and later |\n|---------|------------------|-------------------------------|--------------------------|\n| **Thinking Output** | Returns full thinking output | Returns summarized thinking | Returns summarized thinking |\n| **Interleaved Thinking** | Not supported | Supported with `interleaved-thinking-2025-05-14` beta header | Supported with `interleaved-thinking-2025-05-14` beta header |\n| **Thinking Block Preservation** | Not preserved across turns | Not preserved across turns | **Preserved by default** (enables cache optimization, token savings) |\n\n### Thinking block preservation in Claude Opus 4.5\n\nClaude Opus 4.5 introduces a new default behavior: **thinking blocks from previous assistant turns are preserved in model context by default**. This differs from earlier models, which remove thinking blocks from prior turns.\n\n**Benefits of thinking block preservation:**\n\n- **Cache optimization**: When using tool use, preserved thinking blocks enable cache hits as they are passed back with tool results and cached incrementally across the assistant turn, resulting in token savings in multi-step workflows\n- **No intelligence impact**: Preserving thinking blocks has no negative effect on model performance\n\n**Important considerations:**\n\n- **Context usage**: Long conversations will consume more context space since thinking blocks are retained in context\n- **Automatic behavior**: This is the default behavior for Claude Opus 4.5—no code changes or beta headers required\n- **Backward compatibility**: To leverage this feature, continue passing complete, unmodified thinking blocks back to the API as you would for tool use\n\n<Note>\nFor earlier models (Claude Sonnet 4.5, Opus 4.1, etc.), thinking blocks from previous turns continue to be removed from context. The existing behavior described in the [Extended thinking with prompt caching](#extended-thinking-with-prompt-caching) section applies to those models.\n</Note>\n\n## Pricing\n\nFor complete pricing information including base rates, cache writes, cache hits, and output tokens, see the [pricing page](/docs/en/about-claude/pricing).\n\nThe thinking process incurs charges for:\n- Tokens used during thinking (output tokens)\n- Thinking blocks from the last assistant turn included in subsequent requests (input tokens)\n- Standard text output tokens\n\n<Note>\nWhen extended thinking is enabled, a specialized system prompt is automatically included to support this feature.\n</Note>\n\nWhen using summarized thinking:\n- **Input tokens**: Tokens in your original request (excludes thinking tokens from previous turns)\n- **Output tokens (billed)**: The original thinking tokens that Claude generated internally\n- **Output tokens (visible)**: The summarized thinking tokens you see in the response\n- **No charge**: Tokens used to generate the summary\n\n<Warning>\nThe billed output token count will **not** match the visible token count in the response. You are billed for the full thinking process, not the summary you see.\n</Warning>\n\n## Best practices and considerations for extended thinking\n\n### Working with thinking budgets\n\n- **Budget optimization:** The minimum budget is 1,024 tokens. We suggest starting at the minimum and increasing the thinking budget incrementally to find the optimal range for your use case. Higher token counts enable more comprehensive reasoning but with diminishing returns depending on the task. Increasing the budget can improve response quality at the tradeoff of increased latency. For critical tasks, test different settings to find the optimal balance. Note that the thinking budget is a target rather than a strict limit—actual token usage may vary based on the task.\n- **Starting points:** Start with larger thinking budgets (16k+ tokens) for complex tasks and adjust based on your needs.\n- **Large budgets:** For thinking budgets above 32k, we recommend using [batch processing](/docs/en/build-with-claude/batch-processing) to avoid networking issues. Requests pushing the model to think above 32k tokens causes long running requests that might run up against system timeouts and open connection limits.\n- **Token usage tracking:** Monitor thinking token usage to optimize costs and performance.\n\n### Performance considerations\n\n- **Response times:** Be prepared for potentially longer response times due to the additional processing required for the reasoning process. Factor in that generating thinking blocks may increase overall response time.\n- **Streaming requirements:** Streaming is required when `max_tokens` is greater than 21,333. When streaming, be prepared to handle both thinking and text content blocks as they arrive.\n\n### Feature compatibility\n\n- Thinking isn't compatible with `temperature` or `top_k` modifications as well as [forced tool use](/docs/en/agents-and-tools/tool-use/implement-tool-use#forcing-tool-use).\n- When thinking is enabled, you can set `top_p` to values between 1 and 0.95.\n- You cannot pre-fill responses when thinking is enabled.\n- Changes to the thinking budget invalidate cached prompt prefixes that include messages. However, cached system prompts and tool definitions will continue to work when thinking parameters change.\n\n### Usage guidelines\n\n- **Task selection:** Use extended thinking for particularly complex tasks that benefit from step-by-step reasoning like math, coding, and analysis.\n- **Context handling:** You do not need to remove previous thinking blocks yourself. The Claude API automatically ignores thinking blocks from previous turns and they are not included when calculating context usage.\n- **Prompt engineering:** Review our [extended thinking prompting tips](/docs/en/build-with-claude/prompt-engineering/extended-thinking-tips) if you want to maximize Claude's thinking capabilities.\n\n## Next steps\n\n<CardGroup>\n  <Card title=\"Try the extended thinking cookbook\" icon=\"book\" href=\"https://github.com/anthropics/anthropic-cookbook/tree/main/extended_thinking\">\n    Explore practical examples of thinking in our cookbook.\n  </Card>\n  <Card title=\"Extended thinking prompting tips\" icon=\"code\" href=\"/docs/en/build-with-claude/prompt-engineering/extended-thinking-tips\">\n    Learn prompt engineering best practices for extended thinking.\n  </Card>\n</CardGroup>","durationMs":1417,"url":"https://platform.claude.com/docs/en/build-with-claude/extended-thinking"}}
{"parentUuid":"77ce093c-f4b9-46b1-b122-d9da13590648","isSidechain":false,"userType":"external","cwd":"/Users/yunchang/Documents/GitHub/gemeng2","sessionId":"4ece2923-b68d-44f9-80e5-03967c577bcd","version":"2.0.69","gitBranch":"main","message":{"model":"claude-opus-4-5-20251101","id":"msg_01JyyxWkGvRpBE41ELy34jex","type":"message","role":"assistant","content":[{"type":"thinking","thinking":"根据我查到的信息，我现在可以给用户一个完整的回答了。\n\n关键点：\n1. Claude Opus 4.5 和 Sonnet 4.5 都支持 Extended Thinking（扩展思考）功能\n2. 这不是单独的\"thinking模型\"，而是同一个模型的一个功能特性\n3. 通过在API调用中添加 `thinking` 参数来启用\n\n模型ID保持不变：\n- Claude Opus 4.5: `claude-opus-4-5-20251101`\n- Claude Sonnet 4.5: `claude-sonnet-4-5-20250929`\n\n启用方式是在API请求中添加：\n```json\n\"thinking\": {\n    \"type\": \"enabled\",\n    \"budget_tokens\": 10000\n}\n```\n\n所以不存在像 `claude-opus-4-5-thinking` 这样的单独模型ID，而是通过参数来启用这个功能。","signature":"EsIGCkYIChgCKkCwubMgIGcLLGFcyihlGyy7x2eYqZZei4i6ZXS4U3kb1Gg2EcTxu5LpFbA3xhI6WbJx03IsDj2k9yC9iwn+CDhKEgzRdsCNRMO/xz4jvakaDAErIrt0V141qi6H0SIwhthYmXbrdpCFPo3SYyMAMGcKwDSNMWMyhiZGu84HmH4tcER1L1jT5GbIXUaDJwBNKqkFFavK2DsuEdaCVR/jKPJUYrevz1U5VGmowJkHqqLE3eEw1tUYlMiJamfHdspDICxPZ/l5H2VGG4s/t4287b2b9pLhtVDRYkQuLk2OeC3I5GwRGQIvZym/Sp3KC8osQrL8nKUW2rfPAOstdx3avc0REHk+tzPq7OZxB4GDz0Tp9JfH3GPSkXTEYwUg+Re8ygqQyfj76hSyisYNJXL/dbx/sFpfoIXDh/ucBM4fUWsHDd0FknyHfYyd/ZZDcMVDZfB3+ZebLMYOTDeKT7MqFw6ROOBamcgysZNoTXpQIhYyNEtcYdfdit9LXlvAV8WeJwCiJNE8h1yX0kr/6YslNLhv0r2aHUn18KnhTq7mhqkOREQDiXcSNMny7uNIY/Hu04Jf17E54iejF5L7oglLhNJbNv6DGeIcm2ALL6rPgAE4lZrCWQB566JteJnzBvaOGthrIWst/hum+0ZQCwiEuaILhrzgFX0hcH7uJDCg3Oe2Ckhv2T0oKRq7n9ARnQ3UQ5D9XfGNRHMginy9Pn5FxA+ge00ed+qThLdLguLYdAciU4ywlCXijDLuiMaSyKmvlS3l2a6wrteVZaCb7MKkwK4JMB4bjmOyAvLB6OEVqLRWGZRHCPOwrVfpkvQB/xcWezDKd0H9556EYS1wPjXaBi/foI0OperqdGl+XFhghdThDLTKaEXwTA7kd32HMYs+/IAcOHQTudtcD5GOlPWoNeRrIdO4tNSCmPJ3wxagL7gZbpq/UnQMZjKt5i84AhqqJaaigbB/n6Yt+4ZLGeKTefD4RMXjgpQEwOf7c5yJ0cGa1OdgZuq2syIIhpJuGRLRTZk2rT/rATiH+IWnymOVrhJox9Of3eJHteYGrPh2LlcU47qYCaD887o/pksqILXCTRCJk5xbl77FAJPJGAE="}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":21089,"cache_read_input_tokens":32756,"cache_creation":{"ephemeral_5m_input_tokens":21089,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"type":"assistant","uuid":"84d4b915-9d2f-48d9-95c4-1e96b7e4fe56","timestamp":"2025-12-13T02:57:15.572Z"}
{"parentUuid":"84d4b915-9d2f-48d9-95c4-1e96b7e4fe56","isSidechain":false,"userType":"external","cwd":"/Users/yunchang/Documents/GitHub/gemeng2","sessionId":"4ece2923-b68d-44f9-80e5-03967c577bcd","version":"2.0.69","gitBranch":"main","message":{"model":"claude-opus-4-5-20251101","id":"msg_01JyyxWkGvRpBE41ELy34jex","type":"message","role":"assistant","content":[{"type":"text","text":"根据官方文档，确认如下：\n\n## Extended Thinking 不是单独的模型\n\nClaude **没有**单独的 \"thinking 模型\"。Extended Thinking 是**同一个模型的功能特性**，通过 API 参数启用。\n\n### 支持 Extended Thinking 的模型\n\n| 模型 | Model ID |\n|------|----------|\n| Claude Opus 4.5 | `claude-opus-4-5-20251101` |\n| Claude Sonnet 4.5 | `claude-sonnet-4-5-20250929` |\n| Claude Haiku 4.5 | `claude-haiku-4-5-20251001` |\n| Claude Opus 4.1 | `claude-opus-4-1-20250805` |\n| Claude Opus 4 | `claude-opus-4-20250514` |\n| Claude Sonnet 4 | `claude-sonnet-4-20250514` |\n\n### 如何启用 Extended Thinking\n\n在 API 请求中添加 `thinking` 参数：\n\n```json\n{\n    \"model\": \"claude-opus-4-5\",\n    \"max_tokens\": 16000,\n    \"thinking\": {\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    \"messages\": [...]\n}\n```\n\n### Claude Code 当前状态\n\n当前 Claude Code 使用的是 Opus 4.5，但**未启用** Extended Thinking。如果要在 Claude Code 中启用，需要修改配置或等待官方支持。\n\nSources:\n- [Anthropic Models Overview](https://platform.claude.com/docs/en/about-claude/models)\n- [Building with Extended Thinking](https://platform.claude.com/docs/en/build-with-claude/extended-thinking)"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":21089,"cache_read_input_tokens":32756,"cache_creation":{"ephemeral_5m_input_tokens":21089,"ephemeral_1h_input_tokens":0},"output_tokens":745,"service_tier":"standard"}},"type":"assistant","uuid":"3d1ae4e3-4fa4-4a29-a924-d6f8d1db992a","timestamp":"2025-12-13T02:57:23.251Z"}
{"type":"file-history-snapshot","messageId":"c64d8776-e439-4206-9419-3e672418362c","snapshot":{"messageId":"c64d8776-e439-4206-9419-3e672418362c","trackedFileBackups":{},"timestamp":"2025-12-13T02:59:45.584Z"},"isSnapshotUpdate":false}
{"parentUuid":"3d1ae4e3-4fa4-4a29-a924-d6f8d1db992a","isSidechain":false,"userType":"external","cwd":"/Users/yunchang/Documents/GitHub/gemeng2","sessionId":"4ece2923-b68d-44f9-80e5-03967c577bcd","version":"2.0.69","gitBranch":"main","type":"user","message":{"role":"user","content":"你所说的未启用是什么意思？是我的config里面没有配置，还是官方不支持。"},"uuid":"c64d8776-e439-4206-9419-3e672418362c","timestamp":"2025-12-13T02:59:45.578Z","thinkingMetadata":{"level":"high","disabled":false,"triggers":[]},"todos":[]}
{"parentUuid":"c64d8776-e439-4206-9419-3e672418362c","isSidechain":false,"userType":"external","cwd":"/Users/yunchang/Documents/GitHub/gemeng2","sessionId":"4ece2923-b68d-44f9-80e5-03967c577bcd","version":"2.0.69","gitBranch":"main","message":{"model":"claude-opus-4-5-20251101","id":"msg_01TjYCutgQ1iinvRKAVCTvXf","type":"message","role":"assistant","content":[{"type":"thinking","thinking":"用户问的是Claude Code中Extended Thinking的启用状态。让我澄清一下：\n\n1. Claude Code 是 Anthropic 官方的CLI工具\n2. Extended Thinking 是通过API参数启用的功能\n3. 用户想知道\"未启用\"是指配置问题还是官方不支持\n\n我需要查看一下Claude Code是否支持Extended Thinking，以及如何配置。让我先查一下Claude Code的相关文档或配置。","signature":"ErcECkYIChgCKkAwgrCy1iCS3QmM3quKqhXBgJ1Mg5eL96bG0KBM0Wh6s+nmy73HVshhBFPbWpl1+useS1vbMw1e/U4JC8g9R7TsEgymSfVnXnHHVP/Om9waDLszCgEoYH9vYVgR9SIwsP936683rCgqVC7QYzez0CEcPWXhm1nWdMeDNTJPOr1ytH4ok7BTlE5shfd0BWW0Kp4DBGMpGPgk6cJmmxCSP2/g9nBjmbtxLmlp+LeroOQQ5tuleDYsibroeJujiLJ3romM//kBAVFFF6GjO3e0CQhzK0rV0tmwY8SigtWxH0Z+M46EaoOknliE7OA4kJ7ot6kBoeP/YE+718KyS6N9pdo4vbFhN60LcmK8PBZOfZRCYpebUKem8DEUFglTt+/Wgoc8ogmdXrvS8Idv6vFB2iPke2BE8zeXSpfWUgYwQ8nXshHRqV6f0xqunzOjFFE5dpMOb6xvqSiKEbB6vLjGEteGN3eJ5fZmUTrCRcNDCDwd6Kd1uzkYG0zbcfQFsxKVYOoMBsSgWDoYRJkFrwUJP8r7gqPQckgVqZa6q9W1W2939OTA7S/+xDtkFoccxqXhXbCejYILVW9RN60BAsAjA91ibsgXvF5Jd8kmn1keT/ESoiD8NU6kABpsuIAyjsC2JbGZO9nnpJG7Pz78J9JlyXo6jg3y6qmdP3gxRM0nvn44rUpQ47RBZa93eaKCHPoS+cnYuhChqCRWndtx0+7yw1ApVmBSXsvIM87PH12LIrzAGAE="}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":10,"cache_creation_input_tokens":138,"cache_read_input_tokens":54587,"cache_creation":{"ephemeral_5m_input_tokens":138,"ephemeral_1h_input_tokens":0},"output_tokens":4,"service_tier":"standard"}},"type":"assistant","uuid":"af98cc71-b80c-4e1a-9380-60d1c0826953","timestamp":"2025-12-13T02:59:53.831Z"}
{"parentUuid":"af98cc71-b80c-4e1a-9380-60d1c0826953","isSidechain":false,"userType":"external","cwd":"/Users/yunchang/Documents/GitHub/gemeng2","sessionId":"4ece2923-b68d-44f9-80e5-03967c577bcd","version":"2.0.69","gitBranch":"main","message":{"model":"claude-opus-4-5-20251101","id":"msg_01TjYCutgQ1iinvRKAVCTvXf","type":"message","role":"assistant","content":[{"type":"text","text":"我来查一下 Claude Code 是否支持 Extended Thinking 配置。"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":10,"cache_creation_input_tokens":138,"cache_read_input_tokens":54587,"cache_creation":{"ephemeral_5m_input_tokens":138,"ephemeral_1h_input_tokens":0},"output_tokens":4,"service_tier":"standard"}},"type":"assistant","uuid":"232a78a6-a62c-4895-afe0-8fb24f8b5cd5","timestamp":"2025-12-13T02:59:54.009Z"}
{"parentUuid":"232a78a6-a62c-4895-afe0-8fb24f8b5cd5","isSidechain":false,"userType":"external","cwd":"/Users/yunchang/Documents/GitHub/gemeng2","sessionId":"4ece2923-b68d-44f9-80e5-03967c577bcd","version":"2.0.69","gitBranch":"main","message":{"model":"claude-opus-4-5-20251101","id":"msg_01TjYCutgQ1iinvRKAVCTvXf","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01TCwh8mbeiri11XUMqgPKGB","name":"Task","input":{"subagent_type":"claude-code-guide","prompt":"查找 Claude Code 是否支持 Extended Thinking 功能，以及如何在 Claude Code 中启用 Extended Thinking。查找相关的配置选项、设置方法或限制说明。","description":"查询Claude Code Extended Thinking支持"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":10,"cache_creation_input_tokens":138,"cache_read_input_tokens":54587,"cache_creation":{"ephemeral_5m_input_tokens":138,"ephemeral_1h_input_tokens":0},"output_tokens":315,"service_tier":"standard"}},"type":"assistant","uuid":"cd58863b-3683-4959-a441-9ae3808bd2f0","timestamp":"2025-12-13T02:59:56.279Z"}
{"parentUuid":"cd58863b-3683-4959-a441-9ae3808bd2f0","isSidechain":false,"userType":"external","cwd":"/Users/yunchang/Documents/GitHub/gemeng2","sessionId":"4ece2923-b68d-44f9-80e5-03967c577bcd","version":"2.0.69","gitBranch":"main","slug":"imperative-painting-parasol","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01TCwh8mbeiri11XUMqgPKGB","type":"tool_result","content":[{"type":"text","text":"完美！我已经找到了关于 Claude Code 中 Extended Thinking 功能的详细信息。现在让我为你总结一下。\n\n## Claude Code 中的 Extended Thinking 功能\n\nClaude Code 完全支持 Extended Thinking 功能。以下是详细的配置说明：\n\n### 1. Extended Thinking 默认状态\n\n根据你使用的模型：\n- **Sonnet 4.5 和 Opus 4.5**：Extended Thinking 默认启用\n- **其他模型**：Extended Thinking 默认禁用\n\n查看当前模型：使用 `/model` 命令\n\n### 2. 配置 Extended Thinking 的两种方式\n\n| 方式 | 配置方法 | 详情 |\n|------|--------|------|\n| **全局默认设置** | 使用 `/config` 命令启用 thinking 模式 | 保存在 `~/.claude/settings.json` 的 `alwaysThinkingEnabled` 字段 |\n| **环境变量覆盖** | 设置 `MAX_THINKING_TOKENS` 环境变量 | 当设置时，会覆盖你的 thinking 模式配置 |\n\n### 3. 你当前的配置\n\n根据你的 `~/.claude/settings.json`，你已经启用了 Extended Thinking：\n\n```json\n{\n  \"alwaysThinkingEnabled\": true\n  // ... 其他设置\n}\n```\n\n这意味着 **Extended Thinking 在你的所有项目中默认启用**。\n\n### 4. 单次请求启用 Extended Thinking\n\n使用 `ultrathink` 关键字在单个请求中启用深度思考：\n\n```\n> ultrathink: 设计我们API的缓存层\n```\n\n注意：`ultrathink` 会同时分配思考预算和语义信号，可能导致比必要更深层的思考。\n\n### 5. Token 预算说明\n\n**Extended Thinking Token 预算：**\n\n- **启用时**（通过 `/config` 或 `ultrathink`）：Claude 可使用最多 **31,999 tokens** 用于内部推理\n- **禁用时**：0 tokens 用于思考\n- **自定义预算**：通过设置 `MAX_THINKING_TOKENS` 环境变量自定义\n\n例如：\n```bash\nexport MAX_THINKING_TOKENS=1024\n```\n\n### 6. 查看 Claude 的思考过程\n\n按 **Ctrl+O** 切换详细模式，显示内部推理（灰色斜体文本）。\n\n### 7. 何时使用 Extended Thinking\n\nExtended Thinking 特别适合以下场景：\n- 复杂的架构决策\n- 困难的 bug 修复\n- 多步骤实现规划\n- 评估不同方案的权衡\n\n### 8. 重要提示\n\n- **收费提醒**：所有使用的 thinking tokens 都会被计费，即使 Claude 4 模型显示的是总结后的思考过程\n- **当设置 `MAX_THINKING_TOKENS` 时**：它优先级最高，覆盖所有其他配置\n\n### 快速命令参考\n\n```bash\n# 启用/禁用 thinking 模式（全局默认）\n/config\n\n# 查看当前模型\n/model\n\n# 在单个请求中启用深度思考\nultrathink: <你的提示>\n\n# 启用自定义 thinking token 预算\nexport MAX_THINKING_TOKENS=2048\n\n# 查看详细的思考过程\nCtrl+O  # 在会话中切换详细模式\n```\n\n根据你的配置，Extended Thinking 已经在为你的所有 Claude Code 会话启用了。这对于处理复杂的代码问题和架构决策非常有帮助！\n\nSources:\n- [Claude Code Common Workflows - Extended Thinking](https://code.claude.com/docs/en/common-workflows.md#use-extended-thinking-thinking-mode)\n- [Claude Code Settings - alwaysThinkingEnabled](https://code.claude.com/docs/en/settings.md#available-settings)\n- [Claude Code Model Configuration](https://code.claude.com/docs/en/model-config.md)"},{"type":"text","text":"agentId: a2e90ff (for resuming to continue this agent's work if needed)"}]}]},"uuid":"06e72167-255e-470b-9e9d-88b80052ff97","timestamp":"2025-12-13T03:00:27.095Z","toolUseResult":{"status":"completed","prompt":"查找 Claude Code 是否支持 Extended Thinking 功能，以及如何在 Claude Code 中启用 Extended Thinking。查找相关的配置选项、设置方法或限制说明。","agentId":"a2e90ff","content":[{"type":"text","text":"完美！我已经找到了关于 Claude Code 中 Extended Thinking 功能的详细信息。现在让我为你总结一下。\n\n## Claude Code 中的 Extended Thinking 功能\n\nClaude Code 完全支持 Extended Thinking 功能。以下是详细的配置说明：\n\n### 1. Extended Thinking 默认状态\n\n根据你使用的模型：\n- **Sonnet 4.5 和 Opus 4.5**：Extended Thinking 默认启用\n- **其他模型**：Extended Thinking 默认禁用\n\n查看当前模型：使用 `/model` 命令\n\n### 2. 配置 Extended Thinking 的两种方式\n\n| 方式 | 配置方法 | 详情 |\n|------|--------|------|\n| **全局默认设置** | 使用 `/config` 命令启用 thinking 模式 | 保存在 `~/.claude/settings.json` 的 `alwaysThinkingEnabled` 字段 |\n| **环境变量覆盖** | 设置 `MAX_THINKING_TOKENS` 环境变量 | 当设置时，会覆盖你的 thinking 模式配置 |\n\n### 3. 你当前的配置\n\n根据你的 `~/.claude/settings.json`，你已经启用了 Extended Thinking：\n\n```json\n{\n  \"alwaysThinkingEnabled\": true\n  // ... 其他设置\n}\n```\n\n这意味着 **Extended Thinking 在你的所有项目中默认启用**。\n\n### 4. 单次请求启用 Extended Thinking\n\n使用 `ultrathink` 关键字在单个请求中启用深度思考：\n\n```\n> ultrathink: 设计我们API的缓存层\n```\n\n注意：`ultrathink` 会同时分配思考预算和语义信号，可能导致比必要更深层的思考。\n\n### 5. Token 预算说明\n\n**Extended Thinking Token 预算：**\n\n- **启用时**（通过 `/config` 或 `ultrathink`）：Claude 可使用最多 **31,999 tokens** 用于内部推理\n- **禁用时**：0 tokens 用于思考\n- **自定义预算**：通过设置 `MAX_THINKING_TOKENS` 环境变量自定义\n\n例如：\n```bash\nexport MAX_THINKING_TOKENS=1024\n```\n\n### 6. 查看 Claude 的思考过程\n\n按 **Ctrl+O** 切换详细模式，显示内部推理（灰色斜体文本）。\n\n### 7. 何时使用 Extended Thinking\n\nExtended Thinking 特别适合以下场景：\n- 复杂的架构决策\n- 困难的 bug 修复\n- 多步骤实现规划\n- 评估不同方案的权衡\n\n### 8. 重要提示\n\n- **收费提醒**：所有使用的 thinking tokens 都会被计费，即使 Claude 4 模型显示的是总结后的思考过程\n- **当设置 `MAX_THINKING_TOKENS` 时**：它优先级最高，覆盖所有其他配置\n\n### 快速命令参考\n\n```bash\n# 启用/禁用 thinking 模式（全局默认）\n/config\n\n# 查看当前模型\n/model\n\n# 在单个请求中启用深度思考\nultrathink: <你的提示>\n\n# 启用自定义 thinking token 预算\nexport MAX_THINKING_TOKENS=2048\n\n# 查看详细的思考过程\nCtrl+O  # 在会话中切换详细模式\n```\n\n根据你的配置，Extended Thinking 已经在为你的所有 Claude Code 会话启用了。这对于处理复杂的代码问题和架构决策非常有帮助！\n\nSources:\n- [Claude Code Common Workflows - Extended Thinking](https://code.claude.com/docs/en/common-workflows.md#use-extended-thinking-thinking-mode)\n- [Claude Code Settings - alwaysThinkingEnabled](https://code.claude.com/docs/en/settings.md#available-settings)\n- [Claude Code Model Configuration](https://code.claude.com/docs/en/model-config.md)"}],"totalDurationMs":30784,"totalTokens":43820,"totalToolUseCount":7,"usage":{"input_tokens":11670,"cache_creation_input_tokens":7560,"cache_read_input_tokens":23416,"cache_creation":{"ephemeral_5m_input_tokens":7560,"ephemeral_1h_input_tokens":0},"output_tokens":1174,"service_tier":"standard"}}}
{"parentUuid":"06e72167-255e-470b-9e9d-88b80052ff97","isSidechain":false,"userType":"external","cwd":"/Users/yunchang/Documents/GitHub/gemeng2","sessionId":"4ece2923-b68d-44f9-80e5-03967c577bcd","version":"2.0.69","gitBranch":"main","slug":"imperative-painting-parasol","message":{"model":"claude-opus-4-5-20251101","id":"msg_01VPP1JGtGpSBSxUhe8RqF4w","type":"message","role":"assistant","content":[{"type":"thinking","thinking":"现在我有了完整的信息。让我总结给用户：\n\n1. Claude Code **完全支持** Extended Thinking\n2. 对于 Sonnet 4.5 和 Opus 4.5，Extended Thinking **默认启用**\n3. 用户可以通过 `/config` 命令配置，或者设置 `MAX_THINKING_TOKENS` 环境变量\n\n所以我之前说\"未启用\"是不准确的。实际上对于 Opus 4.5，Extended Thinking 是默认启用的。","signature":"EqkECkYIChgCKkBoUfdL1R6DxxJXoVBZutiVOGIJvE1evZHWPm953Xebdd8Ko45cRe+0LtZ6ocooGdXZJJAdyUtkhM0DZlPfcmWBEgyMfBTlhjujOZ/bxj0aDKTPQwyzirqX7Hi17SIwbDurCwjWeoz/GB9hklmiriBk1YGog7fLOZBK+Gb2MMOnqrxad5Cd9AAzWnsepjGvKpADfnxM1kLCffzMzDipY0+VOGN0sH/tBApbg0OtGxyKSM+1elEdwdUfTKP4IQmkuVkbRgLBgD133t0fAvhH5WBYOnmpHLJ9FV1egO/vHPUX6Xvn9i0UGZopfOVIN9PVz9kRarto7YwPS9DEhqobOh8rFCjmgjGRHmPQ8aDekkbzfQZVWHja4yc+ICcZ4QH/OIS3avg4320ndVlRWCPlo0Yb2l3/ZpxlVTpt+dd+LFrmCpNtAQTOy02wpbcatVI6Ny9J9AkSgmxRw9kc/mpbOXnGRonnfFMggOi5CRU1+PO3ccyrOo9B5Duz7Kxof7+I8mKo8fJkGTpoI9WXu8P5T8BEZ04Sok3EKMgHZkILNtW+FkFne3GHJhRtaj1Phb6g6wONHvkdhs8YogPZfnEEspUYbS4/UT+8mvTddX64h8kzyMk8xI/R6AHRJkZ9mzoB8uYKeFz172kX4Pv4bJ75uyh9IxQLp4ym+MW8VPvIGXfIhvbzO+xlJaAw+cl6PMrEu91qbBn+qbJ7IHaMV0IeT/WqwBgB"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":1526,"cache_read_input_tokens":54725,"cache_creation":{"ephemeral_5m_input_tokens":1526,"ephemeral_1h_input_tokens":0},"output_tokens":4,"service_tier":"standard"}},"type":"assistant","uuid":"032a5cc0-9a7e-4e75-b167-309306be8cc3","timestamp":"2025-12-13T03:00:32.381Z"}
{"parentUuid":"032a5cc0-9a7e-4e75-b167-309306be8cc3","isSidechain":false,"userType":"external","cwd":"/Users/yunchang/Documents/GitHub/gemeng2","sessionId":"4ece2923-b68d-44f9-80e5-03967c577bcd","version":"2.0.69","gitBranch":"main","slug":"imperative-painting-parasol","message":{"model":"claude-opus-4-5-20251101","id":"msg_01VPP1JGtGpSBSxUhe8RqF4w","type":"message","role":"assistant","content":[{"type":"text","text":"抱歉我之前说法不准确。让我更正：\n\n## Claude Code 完全支持 Extended Thinking\n\n### 当前状态\n\n对于 **Opus 4.5 和 Sonnet 4.5**，Extended Thinking **默认启用**。你现在用的 Opus 4.5 已经在使用 Extended Thinking 了。\n\n### 配置方式\n\n| 方式 | 说明 |\n|------|------|\n| `/config` 命令 | 在 Claude Code 中运行，启用 \"thinking mode\" |\n| 环境变量 | `export MAX_THINKING_TOKENS=10000` 自定义预算 |\n| 单次请求 | 用 `ultrathink:` 前缀触发深度思考 |\n\n### 查看思考过程\n\n按 **Ctrl+O** 可以切换详细模式，查看 Claude 的内部推理过程（灰色斜体显示）。\n\n### Token 预算\n\n- 启用时默认最多 **31,999 tokens** 用于思考\n- 可通过 `MAX_THINKING_TOKENS` 环境变量自定义\n\n### 总结\n\n**不是官方不支持，也不是你没配置** —— Extended Thinking 在 Opus 4.5 上已经默认启用并运行中。你当前的对话就在使用这个功能。\n\n如果你想确认或调整设置，可以运行 `/config` 查看。"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":1526,"cache_read_input_tokens":54725,"cache_creation":{"ephemeral_5m_input_tokens":1526,"ephemeral_1h_input_tokens":0},"output_tokens":561,"service_tier":"standard"}},"type":"assistant","uuid":"12a2f05f-4b7b-45d3-a628-2965dedcdc04","timestamp":"2025-12-13T03:00:38.614Z"}
